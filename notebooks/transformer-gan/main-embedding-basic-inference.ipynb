{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main-embedding-basic-inference.ipynb","provenance":[],"collapsed_sections":["j3yqvRko3s8c","IUgSFmiTrwU8","A6F9FE6tTVig","a-aRdpmlTanv","HFFCa2koFv26"],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7AMKmmdQpI16","executionInfo":{"status":"ok","timestamp":1606794695895,"user_tz":480,"elapsed":63940,"user":{"displayName":"Juyoung Lee","photoUrl":"","userId":"16438012406002097348"}},"outputId":"de7a8b58-58e4-48a8-c0bf-580d0af08f21"},"source":["import os \n","from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j3yqvRko3s8c"},"source":["### Basic Imports"]},{"cell_type":"code","metadata":{"id":"wX4s0cGrpiCb","executionInfo":{"status":"ok","timestamp":1606794709495,"user_tz":480,"elapsed":9687,"user":{"displayName":"Juyoung Lee","photoUrl":"","userId":"16438012406002097348"}}},"source":["MAIN_DIR = '/content/drive/My Drive/Colab Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/papertransformer'\n","DATA_DIR = '/content/drive/My Drive/Colab Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/backtranslation/data'\n","MODEL_DIR = os.path.join(MAIN_DIR, 'model')\n","CONF_DIR = os.path.join(MAIN_DIR, 'conf')\n","\n","UNK_TOKEN = '<unk>'\n","PAD_TOKEN = '<pad>'\n","EOS_TOKEN = '</s>'\n","BOS_TOKEN = '<s>'\n","\n","TARGET_PAD = 0.0\n","\n","DEFAULT_UNK_ID = lambda: 0\n","\n","!cp /content/drive/My\\ Drive/Colab\\ Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/papertransformer/vocabulary.py .\n","!cp /content/drive/My\\ Drive/Colab\\ Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/papertransformer/initialization.py .\n","!cp /content/drive/My\\ Drive/Colab\\ Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/papertransformer/transformer_layers.py .\n","!cp /content/drive/My\\ Drive/Colab\\ Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/papertransformer/batch.py .\n","!cp /content/drive/My\\ Drive/Colab\\ Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/papertransformer/loss.py .\n","!cp /content/drive/My\\ Drive/Colab\\ Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/papertransformer/builders.py .\n","!cp /content/drive/My\\ Drive/Colab\\ Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/papertransformer/dtw.py .\n","!cp /content/drive/My\\ Drive/Colab\\ Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/papertransformer/plot_videos.py .\n","\n","\n","import yaml\n","import numpy as np\n","import random\n","import pickle\n","import sys\n","from typing import Optional\n","import queue\n","import glob\n","import time\n","from logging import Logger\n","import logging\n","\n","import torch\n","from torch import nn, Tensor\n","import torch.nn.functional as F\n","from torchtext.data import Dataset, Example, Field, BucketIterator, Iterator\n","\n","from collections import defaultdict, Counter\n","from vocabulary import build_vocab\n","from initialization import initialize_model\n","from transformer_layers import TransformerEncoderLayer, PositionalEncoding, TransformerDecoderLayer\n","from vocabulary import Vocabulary\n","from batch import Batch\n","from loss import RegLoss\n","from builders import build_optimizer, build_scheduler, build_gradient_clipper\n","from dtw import dtw\n","from plot_videos import plot_video, alter_DTW_timing"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1kbo-vhHzbai","executionInfo":{"status":"ok","timestamp":1606794709497,"user_tz":480,"elapsed":9625,"user":{"displayName":"Juyoung Lee","photoUrl":"","userId":"16438012406002097348"}},"outputId":"c1c156fa-2f16-470e-fca5-87cb57193fe1"},"source":["len(os.path.join(MODEL_DIR, 'v3', 'test_videos'))"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["121"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"rCrvrN30tHa3"},"source":["### Helper functions"]},{"cell_type":"code","metadata":{"id":"WEZP6CDXtJnM","executionInfo":{"status":"ok","timestamp":1606794792158,"user_tz":480,"elapsed":559,"user":{"displayName":"Juyoung Lee","photoUrl":"","userId":"16438012406002097348"}}},"source":["def load_config(path=\"model.yaml\") -> dict:\n","    \"\"\"\n","    Loads and parses a YAML configuration file.\n","\n","    :param path: path to YAML configuration file\n","    :return: configuration dictionary\n","    \"\"\"\n","    with open(os.path.join(CONF_DIR, path), 'r') as ymlfile:\n","        cfg = yaml.safe_load(ymlfile)\n","    return cfg\n","\n","def set_seed(seed: int) -> None:\n","    \"\"\"\n","    Set the random seed for modules torch, numpy and random.\n","\n","    :param seed: random seed\n","    \"\"\"\n","    torch.manual_seed(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","\n","def freeze_params(module: nn.Module) -> None:\n","    \"\"\"\n","    Freeze the parameters of this module,\n","    i.e. do not update them during training\n","\n","    :param module: freeze parameters of this module\n","    \"\"\"\n","    for _, p in module.named_parameters():\n","        p.requires_grad = False\n","\n","def subsequent_mask(size: int) -> Tensor:\n","    \"\"\"\n","    Mask out subsequent positions (to prevent attending to future positions)\n","    Transformer helper function.\n","\n","    :param size: size of mask (2nd and 3rd dim)\n","    :return: Tensor with 0s and 1s of shape (1, size, size)\n","    \"\"\"\n","    mask = np.triu(np.ones((1, size, size)), k=1).astype('uint8')\n","\n","    return torch.from_numpy(mask) == 0 # Turns it into True and False's\n","\n","def symlink_update(target, link_name):\n","    try:\n","        os.symlink(target, link_name)\n","    except FileExistsError as e:\n","        if e.errno == errno.EEXIST:\n","            os.remove(link_name)\n","            os.symlink(target, link_name)\n","        else:\n","            raise e\n","\n","def load_checkpoint(path: str, use_cuda: bool = True) -> dict:\n","    \"\"\"\n","    Load model from saved checkpoint.\n","\n","    :param path: path to checkpoint\n","    :param use_cuda: using cuda or not\n","    :return: checkpoint (dict)\n","    \"\"\"\n","    assert os.path.isfile(path), \"Checkpoint %s not found\" % path\n","    checkpoint = torch.load(path, map_location='cuda' if use_cuda else 'cpu')\n","    return checkpoint\n","\n","class ConfigurationError(Exception):\n","    \"\"\" Custom exception for misspecifications of configuration \"\"\"\n","\n","\n","def get_latest_checkpoint(ckpt_dir, post_fix=\"_every\" ) -> Optional[str]:\n","    \"\"\"\n","    Returns the latest checkpoint (by time) from the given directory, of either every validation step or best\n","    If there is no checkpoint in this directory, returns None\n","\n","    :param ckpt_dir: directory of checkpoint\n","    :param post_fixe: type of checkpoint, either \"_every\" or \"_best\"\n","\n","    :return: latest checkpoint file\n","    \"\"\"\n","    # Find all the every validation checkpoints\n","    list_of_files = glob.glob(\"{}/*{}.ckpt\".format(ckpt_dir,post_fix))\n","    latest_checkpoint = None\n","    if list_of_files:\n","        latest_checkpoint = max(list_of_files, key=os.path.getctime)\n","    return latest_checkpoint\n","\n","def log_cfg(cfg: dict, logger: Logger, prefix: str = \"cfg\"):\n","    \"\"\"\n","    Write configuration to log.\n","\n","    :param cfg: configuration to log\n","    :param logger: logger that defines where log is written to\n","    :param prefix: prefix for logging\n","    \"\"\"\n","    for k, v in cfg.items():\n","        if isinstance(v, dict):\n","            p = \".\".join([prefix, k])\n","            log_cfg(v, logger, prefix=p)\n","        else:\n","            p = \".\".join([prefix, k])\n","            logger.info(\"{:34s} : {}\".format(p, v))\n","\n","def make_logger(model_dir: str, log_file: str = \"inference.log\") -> Logger:\n","    \"\"\"\n","    Create a logger for logging the training process.\n","\n","    :param model_dir: path to logging directory\n","    :param log_file: path to logging file\n","    :return: logger object\n","    \"\"\"\n","    #if not logger.handlers:\n","    logger = logging.getLogger(__name__)\n","    while len(logger.handlers) > 0:\n","        h = logger.handlers[0]\n","        print('removing {}'.format(h))\n","        logger.removeHandler(h)\n","    logger.propagate = False\n","    logger.setLevel(logging.INFO)\n","    # Create handlers\n","    c_handler = logging.StreamHandler()\n","    f_handler = logging.FileHandler(os.path.join(model_dir, log_file), mode='w')\n","\n","    # Create formatters and add it to handlers\n","    c_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","    f_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","    c_handler.setFormatter(c_format)\n","    f_handler.setFormatter(f_format)\n","\n","    # Add handlers to the logger\n","    logger.addHandler(c_handler)\n","    logger.addHandler(f_handler)\n","\n","    return logger"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IUgSFmiTrwU8"},"source":["### Data Loading"]},{"cell_type":"code","metadata":{"id":"O-i9NBfQruOe","executionInfo":{"status":"ok","timestamp":1606794710263,"user_tz":480,"elapsed":8489,"user":{"displayName":"Juyoung Lee","photoUrl":"","userId":"16438012406002097348"}}},"source":["def build_vocab(field: str, max_size: int, min_freq: int, dataset: Dataset,\n","                vocab_file: str = None) -> Vocabulary:\n","    \"\"\"\n","    Builds vocabulary for a torchtext `field` from given`dataset` or\n","    `vocab_file`.\n","\n","    :param field: attribute e.g. \"src\"\n","    :param max_size: maximum size of vocabulary\n","    :param min_freq: minimum frequency for an item to be included\n","    :param dataset: dataset to load data for field from\n","    :param vocab_file: file to store the vocabulary,\n","        if not None, load vocabulary from here\n","    :return: Vocabulary created from either `dataset` or `vocab_file`\n","    \"\"\"\n","\n","    if vocab_file is not None:\n","        # load it from file\n","        vocab = Vocabulary(file=vocab_file)\n","    else:\n","        # create newly\n","        def filter_min(counter: Counter, min_freq: int):\n","            \"\"\" Filter counter by min frequency \"\"\"\n","            filtered_counter = Counter({t: c for t, c in counter.items()\n","                                        if c >= min_freq})\n","            return filtered_counter\n","\n","        def sort_and_cut(counter: Counter, limit: int):\n","            \"\"\" Cut counter to most frequent,\n","            sorted numerically and alphabetically\"\"\"\n","            # sort by frequency, then alphabetically\n","            tokens_and_frequencies = sorted(counter.items(),\n","                                            key=lambda tup: tup[0])\n","            tokens_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n","            vocab_tokens = [i[0] for i in tokens_and_frequencies[:limit]]\n","            return vocab_tokens\n","\n","        tokens = []\n","        for i in dataset.examples:\n","            if field == \"src\":\n","                tokens.extend(i.src)\n","            elif field == \"trg\":\n","                tokens.extend(i.trg)\n","            elif field == 'emb':\n","                tokens.extend(i.emb)\n","\n","        counter = Counter(tokens)\n","        if min_freq > -1:\n","            counter = filter_min(counter, min_freq)\n","        vocab_tokens = sort_and_cut(counter, max_size)\n","        assert len(vocab_tokens) <= max_size\n","\n","        vocab = Vocabulary(tokens=vocab_tokens)\n","        assert len(vocab) <= max_size + len(vocab.specials)\n","        assert vocab.itos[DEFAULT_UNK_ID()] == UNK_TOKEN\n","\n","    # check for all except for UNK token whether they are OOVs\n","    for s in vocab.specials[1:]:\n","        assert not vocab.is_unk(s)\n","\n","    return vocab\n","\n","def load_data(cfg: dict):\n","    \n","    # read parsed data\n","    # list of ['video_id', 'skeletons', 'frame_cnt', 'word', 'embedding']\n","    # skeleton shape [frame #, skeleton + counter = 151]\n","\n","    train_path = os.path.join(DATA_DIR, 'train.pkl')\n","    val_path = os.path.join(DATA_DIR, 'val.pkl')\n","    level = \"word\"\n","    data_cfg = cfg[\"data\"]\n","    src_lang = data_cfg[\"src\"] #gloss\n","    trg_lang = data_cfg[\"trg\"] #skels\n","    lowercase = False\n","    max_sent_length = data_cfg[\"max_sent_length\"] # 1\n","    trg_size = cfg[\"model\"][\"trg_size\"] + 1 # to account for counter\n","    skip_frames = data_cfg.get(\"skip_frames\", 1)\n","\n","    EOS_TOKEN = '</s>'\n","    tok_fun = lambda s: list(s) if level == \"char\" else s.split()\n","    \n","    src_field = Field(init_token=None,\n","                      pad_token=PAD_TOKEN, tokenize=tok_fun,\n","                      batch_first=True, lower=lowercase,\n","                      unk_token=UNK_TOKEN,\n","                      include_lengths=True)\n","        \n","\n","    reg_trg_field = Field(sequential=True,\n","                          use_vocab=False,\n","                          dtype=torch.float32,\n","                          batch_first=True,\n","                          include_lengths=False,\n","                          pad_token=torch.ones((trg_size))*TARGET_PAD)\n","    \n","    embedding_field = Field(sequential=True,\n","                            use_vocab=False,\n","                            dtype=torch.float32,\n","                            batch_first=True,\n","                            include_lengths=True)\n","\n","\n","    train_data = SignProdDataset(fields=(embedding_field, reg_trg_field, src_field),\n","                                 path=train_path,\n","                                 trg_size=trg_size,\n","                                 skip_frames=skip_frames)\n","    \n","    src_vocab = build_vocab(field=\"emb\", min_freq=1,\n","                            max_size=sys.maxsize,\n","                            dataset=train_data, vocab_file=None)\n","    \n","    src_field.vocab = src_vocab\n","    trg_vocab = [None]*(trg_size)\n","\n","      # Create the Validation Data\n","    dev_data = SignProdDataset(fields=(embedding_field, reg_trg_field, src_field),\n","                               path=val_path,\n","                               trg_size=trg_size,\n","                               skip_frames=skip_frames)\n","    \n","    \n","    return train_data, dev_data, src_vocab, trg_vocab\n","\n","global max_src_in_batch, max_tgt_in_batch\n","\n","\n","# pylint: disable=unused-argument,global-variable-undefined\n","def token_batch_size_fn(new, count, sofar):\n","    \"\"\"Compute batch size based on number of tokens (+padding).\"\"\"\n","    global max_src_in_batch, max_tgt_in_batch\n","    if count == 1:\n","        max_src_in_batch = 0\n","        max_tgt_in_batch = 0\n","    max_src_in_batch = max(max_src_in_batch, len(new.src))\n","    src_elements = count * max_src_in_batch\n","    if hasattr(new, 'trg'):  # for monolingual data sets (\"translate\" mode)\n","        max_tgt_in_batch = max(max_tgt_in_batch, len(new.trg) + 2)\n","        tgt_elements = count * max_tgt_in_batch\n","    else:\n","        tgt_elements = 0\n","    return max(src_elements, tgt_elements)\n","\n","class SignProdDataset(Dataset):\n","    def __init__(self, \n","                 fields,\n","                 path,\n","                 trg_size,\n","                 skip_frames=1,\n","                 **kwargs):\n","        if not isinstance(fields[0], (tuple, list)):\n","            fields = [('src', fields[0]), ('trg', fields[1]), ('emb', fields[2])]\n","        \n","        examples = []\n","        with open(path, 'rb') as data_file:\n","            dataset = pickle.load(data_file)\n","        with open(os.path.join(DATA_DIR, 'embedding.200.pkl') ,'rb') as embed_file:\n","            embed_dict= pickle.load(embed_file)\n","\n","        for i, skeleton in enumerate(dataset[\"skeleton\"]):\n","            src_line = dataset[\"gloss\"][i].replace(' ', '')\n","            # add start frame (zeros)\n","            start_frame = np.zeros((1, skeleton.shape[-1]))\n","            with_start_frame = np.concatenate((start_frame, skeleton))\n","            normalized = with_start_frame + 1e-8\n","\n","            if skip_frames > 1:\n","              normalized = normalized[0::skip_frames]\n","\n","            # add counter here\n","            counters = np.arange(0,len(normalized),1)/len(normalized)\n","            with_counter = np.concatenate((normalized, counters[:, np.newaxis]), axis=1)\n","\n","            # set embedding\n","            embed_x = embed_dict[src_line][np.newaxis, :]\n","\n","            examples.append(Example.fromlist([embed_x, with_counter, src_line], fields))\n","            super(SignProdDataset, self).__init__(examples, fields, **kwargs)\n","         \n","            \n","def make_data_iter(dataset: Dataset,\n","                   batch_size: int,\n","                   batch_type: str = \"sentence\",\n","                   train: bool = False,\n","                   shuffle: bool = False) -> Iterator:\n","    \"\"\"\n","    Returns a torchtext iterator for a torchtext dataset.\n","\n","    :param dataset: torchtext dataset containing src and optionally trg\n","    :param batch_size: size of the batches the iterator prepares\n","    :param batch_type: measure batch size by sentence count or by token count\n","    :param train: whether it's training time, when turned off,\n","        bucketing, sorting within batches and shuffling is disabled\n","    :param shuffle: whether to shuffle the data before each epoch\n","        (no effect if set to True for testing)\n","    :return: torchtext iterator\n","    \"\"\"\n","\n","    batch_size_fn = token_batch_size_fn if batch_type == \"token\" else None\n","\n","    if train:\n","        # optionally shuffle and sort during training\n","        data_iter = BucketIterator(\n","            repeat=False, sort=False, dataset=dataset,\n","            batch_size=batch_size, batch_size_fn=batch_size_fn,\n","            train=True, sort_within_batch=True,\n","            sort_key=lambda x: len(x.src), shuffle=shuffle)\n","    else:\n","        # don't sort/shuffle for validation/inference\n","        data_iter = BucketIterator(\n","            repeat=False, dataset=dataset,\n","            batch_size=batch_size, batch_size_fn=batch_size_fn,\n","            train=False, sort=False)\n","\n","    return data_iter\n","\n","import torch\n","import torch.nn.functional as F\n","\n","\n","class Batch:\n","    \"\"\"Object for holding a batch of data with mask during training.\n","    Input is a batch from a torch text iterator.\n","    \"\"\"\n","\n","    def __init__(self, torch_batch, pad_index, model):\n","\n","        \"\"\"\n","        Create a new joey batch from a torch batch.\n","        This batch extends torch text's batch attributes with src and trg\n","        length, masks, number of non-padded tokens in trg.\n","        Furthermore, it can be sorted by src length.\n","\n","        :param torch_batch:\n","        :param pad_index:\n","        :param use_cuda:\n","        \"\"\"\n","        self.src, self.src_lengths = torch_batch.src\n","\n","        self.emb, _ = torch_batch.emb\n","        self.src_mask = (self.emb != pad_index).unsqueeze(1)\n","        self.nseqs = self.src.size(0)\n","        self.trg_input = None\n","        self.trg = None\n","        self.trg_mask = None\n","        self.trg_lengths = None\n","        self.ntokens = None\n","\n","        self.use_cuda = model.use_cuda\n","        self.target_pad = TARGET_PAD\n","        # Just Count\n","        self.just_count_in = model.just_count_in\n","        # Future Prediction\n","        self.future_prediction = model.future_prediction\n","\n","        if hasattr(torch_batch, \"trg\"):\n","            trg = torch_batch.trg\n","            trg_lengths = torch_batch.trg.shape[1]\n","            # trg_input is used for teacher forcing, last one is cut off\n","            # Remove the last frame for target input, as inputs are only up to frame N-1\n","            self.trg_input = trg.clone()[:, :-1,:]\n","\n","            self.trg_lengths = trg_lengths\n","            # trg is used for loss computation, shifted by one since BOS\n","            self.trg = trg.clone()[:, 1:, :]\n","\n","            # Just Count\n","            if self.just_count_in:\n","                # If Just Count, cut off the first frame of trg_input\n","                self.trg_input = self.trg_input[:, :, -1:]\n","\n","            # Future Prediction\n","            if self.future_prediction != 0:\n","                # Loop through the future prediction, concatenating the frames shifted across once each time\n","                future_trg = torch.Tensor()\n","                # Concatenate each frame (Not counter)\n","                for i in range(0, self.future_prediction):\n","                    future_trg = torch.cat((future_trg, self.trg[:, i:-(self.future_prediction - i), :-1].clone()), dim=2)\n","                # Create the final target using the collected future_trg and original trg\n","                self.trg = torch.cat((future_trg, self.trg[:,:-self.future_prediction,-1:]), dim=2)\n","\n","                # Cut off the last N frames of the trg_input\n","                self.trg_input = self.trg_input[:, :-self.future_prediction, :]\n","\n","            # Target Pad is dynamic, so we exclude the padded areas from the loss computation\n","            trg_mask = (self.trg_input != self.target_pad).unsqueeze(1)\n","            # This increases the shape of the target mask to be even (16,1,120,120) -\n","            # adding padding that replicates - so just continues the False's or True's\n","            pad_amount = self.trg_input.shape[1] - self.trg_input.shape[2]\n","            # Create the target mask the same size as target input\n","            self.trg_mask = (F.pad(input=trg_mask.double(), pad=(pad_amount, 0, 0, 0), mode='replicate') == 1.0)\n","            self.ntokens = (self.trg != pad_index).data.sum().item()\n","\n","        if self.use_cuda:\n","            self._make_cuda()\n","\n","    # If using Cuda\n","    def _make_cuda(self):\n","        \"\"\"\n","        Move the batch to GPU\n","\n","        :return:\n","        \"\"\"\n","        self.src = self.src.cuda()\n","        self.src_mask = self.src_mask.cuda()\n","\n","        if self.trg_input is not None:\n","            self.trg_input = self.trg_input.cuda()\n","            self.trg = self.trg.cuda()\n","            self.trg_mask = self.trg_mask.cuda()\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A6F9FE6tTVig"},"source":["### Building blocks of the Model"]},{"cell_type":"code","metadata":{"id":"7VFATXOht3GB","executionInfo":{"status":"ok","timestamp":1606794710665,"user_tz":480,"elapsed":7943,"user":{"displayName":"Juyoung Lee","photoUrl":"","userId":"16438012406002097348"}}},"source":["class Embeddings(nn.Module):\n","\n","    \"\"\"\n","    Updated Embedding Class\n","    \"\"\"\n","\n","    # pylint: disable=unused-argument\n","    def __init__(self,\n","                 embedding_dim: int = 64,\n","                 scale: bool = False,\n","                 vocab_size: int = 0,\n","                 padding_idx: int = 1,\n","                 freeze: bool = False,\n","                 **kwargs):\n","        \"\"\"\n","        Create new embeddings for the vocabulary.\n","        Use scaling for the Transformer.\n","\n","        :param embedding_dim:\n","        :param scale:\n","        :param vocab_size:\n","        :param padding_idx:\n","        :param freeze: freeze the embeddings during training\n","        \"\"\"\n","        super(Embeddings, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.scale = scale\n","        self.vocab_size = vocab_size\n","        #self.lut = nn.Embedding(vocab_size, self.embedding_dim,\n","        #                        padding_idx=padding_idx)\n","        self.lut = nn.Linear(200, embedding_dim)\n","\n","        if freeze:\n","            freeze_params(self)\n","\n","    # pylint: disable=arguments-differ\n","    def forward(self, x: Tensor) -> Tensor:\n","        \"\"\"\n","        Perform lookup for input `x` in the embedding table.\n","\n","        :param x: index in the vocabulary\n","        :return: embedded representation for `x`\n","        \"\"\"\n","        if self.scale:\n","            return self.lut(x) * math.sqrt(self.embedding_dim)\n","        return self.lut(x)\n","\n","    def __repr__(self):\n","        return \"%s(embedding_dim=%d, vocab_size=%d)\" % (\n","            self.__class__.__name__, self.embedding_dim, self.vocab_size)\n","\n","        \n","class Decoder(nn.Module):\n","    \"\"\"\n","    Base decoder class\n","    \"\"\"\n","\n","    @property\n","    def output_size(self):\n","        \"\"\"\n","        Return the output size (size of the target vocabulary)\n","\n","        :return:\n","        \"\"\"\n","        return self._output_size\n","\n","class TransformerDecoder(Decoder):\n","    \"\"\"\n","    A transformer decoder with N masked layers.\n","    Decoder layers are masked so that an attention head cannot see the future.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 num_layers: int = 4,\n","                 num_heads: int = 8,\n","                 hidden_size: int = 512,\n","                 ff_size: int = 2048,\n","                 dropout: float = 0.1,\n","                 emb_dropout: float = 0.1,\n","                 vocab_size: int = 1,\n","                 freeze: bool = False,\n","                 trg_size: int = 97,\n","                 decoder_trg_trg_: bool = True,\n","                 **kwargs):\n","        \"\"\"\n","        Initialize a Transformer decoder.\n","\n","        :param num_layers: number of Transformer layers\n","        :param num_heads: number of heads for each layer\n","        :param hidden_size: hidden size\n","        :param ff_size: position-wise feed-forward size\n","        :param dropout: dropout probability (1-keep)\n","        :param emb_dropout: dropout probability for embeddings\n","        :param vocab_size: size of the output vocabulary\n","        :param freeze: set to True keep all decoder parameters fixed\n","        :param kwargs:\n","        \"\"\"\n","        super(TransformerDecoder, self).__init__()\n","\n","        self._hidden_size = hidden_size\n","\n","        # Dynamic output size depending on the target size\n","        self._output_size = trg_size\n","\n","        # create num_layers decoder layers and put them in a list\n","        self.layers = nn.ModuleList([TransformerDecoderLayer(\n","                size=hidden_size, ff_size=ff_size, num_heads=num_heads,\n","                dropout=dropout, decoder_trg_trg=decoder_trg_trg_) for _ in range(num_layers)])\n","\n","        self.pe = PositionalEncoding(hidden_size,mask_count=True)\n","        self.layer_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n","\n","        self.emb_dropout = nn.Dropout(p=emb_dropout)\n","\n","        # Output layer to be the size of joints vector + 1 for counter (total is trg_size)\n","        self.output_layer = nn.Linear(hidden_size, trg_size, bias=False)\n","\n","        if freeze:\n","            freeze_params(self)\n","\n","    def forward(self,\n","                trg_embed: Tensor = None,\n","                encoder_output: Tensor = None,\n","                src_mask: Tensor = None,\n","                trg_mask: Tensor = None,\n","                **kwargs):\n","        \"\"\"\n","        Transformer decoder forward pass.\n","\n","        :param trg_embed: embedded targets\n","        :param encoder_output: source representations\n","        :param encoder_hidden: unused\n","        :param src_mask:\n","        :param unroll_steps: unused\n","        :param hidden: unused\n","        :param trg_mask: to mask out target paddings\n","                         Note that a subsequent mask is applied here.\n","        :param kwargs:\n","        :return:\n","        \"\"\"\n","        assert trg_mask is not None, \"trg_mask required for Transformer\"\n","\n","        # add position encoding to word embedding\n","        x = self.pe(trg_embed)\n","        # Dropout if given\n","        x = self.emb_dropout(x)\n","\n","        padding_mask = trg_mask\n","        # Create subsequent mask for decoding\n","        sub_mask = subsequent_mask(\n","            trg_embed.size(1)).type_as(trg_mask)\n","\n","        # Apply each layer to the input\n","        for layer in self.layers:\n","            x = layer(x=x, memory=encoder_output,\n","                      src_mask=src_mask, trg_mask=sub_mask, padding_mask=padding_mask)\n","\n","        # Apply a layer normalisation\n","        x = self.layer_norm(x)\n","        # Output layer turns it back into vectors of size trg_size\n","        output = self.output_layer(x)\n","\n","        return output, x, None, None\n","\n","    def __repr__(self):\n","        return \"%s(num_layers=%r, num_heads=%r)\" % (\n","            self.__class__.__name__, len(self.layers),\n","            self.layers[0].trg_trg_att.num_heads)\n","\n","class Encoder(nn.Module):\n","    \"\"\"\n","    Base encoder class\n","    \"\"\"\n","    @property\n","    def output_size(self):\n","        \"\"\"\n","        Return the output size\n","\n","        :return:\n","        \"\"\"\n","        return self._output_size\n","\n","class TransformerEncoder(Encoder):\n","    \"\"\"\n","    Transformer Encoder\n","    \"\"\"\n","\n","    #pylint: disable=unused-argument\n","    def __init__(self,\n","                 hidden_size: int = 512,\n","                 ff_size: int = 2048,\n","                 num_layers: int = 8,\n","                 num_heads: int = 4,\n","                 dropout: float = 0.1,\n","                 emb_dropout: float = 0.1,\n","                 freeze: bool = False,\n","                 **kwargs):\n","        \"\"\"\n","        Initializes the Transformer.\n","        :param hidden_size: hidden size and size of embeddings\n","        :param ff_size: position-wise feed-forward layer size.\n","          (Typically this is 2*hidden_size.)\n","        :param num_layers: number of layers\n","        :param num_heads: number of heads for multi-headed attention\n","        :param dropout: dropout probability for Transformer layers\n","        :param emb_dropout: Is applied to the input (word embeddings).\n","        :param freeze: freeze the parameters of the encoder during training\n","        :param kwargs:\n","        \"\"\"\n","        super(TransformerEncoder, self).__init__()\n","\n","        # build all (num_layers) layers\n","        self.layers = nn.ModuleList([\n","            TransformerEncoderLayer(size=hidden_size, ff_size=ff_size,\n","                                    num_heads=num_heads, dropout=dropout)\n","            for _ in range(num_layers)])\n","\n","        self.layer_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n","        self.pe = PositionalEncoding(hidden_size)\n","        self.emb_dropout = nn.Dropout(p=emb_dropout)\n","        self._output_size = hidden_size\n","\n","        if freeze:\n","            freeze_params(self)\n","\n","    #pylint: disable=arguments-differ\n","    def forward(self,\n","                embed_src: Tensor,\n","                src_length: Tensor,\n","                mask: Tensor) -> (Tensor, Tensor):\n","        \"\"\"\n","        Pass the input (and mask) through each layer in turn.\n","        Applies a Transformer encoder to sequence of embeddings x.\n","        The input mini-batch x needs to be sorted by src length.\n","        x and mask should have the same dimensions [batch, time, dim].\n","\n","        :param embed_src: embedded src inputs,\n","            shape (batch_size, src_len, embed_size)\n","        :param src_length: length of src inputs\n","            (counting tokens before padding), shape (batch_size)\n","        :param mask: indicates padding areas (zeros where padding), shape\n","            (batch_size, src_len, embed_size)\n","        :return:\n","            - output: hidden states with\n","                shape (batch_size, max_length, directions*hidden),\n","            - hidden_concat: last hidden state with\n","                shape (batch_size, directions*hidden)\n","        \"\"\"\n","\n","        x = embed_src\n","        # Add position encoding to word embeddings\n","        x = self.pe(x)\n","        # Add Dropout\n","        x = self.emb_dropout(x)\n","\n","        # Apply each layer to the input\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","\n","        return self.layer_norm(x), None\n","\n","    def __repr__(self):\n","        return \"%s(num_layers=%r, num_heads=%r)\" % (\n","            self.__class__.__name__, len(self.layers),\n","            self.layers[0].src_src_att.num_heads)\n","\n","\n","class RegLoss(nn.Module):\n","    \"\"\"\n","    Regression Loss\n","    \"\"\"\n","\n","    def __init__(self, cfg, target_pad=0.0):\n","        super(RegLoss, self).__init__()\n","\n","        self.loss = cfg[\"training\"][\"loss\"].lower()\n","\n","        if self.loss == \"l1\":\n","            self.criterion = nn.L1Loss()\n","        elif self.loss == \"mse\":\n","            self.criterion = nn.MSELoss()\n","\n","        else:\n","            print(\"Loss not found - revert to default L1 loss\")\n","            self.criterion = nn.L1Loss()\n","\n","        model_cfg = cfg[\"model\"]\n","\n","        self.target_pad = target_pad\n","        self.loss_scale = model_cfg.get(\"loss_scale\", 1.0)\n","\n","    # pylint: disable=arguments-differ\n","    def forward(self, preds, targets):\n","\n","        loss_mask = (targets != self.target_pad)\n","       \n","        # Find the masked predictions and targets using loss mask\n","        preds_masked = preds * loss_mask\n","        targets_masked = targets * loss_mask\n","\n","        # Calculate loss just over the masked predictions\n","        loss = self.criterion(preds_masked, targets_masked)\n","\n","        # Multiply loss by the loss scale\n","        if self.loss_scale != 1.0:\n","            loss = loss * self.loss_scale\n","\n","        return loss\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a-aRdpmlTanv"},"source":["### Main Model"]},{"cell_type":"code","metadata":{"id":"N32V5ozAtFQx","executionInfo":{"status":"ok","timestamp":1606794710881,"user_tz":480,"elapsed":7183,"user":{"displayName":"Juyoung Lee","photoUrl":"","userId":"16438012406002097348"}}},"source":["class Model(nn.Module):\n","    \"\"\"\n","    Base Model class\n","    \"\"\"\n","\n","    def __init__(self,\n","                 encoder: Encoder,\n","                 decoder: Decoder,\n","                 src_embed: Embeddings,\n","                 trg_embed: Embeddings,\n","                 src_vocab: Vocabulary,\n","                 trg_vocab: Vocabulary,\n","                 cfg: dict,\n","                 in_trg_size: int,\n","                 out_trg_size: int,\n","                 ) -> None:\n","        \"\"\"\n","        Create a new encoder-decoder model\n","\n","        :param encoder: encoder\n","        :param decoder: decoder\n","        :param src_embed: source embedding\n","        :param trg_embed: target embedding\n","        :param src_vocab: source vocabulary\n","        :param trg_vocab: target vocabulary\n","        \"\"\"\n","        super(Model, self).__init__()\n","\n","        model_cfg = cfg[\"model\"]\n","\n","        self.src_embed = src_embed\n","        self.trg_embed = trg_embed\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_vocab = src_vocab\n","        self.trg_vocab = trg_vocab\n","        self.bos_index = self.src_vocab.stoi[BOS_TOKEN]\n","        self.pad_index = self.src_vocab.stoi[PAD_TOKEN]\n","        self.eos_index = self.src_vocab.stoi[EOS_TOKEN]\n","        self.target_pad = TARGET_PAD\n","\n","        self.use_cuda = cfg[\"training\"][\"use_cuda\"]\n","\n","        self.in_trg_size = in_trg_size\n","        self.out_trg_size = out_trg_size\n","        self.count_in = model_cfg.get(\"count_in\",True)\n","        # Just Counter\n","        self.just_count_in = model_cfg.get(\"just_count_in\",False)\n","        # Gaussian Noise\n","        self.gaussian_noise = model_cfg.get(\"gaussian_noise\",False)\n","        # Gaussian Noise\n","        if self.gaussian_noise:\n","            self.noise_rate = model_cfg.get(\"noise_rate\", 1.0)\n","\n","        # Future Prediction - predict for this many frames in the future\n","        self.future_prediction = model_cfg.get(\"future_prediction\", 0)\n","\n","    # pylint: disable=arguments-differ\n","    def forward(self,\n","                src: Tensor,\n","                trg_input: Tensor,\n","                src_mask: Tensor,\n","                src_lengths: Tensor,\n","                trg_mask: Tensor = None) -> (\n","        Tensor, Tensor, Tensor, Tensor):\n","        \"\"\"\n","        First encodes the source sentence.\n","        Then produces the target one word at a time.\n","\n","        :param src: source input\n","        :param trg_input: target input\n","        :param src_mask: source mask\n","        :param src_lengths: length of source inputs\n","        :param trg_mask: target mask\n","        :return: decoder outputs\n","        \"\"\"\n","\n","        # Encode the source sequence\n","        encoder_output, encoder_hidden = self.encode(src=src,\n","                                                     src_length=src_lengths,\n","                                                     src_mask=src_mask)\n","        unroll_steps = trg_input.size(1)\n","\n","        # Add gaussian noise to the target inputs, if in training\n","        if (self.gaussian_noise) and (self.training) and (self.out_stds is not None):\n","\n","            # Create a normal distribution of random numbers between 0-1\n","            noise = trg_input.data.new(trg_input.size()).normal_(0, 1)\n","            # Zero out the noise over the counter\n","            noise[:,:,-1] = torch.zeros_like(noise[:, :, -1])\n","\n","            # Need to add a zero on the end of\n","            if self.future_prediction != 0:\n","                self.out_stds = torch.cat((self.out_stds,torch.zeros_like(self.out_stds)))[:trg_input.shape[-1]]\n","\n","            # Need to multiply by the standard deviations\n","            noise = noise * self.out_stds\n","\n","            # Add to trg_input multiplied by the noise rate\n","            trg_input = trg_input + self.noise_rate*noise\n","\n","        # Decode the target sequence\n","        skel_out, dec_hidden, _, _ = self.decode(encoder_output=encoder_output,\n","                                                 src_mask=src_mask, trg_input=trg_input,\n","                                                 trg_mask=trg_mask)\n","\n","        gloss_out = None\n","\n","        return skel_out, gloss_out\n","\n","    def encode(self, src: Tensor, src_length: Tensor, src_mask: Tensor) \\\n","        -> (Tensor, Tensor):\n","        \"\"\"\n","        Encodes the source sentence.\n","\n","        :param src:\n","        :param src_length:\n","        :param src_mask:\n","        :return: encoder outputs (output, hidden_concat)\n","        \"\"\"\n","        # Encode an embedded source\n","        encode_output = self.encoder(self.src_embed(src), src_length, src_mask)\n","\n","        return encode_output\n","\n","\n","    def decode(self, encoder_output: Tensor,\n","               src_mask: Tensor, trg_input: Tensor,\n","               trg_mask: Tensor = None) \\\n","        -> (Tensor, Tensor, Tensor, Tensor):\n","\n","        \"\"\"\n","        Decode, given an encoded source sentence.\n","\n","        :param encoder_output: encoder states for attention computation\n","        :param encoder_hidden: last encoder state for decoder initialization\n","        :param src_mask: source mask, 1 at valid tokens\n","        :param trg_input: target inputs\n","        :param unroll_steps: number of steps to unrol the decoder for\n","        :param decoder_hidden: decoder hidden state (optional)\n","        :param trg_mask: mask for target steps\n","        :return: decoder outputs (outputs, hidden, att_probs, att_vectors)\n","        \"\"\"\n","\n","        # Enbed the target using a linear layer\n","        trg_embed = self.trg_embed(trg_input)\n","        # Apply decoder to the embedded target\n","        decoder_output = self.decoder(trg_embed=trg_embed, encoder_output=encoder_output,\n","                               src_mask=src_mask,trg_mask=trg_mask)\n","\n","        return decoder_output\n","\n","    def get_loss_for_batch(self, batch: Batch, loss_function: nn.Module):\n","        \"\"\"\n","        Compute non-normalized loss and number of tokens for a batch\n","\n","        :param batch: batch to compute loss for\n","        :param loss_function: loss function, computes for input and target\n","            a scalar loss for the complete batch\n","        :return: batch_loss: sum of losses over non-pad elements in the batch\n","        \"\"\"\n","        # Forward through the batch input\n","        skel_out, _ = self.forward(\n","            src=batch.src, trg_input=batch.trg_input,\n","            src_mask=batch.src_mask, src_lengths=batch.src_lengths,\n","            trg_mask=batch.trg_mask)\n","\n","        # compute batch loss using skel_out and the batch target\n","        # do it by weight\n","        torso_loss = loss_function(skel_out[:, :, 0:16], batch.trg[:, :, 0:16])\n","        hand_loss = loss_function(skel_out[:, :, 16:16+84], batch.trg[:, :, 16:16+84])\n","        face_loss = loss_function(skel_out[:, :, 16+84:], batch.trg[:, :, 16+84:])\n","        \n","        #batch_loss = loss_function(skel_out, batch.trg)\n","        batch_loss = torso_loss * 0.4 + hand_loss * 0.5 + face_loss * 0.1\n","\n","        # If gaussian noise, find the noise for the next epoch\n","        if self.gaussian_noise:\n","            # Calculate the difference between prediction and GT, to find STDs of error\n","            with torch.no_grad():\n","                noise = skel_out.detach() - batch.trg.detach()\n","\n","            if self.future_prediction != 0:\n","                # Cut to only the first frame prediction + add the counter\n","                noise = noise[:, :, :noise.shape[2] // (self.future_prediction)]\n","\n","        else:\n","            noise = None\n","\n","        dtw = 0 #calculate_dtw(batch.trg, skel_out)\n","        # return batch loss = sum over all elements in batch that are not pad\n","        return batch_loss, torso_loss.item(), hand_loss.item(), face_loss.item(), noise\n","\n","    def run_batch(self, batch: Batch, max_output_length: int,) -> (np.array, np.array):\n","        \"\"\"\n","        Get outputs and attentions scores for a given batch\n","\n","        :param batch: batch to generate hypotheses for\n","        :param max_output_length: maximum length of hypotheses\n","        :param beam_size: size of the beam for beam search, if 0 use greedy\n","        :param beam_alpha: alpha value for beam search\n","        :return: stacked_output: hypotheses for batch,\n","            stacked_attention_scores: attention scores for batch\n","        \"\"\"\n","        # First encode the batch, as this can be done in all one go\n","        encoder_output, encoder_hidden = self.encode(\n","            batch.src, batch.src_lengths,\n","            batch.src_mask)\n","\n","        # if maximum output length is not globally specified, adapt to src len\n","        if max_output_length is None:\n","            max_output_length = int(max(batch.src_lengths.cpu().numpy()) * 1.5)\n","\n","        # Then decode the batch separately, as needs to be done iteratively\n","        # greedy decoding\n","        stacked_output, stacked_attention_scores = greedy(\n","                encoder_output=encoder_output,\n","                src_mask=batch.src_mask,\n","                embed=self.trg_embed,\n","                decoder=self.decoder,\n","                trg_input=batch.trg_input,\n","                model=self)\n","\n","        return stacked_output, stacked_attention_scores\n","\n","    def __repr__(self) -> str:\n","        \"\"\"\n","        String representation: a description of encoder, decoder and embeddings\n","\n","        :return: string representation\n","        \"\"\"\n","        return \"%s(\\n\" \\\n","               \"\\tencoder=%s,\\n\" \\\n","               \"\\tdecoder=%s,\\n\" \\\n","               \"\\tsrc_embed=%s,\\n\" \\\n","               \"\\ttrg_embed=%s)\" % (self.__class__.__name__, self.encoder,\n","                   self.decoder, self.src_embed, self.trg_embed)\n","               \n","def build_model(cfg: dict = None,\n","                src_vocab = None,\n","                trg_vocab = None) -> Model:\n","    \"\"\"\n","    Build and initialize the model according to the configuration.\n","\n","    :param cfg: dictionary configuration containing model specifications\n","    :param src_vocab: source vocabulary\n","    :param trg_vocab: target vocabulary\n","    :return: built and initialized model\n","    \"\"\"\n","\n","    full_cfg = cfg\n","    cfg = cfg[\"model\"]\n","\n","    src_padding_idx = None\n","    trg_padding_idx = 0\n","\n","    # Input target size is the joint vector length plus one for counter\n","    in_trg_size = cfg[\"trg_size\"] + 1\n","    # Output target size is the joint vector length plus one for counter\n","    out_trg_size = cfg[\"trg_size\"] + 1\n","\n","    just_count_in = cfg.get(\"just_count_in\", False)\n","    future_prediction = cfg.get(\"future_prediction\", 0)\n","\n","    #  Just count in limits the in target size to 1\n","    if just_count_in:\n","        in_trg_size = 1\n","\n","    # Future Prediction increases the output target size\n","    if future_prediction != 0:\n","        # Times the trg_size (minus counter) by amount of predicted frames, and then add back counter\n","        out_trg_size = (out_trg_size - 1 ) * future_prediction + 1\n","\n","    # Define source embedding\n","\n","    with open(os.path.join(DATA_DIR, 'embedding.200.pkl') ,'rb') as f:\n","        embedding_dict = pickle.load(f)\n","    src_embed = Embeddings(\n","        embedding_dict=embedding_dict,\n","        **cfg[\"encoder\"][\"embeddings\"], vocab_size=len(src_vocab),\n","        padding_idx=src_padding_idx)\n","\n","    # Define target linear\n","    # Linear layer replaces an embedding layer - as this takes in the joints size as opposed to a token\n","    trg_linear = nn.Linear(in_trg_size, cfg[\"decoder\"][\"embeddings\"][\"embedding_dim\"])\n","\n","    ## Encoder -------\n","    enc_dropout = cfg[\"encoder\"].get(\"dropout\", 0.) # Dropout\n","    enc_emb_dropout = cfg[\"encoder\"][\"embeddings\"].get(\"dropout\", enc_dropout)\n","    assert cfg[\"encoder\"][\"embeddings\"][\"embedding_dim\"] == \\\n","           cfg[\"encoder\"][\"hidden_size\"], \\\n","           \"for transformer, emb_size must be hidden_size\"\n","\n","    # Transformer Encoder\n","    encoder = TransformerEncoder(**cfg[\"encoder\"],\n","                                 emb_size=src_embed.embedding_dim,\n","                                 emb_dropout=enc_emb_dropout)\n","\n","    ## Decoder -------\n","    dec_dropout = cfg[\"decoder\"].get(\"dropout\", 0.) # Dropout\n","    dec_emb_dropout = cfg[\"decoder\"][\"embeddings\"].get(\"dropout\", dec_dropout)\n","    decoder_trg_trg = cfg[\"decoder\"].get(\"decoder_trg_trg\", True)\n","    # Transformer Decoder\n","    decoder = TransformerDecoder(\n","        **cfg[\"decoder\"], encoder=encoder, vocab_size=len(trg_vocab),\n","        emb_size=trg_linear.out_features, emb_dropout=dec_emb_dropout,\n","        trg_size=out_trg_size, decoder_trg_trg_=decoder_trg_trg)\n","\n","    # Define the model\n","    model = Model(encoder=encoder,\n","                  decoder=decoder,\n","                  src_embed=src_embed,\n","                  trg_embed=trg_linear,\n","                  src_vocab=src_vocab,\n","                  trg_vocab=trg_vocab,\n","                  cfg=full_cfg,\n","                  in_trg_size=in_trg_size,\n","                  out_trg_size=out_trg_size)\n","\n","    # Custom initialization of model parameters\n","    initialize_model(model, cfg, src_padding_idx, trg_padding_idx)\n","\n","    return model"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6xnnemWTTdmc"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"wgalc53TTiWV"},"source":["### Main Train Call"]},{"cell_type":"code","metadata":{"id":"uPS0rTD2xRzq","executionInfo":{"status":"ok","timestamp":1606794887444,"user_tz":480,"elapsed":2053,"user":{"displayName":"Juyoung Lee","photoUrl":"","userId":"16438012406002097348"}}},"source":["class TrainManager:\n","\n","    def __init__(self, model: Model, config: dict, test=False) -> None:\n","\n","        train_config = config[\"training\"]\n","        model_dir = os.path.join(MODEL_DIR, version)\n","        # If model continue, continues model from the latest checkpoint\n","        model_continue = train_config.get(\"continue\", True)\n","        # If the directory has not been created, can't continue from anything\n","        if not os.path.isdir(model_dir):\n","            model_continue = False\n","        if test:\n","            model_continue = True\n","\n","        # files for logging and storing\n","        self.model_dir = model_dir\n","        \n","        # Build validation files\n","        self.valid_report_file = \"{}/validations.txt\".format(self.model_dir)\n","        self.logger = make_logger(model_dir=self.model_dir)\n","        self.logging_freq = train_config.get('logging_freq', 100)\n","\n","        # model\n","        self.model = model\n","        self.pad_index = self.model.pad_index\n","        self.bos_index = self.model.bos_index\n","        self._log_parameters_list()\n","        self.target_pad = TARGET_PAD\n","\n","        # New Regression loss - depending on config\n","        self.loss = RegLoss(cfg = config,\n","                            target_pad=self.target_pad)\n","\n","        self.normalization = \"batch\"\n","\n","        # optimization\n","        self.learning_rate_min = train_config.get(\"learning_rate_min\", 1.0e-8)\n","        self.clip_grad_fun = build_gradient_clipper(config=train_config)\n","        self.optimizer = build_optimizer(config=train_config, parameters=model.parameters())\n","\n","        # validation & early stopping\n","        self.validation_freq = train_config.get(\"validation_freq\", 1000)\n","        self.ckpt_best_queue = queue.Queue(maxsize=train_config.get(\"keep_last_ckpts\", 1))\n","        self.ckpt_queue = queue.Queue(maxsize=1)\n","\n","        self.val_on_train = config[\"data\"].get(\"val_on_train\", True)\n","\n","        # TODO - Include Back Translation\n","        self.eval_metric = train_config.get(\"eval_metric\", \"dtw\").lower()\n","        if self.eval_metric not in ['bleu', 'chrf', \"dtw\"]:\n","            raise ConfigurationError(\"Invalid setting for 'eval_metric', \"\n","                                     \"valid options: 'bleu', 'chrf', 'DTW'\")\n","        self.early_stopping_metric = train_config.get(\"early_stopping_metric\",\n","                                                       \"eval_metric\")\n","\n","        # if we schedule after BLEU/chrf, we want to maximize it, else minimize\n","        # early_stopping_metric decides on how to find the early stopping point:\n","        # ckpts are written when there's a new high/low score for this metric\n","        if self.early_stopping_metric in [\"loss\",\"dtw\"]:\n","            self.minimize_metric = True\n","        else:\n","            raise ConfigurationError(\"Invalid setting for 'early_stopping_metric', \"\n","                                    \"valid options: 'loss', 'dtw',.\")\n","\n","        # learning rate scheduling\n","        self.scheduler, self.scheduler_step_at = build_scheduler(\n","            config=train_config,\n","            scheduler_mode=\"min\" if self.minimize_metric else \"max\",\n","            optimizer=self.optimizer,\n","            hidden_size=config[\"model\"][\"encoder\"][\"hidden_size\"])\n","\n","        # data & batch handling\n","        self.level = \"word\"\n","        self.shuffle = train_config.get(\"shuffle\", True)\n","        self.epochs = train_config.get('epochs')\n","        self.batch_size = train_config[\"batch_size\"]\n","        self.batch_type = \"sentence\"\n","        self.eval_batch_size = train_config.get(\"eval_batch_size\",self.batch_size)\n","        self.eval_batch_type = train_config.get(\"eval_batch_type\",self.batch_type)\n","        self.batch_multiplier = train_config.get(\"batch_multiplier\", 1)\n","\n","        # generation\n","        self.max_output_length = train_config.get(\"max_output_length\", None)\n","\n","        # CPU / GPU\n","        self.use_cuda = train_config[\"use_cuda\"]\n","        if self.use_cuda:\n","            self.model.cuda()\n","            self.loss.cuda()\n","\n","        # initialize training statistics\n","        self.steps = 0\n","        # stop training if this flag is True by reaching learning rate minimum\n","        self.stop = False\n","        self.total_tokens = 0\n","        self.best_ckpt_iteration = 0\n","        # initial values for best scores\n","        self.best_ckpt_score = np.inf if self.minimize_metric else -np.inf\n","        # comparison function for scores\n","        self.is_best = lambda score: score < self.best_ckpt_score \\\n","            if self.minimize_metric else score > self.best_ckpt_score\n","\n","        ## Checkpoint restart\n","        # If continuing\n","        model_continue = False\n","        if model_continue:\n","            # Get the latest checkpoint\n","            ckpt = get_latest_checkpoint(model_dir)\n","            if ckpt is None:\n","                self.logger.info(f\"Can't find checkpoint in directory {ckpt}\")\n","            else:\n","                self.logger.info(f\"Continuing model from {ckpt}\", )\n","                self.init_from_checkpoint(ckpt)\n","\n","        # Skip frames\n","        self.skip_frames = config[\"data\"].get(\"skip_frames\", 1)\n","\n","        ## -- Data augmentation --\n","        # Just Counter\n","        self.just_count_in = config[\"model\"].get(\"just_count_in\",False)\n","        # Gaussian Noise\n","        self.gaussian_noise = config[\"model\"].get(\"gaussian_noise\", False)\n","        \n","        if self.gaussian_noise:\n","            # How much the noise is added in\n","            self.noise_rate = config[\"model\"].get(\"noise_rate\", 1.0)\n","\n","        if self.just_count_in and (self.gaussian_noise):\n","            raise ConfigurationError(\"Can't have both just_count_in and gaussian_noise as True\")\n","\n","        self.future_prediction = config[\"model\"].get(\"future_prediction\", 0)\n","        if self.future_prediction != 0:\n","            frames_predicted = [i for i in range(self.future_prediction)]\n","            print(f\"Future prediction. Frames predicted: {frames_predicted}\")\n","\n","    # Save a checkpoint\n","    def _save_checkpoint(self, type=\"every\") -> None:\n","        # Define model path\n","        model_path = \"{}/{}_{}.ckpt\".format(self.model_dir, self.steps, type)\n","        # Define State\n","        state = {\n","            \"steps\": self.steps,\n","            \"total_tokens\": self.total_tokens,\n","            \"best_ckpt_score\": self.best_ckpt_score,\n","            \"best_ckpt_iteration\": self.best_ckpt_iteration,\n","            \"model_state\": self.model.state_dict(),\n","            \"optimizer_state\": self.optimizer.state_dict(),\n","            \"scheduler_state\": self.scheduler.state_dict() if \\\n","            self.scheduler is not None else None,\n","        }\n","        torch.save(state, model_path)\n","        # If this is the best checkpoint\n","        if type == \"best\":\n","            if self.ckpt_best_queue.full():\n","              to_delete = self.ckpt_best_queue.get()  # delete oldest ckpt\n","              try:\n","                os.remove(to_delete)\n","              except FileNotFoundError:\n","                print(f\"Wanted to delete old checkpoint {to_delete} but file does not exist.\")\n","            self.ckpt_best_queue.put(model_path)\n","\n","            best_path = \"{}/best.ckpt\".format(self.model_dir)\n","            torch.save(state, best_path)\n","\n","        # If this is just the checkpoint at every validation\n","        elif type == \"every\":\n","            if self.ckpt_queue.full():\n","              to_delete = self.ckpt_queue.get()  # delete oldest ckpt\n","              try:\n","                os.remove(to_delete)\n","              except FileNotFoundError:\n","                print(f\"Wanted to delete old checkpoint {to_delete} but file does not exist.\")\n","\n","            self.ckpt_queue.put(model_path)\n","            every_path = \"{}/every.ckpt\".format(self.model_dir)\n","            # overwrite every.ckpt\n","            torch.save(state, every_path)\n","\n","    # Initialise from a checkpoint\n","    def init_from_checkpoint(self, path: str) -> None:\n","        # Find last checkpoint\n","        model_checkpoint = load_checkpoint(path=path, use_cuda=self.use_cuda)\n","\n","        # restore model and optimizer parameters\n","        self.model.load_state_dict(model_checkpoint[\"model_state\"])\n","        self.optimizer.load_state_dict(model_checkpoint[\"optimizer_state\"])\n","\n","        if model_checkpoint[\"scheduler_state\"] is not None and \\\n","                self.scheduler is not None:\n","            # Load the scheduler state\n","            self.scheduler.load_state_dict(model_checkpoint[\"scheduler_state\"])\n","\n","        # restore counts\n","        self.steps = model_checkpoint[\"steps\"]\n","        self.total_tokens = model_checkpoint[\"total_tokens\"]\n","        self.best_ckpt_score = model_checkpoint[\"best_ckpt_score\"]\n","        self.best_ckpt_iteration = model_checkpoint[\"best_ckpt_iteration\"]\n","\n","        # move parameters to cuda\n","        if self.use_cuda:\n","            self.model.cuda()\n","\n","    # Train and validate function\n","    def train_and_validate(self, train_data: Dataset, valid_data: Dataset) \\\n","            -> None:\n","        # Make training iterator\n","        train_iter = make_data_iter(train_data,\n","                                    batch_size=self.batch_size,\n","                                    batch_type=self.batch_type,\n","                                    train=True, shuffle=self.shuffle)\n","\n","        val_step = 0\n","        if self.gaussian_noise:\n","            all_epoch_noise = []\n","        # Loop through epochs\n","        epoch_start_time = time.time()\n","        for epoch_no in range(self.epochs):\n","            if self.scheduler is not None and self.scheduler_step_at == \"epoch\":\n","                self.scheduler.step(epoch=epoch_no)\n","\n","            self.model.train()\n","\n","            # Reset statistics for each epoch.\n","            start = time.time()\n","            total_valid_duration = 0\n","            start_tokens = self.total_tokens\n","            count = self.batch_multiplier - 1\n","            epoch_loss = 0\n","            epoch_torso_loss = 0\n","            epoch_hand_loss = 0\n","            epoch_face_loss = 0\n","\n","            # If Gaussian Noise, extract STDs for each joint position\n","            if self.gaussian_noise:\n","                if len(all_epoch_noise) != 0:\n","                    self.model.out_stds = torch.mean(torch.stack(([noise.std(dim=[0]) for noise in all_epoch_noise])),dim=-2)\n","                else:\n","                    self.model.out_stds = None\n","                all_epoch_noise = []\n","\n","            for batch in iter(train_iter):\n","                # reactivate training\n","                self.model.train()\n","\n","                # create a Batch object from torchtext batch\n","                batch = Batch(torch_batch=batch,\n","                              pad_index=self.pad_index,\n","                              model=self.model)\n","\n","                update = count == 0\n","                # Train the model on a batch\n","                batch_loss, torso_loss, hand_loss, face_loss, noise = self._train_batch(batch, update=update)\n","                # If Gaussian Noise, collect the noise\n","                if self.gaussian_noise:\n","                    # If future Prediction, cut down the noise size to just one frame\n","                    if self.future_prediction != 0:\n","                        all_epoch_noise.append(noise.reshape(-1, self.model.out_trg_size // self.future_prediction))\n","                    else:\n","                        all_epoch_noise.append(noise.reshape(-1,self.model.out_trg_size))\n","\n","                count = self.batch_multiplier if update else count\n","                count -= 1\n","                epoch_loss += batch_loss.detach().cpu().numpy()\n","                epoch_torso_loss += torso_loss\n","                epoch_hand_loss += hand_loss\n","                epoch_face_loss += face_loss\n","\n","                if self.scheduler is not None and self.scheduler_step_at == \"step\" and update:\n","                    self.scheduler.step()\n","\n","                # log learning progress\n","                if self.steps % self.logging_freq == 0 and update:\n","                    elapsed = time.time() - start - total_valid_duration\n","                    elapsed_tokens = self.total_tokens - start_tokens\n","                    self.logger.info(\n","                        \"Epoch %3d Step: %8d Batch Loss: %12.6f [Torso : %12.6f, Hand : %12.6f, Face : %12.6f]\"\n","                        \"Tokens per Sec: %8.0f, Lr: %.6f\",\n","                        epoch_no + 1, self.steps, batch_loss, torso_loss, hand_loss, face_loss,\n","                        elapsed_tokens / elapsed,\n","                        self.optimizer.param_groups[0][\"lr\"])\n","                    start = time.time()\n","                    total_valid_duration = 0\n","                    start_tokens = self.total_tokens\n","\n","                # validate on the entire dev set\n","                if self.steps % self.validation_freq == 0 and update:\n","                    self.logger.info(\"Starting validation calculation.\")\n","                    valid_start_time = time.time()\n","\n","                    valid_score, valid_loss, valid_references, valid_hypotheses, \\\n","                        valid_inputs, all_dtw_scores, valid_file_paths = \\\n","                        validate_on_data(\n","                            batch_size=self.eval_batch_size,\n","                            data=valid_data,\n","                            eval_metric=self.eval_metric,\n","                            model=self.model,\n","                            max_output_length=self.max_output_length,\n","                            loss_function=self.loss,\n","                            batch_type=self.eval_batch_type,\n","                            type=\"val\",\n","                        )\n","\n","                    val_step += 1\n","\n","                    if self.early_stopping_metric == \"loss\":\n","                        ckpt_score = valid_loss\n","                    elif self.early_stopping_metric == \"dtw\":\n","                        ckpt_score = valid_score\n","                    else:\n","                        ckpt_score = valid_score\n","\n","                    new_best = False\n","                    self.best = False\n","                    if self.is_best(ckpt_score):\n","                        self.best = True\n","                        self.best_ckpt_score = ckpt_score\n","                        self.best_ckpt_iteration = self.steps\n","                        self.logger.info(\n","                            'Hooray! New best validation result [%s]!',\n","                            self.early_stopping_metric)\n","                        if self.ckpt_queue.maxsize > 0:\n","                            self.logger.info(\"Saving new checkpoint.\")\n","                            new_best = True\n","                            self._save_checkpoint(type=\"best\")\n","\n","                        # Display these sequences, in this index order\n","                        display = list(range(0, len(valid_hypotheses), int(np.ceil(len(valid_hypotheses) / 13.15))))\n","                        self.produce_validation_video(\n","                            output_joints=valid_hypotheses,\n","                            inputs=valid_inputs,\n","                            references=valid_references,\n","                            model_dir=self.model_dir,\n","                            steps=self.steps,\n","                            display=display,\n","                            type=\"val_inf\",\n","                            file_paths=valid_file_paths,\n","                        )\n","\n","                    self._save_checkpoint(type=\"every\")\n","\n","                    if self.scheduler is not None and self.scheduler_step_at == \"validation\":\n","                        self.scheduler.step(ckpt_score)\n","\n","                    # append to validation report\n","                    self._add_report(\n","                        valid_score=valid_score, valid_loss=valid_loss,\n","                        eval_metric=self.eval_metric,\n","                        new_best=new_best, report_type=\"val\",)\n","\n","                    valid_duration = time.time() - valid_start_time\n","                    total_valid_duration += valid_duration\n","                    self.logger.info(\n","                        'Validation result at epoch %3d, step %8d: Val DTW Score: %6.2f, '\n","                        'loss: %8.4f,  duration: %.4fs',\n","                            epoch_no+1, self.steps, valid_score,\n","                            valid_loss, valid_duration)\n","\n","                if self.stop:\n","                    break\n","            if self.stop:\n","                self.logger.info(\n","                    'Training ended since minimum lr %f was reached.',\n","                     self.learning_rate_min)\n","                break\n","\n","            self.logger.info('Epoch %3d: total training loss %.5f [torso: %.5f, hand: %.5f, face: %.5f', epoch_no+1,\n","                             epoch_loss, epoch_torso_loss, epoch_hand_loss, epoch_face_loss)\n","        else:\n","            self.logger.info('Training ended after %3d epochs.', epoch_no+1)\n","        self.logger.info('Best validation result at step %8d: %6.2f %s.',\n","                         self.best_ckpt_iteration, self.best_ckpt_score,\n","                         self.early_stopping_metric)\n","\n","\n","    # Train the batch\n","    def _train_batch(self, batch: Batch, update: bool = True):\n","\n","        # Get loss from this batch\n","        batch_loss, torso_loss, hand_loss, face_loss, noise = self.model.get_loss_for_batch(\n","            batch=batch, loss_function=self.loss)\n","\n","        # normalize batch loss\n","        if self.normalization == \"batch\":\n","            normalizer = batch.nseqs\n","        elif self.normalization == \"tokens\":\n","            normalizer = batch.ntokens\n","        else:\n","            raise NotImplementedError(\"Only normalize by 'batch' or 'tokens'\")\n","\n","        norm_batch_loss = batch_loss / normalizer\n","        norm_torso_loss = torso_loss / normalizer\n","        norm_hand_loss = hand_loss / normalizer\n","        norm_face_loss = face_loss /normalizer\n","        # division needed since loss.backward sums the gradients until updated\n","        norm_batch_multiply = norm_batch_loss / self.batch_multiplier\n","\n","        # compute gradients\n","        norm_batch_multiply.backward()\n","\n","        if self.clip_grad_fun is not None:\n","            # clip gradients (in-place)\n","            self.clip_grad_fun(params=self.model.parameters())\n","\n","        if update:\n","            # make gradient step\n","            self.optimizer.step()\n","            self.optimizer.zero_grad()\n","\n","            # increment step counter\n","            self.steps += 1\n","\n","        # increment token counter\n","        self.total_tokens += batch.ntokens\n","\n","        return norm_batch_loss, norm_torso_loss, norm_hand_loss, norm_face_loss, noise\n","\n","    def _add_report(self, valid_score: float, valid_loss: float, eval_metric: str,\n","                    new_best: bool = False, report_type: str = \"val\") -> None:\n","\n","        current_lr = -1\n","        # ignores other param groups for now\n","        for param_group in self.optimizer.param_groups:\n","            current_lr = param_group['lr']\n","\n","        if current_lr < self.learning_rate_min:\n","            self.stop = True\n","\n","        if report_type == \"val\":\n","            with open(self.valid_report_file, 'a') as opened_file:\n","                opened_file.write(\n","                    \"Steps: {} Loss: {:.5f}| DTW: {:.3f}|\"\n","                    \" LR: {:.6f} {}\\n\".format(\n","                        self.steps, valid_loss, valid_score,\n","                        current_lr, \"*\" if new_best else \"\"))\n","\n","    def _log_parameters_list(self) -> None:\n","        \"\"\"\n","        Write all model parameters (name, shape) to the log.\n","        \"\"\"\n","        model_parameters = filter(lambda p: p.requires_grad,\n","                                  self.model.parameters())\n","        n_params = sum([np.prod(p.size()) for p in model_parameters])\n","        self.logger.info(\"Total params: %d\", n_params)\n","        trainable_params = [n for (n, p) in self.model.named_parameters()\n","                            if p.requires_grad]\n","        self.logger.info(\"Trainable parameters: %s\", sorted(trainable_params))\n","        assert trainable_params\n","        \n","    # Produce the video of Phoenix MTC joints\n","    def produce_validation_video(self,output_joints, inputs, references, display, model_dir, type, steps=\"\", file_paths=None):\n","\n","        # If not at test\n","        if type != \"test\":\n","            dir_name = model_dir + \"/videos/Step_{}/\".format(steps)\n","            if not os.path.exists(model_dir + \"/videos/\"):\n","                os.mkdir(model_dir + \"/videos/\")\n","\n","        # If at test time\n","        elif type == \"test\":\n","            dir_name = model_dir + \"/test_videos/\"\n","\n","        # Create model video folder if not exist\n","        if not os.path.exists(dir_name):\n","            os.mkdir(dir_name)\n","        # For sequence to display\n","\n","        for i in display:\n","\n","            seq = output_joints[i]\n","            ref_seq = references[i]\n","            input = inputs[i]\n","            # Write gloss label\n","            gloss_label = input[0] # [\"word\"]\n","\n","\n","            # Alter the dtw timing of the produced sequence, and collect the DTW score\n","            timing_hyp_seq, ref_seq_count, dtw_score = alter_DTW_timing(seq, ref_seq)\n","            video_ext = \"{}_{}.mp4\".format(gloss_label, \"{0:.2f}\".format(float(dtw_score)).replace(\".\", \"_\"))\n","\n","            try :\n","              if file_paths is not None:\n","                  sequence_ID = file_paths[i]\n","              else:\n","                  sequence_ID = None\n","            except:\n","              sequence_ID = None\n","        \n","            # Plot this sequences video\n","            if \"<\" not in video_ext:\n","                plot_video(joints=timing_hyp_seq,\n","                            file_path=dir_name,\n","                            video_name=video_ext,\n","                            references=ref_seq_count,\n","                            skip_frames=self.skip_frames,\n","                            sequence_ID=gloss_label)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KMqh4f17vW5p"},"source":["#### Train Helper Methods"]},{"cell_type":"code","metadata":{"id":"OejOwbr8vYx-","executionInfo":{"status":"ok","timestamp":1606794712707,"user_tz":480,"elapsed":8126,"user":{"displayName":"Juyoung Lee","photoUrl":"","userId":"16438012406002097348"}}},"source":["\n","                \n","def greedy(\n","        src_mask: Tensor,\n","        embed: Embeddings,\n","        decoder: Decoder,\n","        encoder_output: Tensor,\n","        trg_input: Tensor,\n","        model,\n","        ) -> (np.array, np.array):\n","    \"\"\"\n","    Special greedy function for transformer, since it works differently.\n","    The transformer remembers all previous states and attends to them.\n","\n","    :param src_mask: mask for source inputs, 0 for positions after </s>\n","    :param embed: target embedding\n","    :param bos_index: index of <s> in the vocabulary\n","    :param max_output_length: maximum length for the hypotheses\n","    :param decoder: decoder to use for greedy decoding\n","    :param encoder_output: encoder hidden states for attention\n","    :param encoder_hidden: encoder final state (unused in Transformer)\n","    :return:\n","        - stacked_output: output hypotheses (2d array of indices),\n","        - stacked_attention_scores: attention scores (3d array)\n","    \"\"\"\n","    # Initialise the input\n","    # Extract just the BOS first frame from the target\n","    ys = trg_input[:,:1,:].float()\n","\n","    # If the counter is coming into the decoder or not\n","    ys_out = ys\n","\n","    # Set the target mask, by finding the padded rows\n","    trg_mask = trg_input != 0.0\n","    trg_mask = trg_mask.unsqueeze(1)\n","\n","    # Find the maximum output length for this batch\n","    max_output_length = trg_input.shape[1]\n","\n","    # If just count in, input is just the counter\n","    if model.just_count_in:\n","        ys = ys[:,:,-1:]\n","\n","    for i in range(max_output_length):\n","\n","        # ys here is the input\n","        # Drive the timing by giving the GT timing - add in the counter to the last column\n","\n","        if model.just_count_in:\n","            # If just counter, drive the input using the GT counter\n","            ys[:,-1] = trg_input[:, i, -1:]\n","\n","        else:\n","            # Give the GT counter for timing, to drive the timing\n","            ys[:,-1,-1:] = trg_input[:, i, -1:]\n","\n","        # Embed the target input before passing to the decoder\n","        trg_embed = embed(ys)\n","\n","        # Cut padding mask to required size (of the size of the input)\n","        padding_mask = trg_mask[:, :, :i+1, :i+1]\n","        # Pad the mask (If required) (To make it square, and used later on correctly)\n","        pad_amount = padding_mask.shape[2] - padding_mask.shape[3]\n","        padding_mask = (F.pad(input=padding_mask.double(), pad=(pad_amount, 0, 0, 0), mode='replicate') == 1.0)\n","\n","        # Pass the embedded input and the encoder output into the decoder\n","        with torch.no_grad():\n","            out, _, _, _ = decoder(\n","                trg_embed=trg_embed,\n","                encoder_output=encoder_output,\n","                src_mask=src_mask,\n","                trg_mask=padding_mask,\n","            )\n","\n","            if model.future_prediction != 0:\n","                # Cut to only the first frame prediction\n","                out = torch.cat((out[:, :, :out.shape[2] // (model.future_prediction)],out[:,:,-1:]),dim=2)\n","\n","            if model.just_count_in:\n","                # If just counter in trg_input, concatenate counters of output\n","                ys = torch.cat([ys, out[:,-1:,-1:]], dim=1)\n","\n","            # Add this frame prediction to the overall prediction\n","            ys = torch.cat([ys, out[:,-1:,:]], dim=1)\n","\n","            # Add this next predicted frame to the full frame output\n","            ys_out = torch.cat([ys_out, out[:,-1:,:]], dim=1)\n","\n","    return ys_out, None\n","\n","\n","# Find the best timing match between a reference and a hypothesis, using DTW\n","def calculate_dtw(references, hypotheses):\n","    \"\"\"\n","    Calculate the DTW costs between a list of references and hypotheses\n","\n","    :param references: list of reference sequences to compare against\n","    :param hypotheses: list of hypothesis sequences to fit onto the reference\n","\n","    :return: dtw_scores: list of DTW costs\n","    \"\"\"\n","    # Euclidean norm is the cost function, difference of coordinates\n","    euclidean_norm = lambda x, y: np.sum(np.abs(x - y))\n","\n","    dtw_scores = []\n","\n","    # Remove the BOS frame from the hypothesis\n","    hypotheses = hypotheses[:, 1:]\n","\n","    # For each reference in the references list\n","    for i, ref in enumerate(references):\n","        # Cut the reference down to the max count value\n","        _ , ref_max_idx = torch.max(ref[:, -1], 0)\n","        if ref_max_idx == 0: ref_max_idx += 1\n","        # Cut down frames by to the max counter value, and chop off counter from joints\n","        ref_count = ref[:ref_max_idx,:-1].cpu().numpy()\n","\n","        # Cut the hypothesis down to the max count value\n","        hyp = hypotheses[i]\n","        _, hyp_max_idx = torch.max(hyp[:, -1], 0)\n","        if hyp_max_idx == 0: hyp_max_idx += 1\n","        # Cut down frames by to the max counter value, and chop off counter from joints\n","        hyp_count = hyp[:hyp_max_idx,:-1].cpu().numpy()\n","\n","        # Calculate DTW of the reference and hypothesis, using euclidean norm\n","        d, cost_matrix, acc_cost_matrix, path = dtw(ref_count, hyp_count, dist=euclidean_norm)\n","\n","        # Normalise the dtw cost by sequence length\n","        d = d/acc_cost_matrix.shape[0]\n","\n","        dtw_scores.append(d)\n","\n","    # Return dtw scores and the hypothesis with altered timing\n","    return dtw_scores\n","\n","# Validate epoch given a dataset\n","def validate_on_data(model: Model,\n","                     data: Dataset,\n","                     batch_size: int,\n","                     max_output_length: int,\n","                     eval_metric: str,\n","                     loss_function: torch.nn.Module = None,\n","                     batch_type: str = \"sentence\",\n","                     type = \"val\",\n","                     BT_model = None):\n","\n","    valid_iter = make_data_iter(\n","        dataset=data, batch_size=batch_size, batch_type=batch_type,\n","        shuffle=True, train=False)\n","\n","    pad_index = model.src_vocab.stoi[PAD_TOKEN]\n","    # disable dropout\n","    model.eval()\n","    # don't track gradients during validation\n","    with torch.no_grad():\n","        valid_hypotheses = []\n","        valid_references = []\n","        valid_inputs = []\n","        file_paths = []\n","        all_dtw_scores = []\n","\n","        valid_loss = 0\n","        total_ntokens = 0\n","        total_nseqs = 0\n","\n","        batches = 0\n","        for valid_batch in iter(valid_iter):\n","            # Extract batch\n","            batch = Batch(torch_batch=valid_batch,\n","                          pad_index = pad_index,\n","                          model = model)\n","            targets = batch.trg\n","\n","            # run as during training with teacher forcing\n","            if loss_function is not None and batch.trg is not None:\n","                # Get the loss for this batch\n","                batch_loss, _, _, _, _, = model.get_loss_for_batch(\n","                    batch, loss_function=loss_function)\n","\n","                valid_loss += batch_loss\n","                total_ntokens += batch.ntokens\n","                total_nseqs += batch.nseqs\n","\n","            # If not just count in, run inference to produce translation videos\n","            if not model.just_count_in:\n","                # Run batch through the model in an auto-regressive format\n","                output, attention_scores = model.run_batch(\n","                                            batch=batch,\n","                                            max_output_length=max_output_length)\n","\n","            # If future prediction\n","            if model.future_prediction != 0:\n","                # Cut to only the first frame prediction + add the counter\n","                # output = torch.cat((output[:, :, :output.shape[2] // (model.future_prediction)], output[:, :, -1:]),dim=2)\n","                # Cut to only the first frame prediction + add the counter\n","                targets = torch.cat((targets[:, :, :targets.shape[2] // (model.future_prediction)], targets[:, :, -1:]),dim=2)\n","\n","            # For just counter, the inference is the same as GTing\n","            if model.just_count_in:\n","                output = train_output\n","\n","            # Add references, hypotheses and file paths to list\n","            valid_references.extend(targets)\n","            valid_hypotheses.extend(output)\n","            #file_paths.extend(batch.file_paths)\n","            # Add the source sentences to list, by using the model source vocab and batch indices\n","            valid_inputs.extend([[model.src_vocab.itos[batch.emb[i][j]] for j in range(len(batch.emb[i]))] for i in\n","                                 range(len(batch.emb))])\n","\n","            # Calculate the full Dynamic Time Warping score - for evaluation\n","            dtw_score = calculate_dtw(targets, output)\n","            all_dtw_scores.extend(dtw_score)\n","\n","            # Can set to only run a few batches\n","            # if batches == math.ceil(100/batch_size):\n","            #     break\n","            batches += 1\n","\n","        # Dynamic Time Warping scores\n","        current_valid_score = np.mean(all_dtw_scores)\n","\n","    return current_valid_score, valid_loss, valid_references, valid_hypotheses, \\\n","           valid_inputs, all_dtw_scores, file_paths"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HFFCa2koFv26"},"source":["### Plotting Videos"]},{"cell_type":"code","metadata":{"id":"4OEWsHWPFuVk","executionInfo":{"status":"ok","timestamp":1606794713633,"user_tz":480,"elapsed":7961,"user":{"displayName":"Juyoung Lee","photoUrl":"","userId":"16438012406002097348"}}},"source":["import sys\n","import math\n","import numpy as np\n","import cv2\n","import torch\n","from numpy import array, zeros, full, argmin, inf, ndim\n","from scipy.spatial.distance import cdist\n","from math import isinf\n","\n","PAD_TOKEN = '<pad>'\n","\n","def dtw(x, y, dist, warp=1, w=inf, s=1.0):\n","    \"\"\"\n","    Computes Dynamic Time Warping (DTW) of two sequences.\n","\n","    :param array x: N1*M array\n","    :param array y: N2*M array\n","    :param func dist: distance used as cost measure\n","    :param int warp: how many shifts are computed.\n","    :param int w: window size limiting the maximal distance between indices of matched entries |i,j|.\n","    :param float s: weight applied on off-diagonal moves of the path. As s gets larger, the warping path is increasingly biased towards the diagonal\n","    Returns the minimum distance, the cost matrix, the accumulated cost matrix, and the wrap path.\n","    \"\"\"\n","    assert len(x)\n","    assert len(y)\n","    assert isinf(w) or (w >= abs(len(x) - len(y)))\n","    assert s > 0\n","    r, c = len(x), len(y)\n","    if not isinf(w):\n","        D0 = full((r + 1, c + 1), inf)\n","        for i in range(1, r + 1):\n","            D0[i, max(1, i - w):min(c + 1, i + w + 1)] = 0\n","        D0[0, 0] = 0\n","    else:\n","        D0 = zeros((r + 1, c + 1))\n","        D0[0, 1:] = inf\n","        D0[1:, 0] = inf\n","    D1 = D0[1:, 1:]  # view\n","    for i in range(r):\n","        for j in range(c):\n","            if (isinf(w) or (max(0, i - w) <= j <= min(c, i + w))):\n","                D1[i, j] = dist(x[i], y[j])\n","    C = D1.copy()\n","    jrange = range(c)\n","    for i in range(r):\n","        if not isinf(w):\n","            jrange = range(max(0, i - w), min(c, i + w + 1))\n","        for j in jrange:\n","            min_list = [D0[i, j]]\n","            for k in range(1, warp + 1):\n","                i_k = min(i + k, r)\n","                j_k = min(j + k, c)\n","                min_list += [D0[i_k, j] * s, D0[i, j_k] * s]\n","            D1[i, j] += min(min_list)\n","    if len(x) == 1:\n","        path = zeros(len(y)), range(len(y))\n","    elif len(y) == 1:\n","        path = range(len(x)), zeros(len(x))\n","    else:\n","        path = _traceback(D0)\n","    return D1[-1, -1], C, D1, path\n","\n","def _traceback(D):\n","    i, j = array(D.shape) - 2\n","    p, q = [i], [j]\n","    while (i > 0) or (j > 0):\n","        tb = argmin((D[i, j], D[i, j + 1], D[i + 1, j]))\n","        if tb == 0:\n","            i -= 1\n","            j -= 1\n","        elif tb == 1:\n","            i -= 1\n","        else:  # (tb == 2):\n","            j -= 1\n","        p.insert(0, i)\n","        q.insert(0, j)\n","    return array(p), array(q)\n","\n","\n","# Plot a video given a tensor of joints, a file path, video name and references/sequence ID\n","def plot_video(joints,\n","               file_path,\n","               video_name,\n","               references=None,\n","               skip_frames=1,\n","               sequence_ID=None):\n","    # Create video template\n","    FPS = (25 // skip_frames)\n","    video_file = file_path + \"/{}.mp4\".format(video_name.split(\".\")[0])\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","\n","    if references is None:\n","        video = cv2.VideoWriter(video_file, fourcc, float(FPS), (650, 650), True)\n","    elif references is not None:\n","        video = cv2.VideoWriter(video_file, fourcc, float(FPS), (1300, 650), True)  # Long\n","\n","    num_frames = 0\n","\n","    for (j, frame_joints) in enumerate(joints):\n","\n","        # Reached padding\n","        if PAD_TOKEN in frame_joints:\n","            continue\n","\n","        # Initialise frame of white\n","        frame = np.ones((650, 650, 3), np.uint8) * 255\n","\n","        # Cut off the percent_tok, multiply by 3 to restore joint size\n","        # TODO - Remove the *3 if the joints weren't divided by 3 in data creation\n","        \n","        frame_joints = frame_joints[:-1]\n","        # Reduce the frame joints down to 2D for visualisation - Frame joints 2d shape is (48,2)\n","        frame_joints_2d = np.reshape(frame_joints, (-1, 2))\n","        \n","        # Draw the frame given 2D joints\n","        draw_frame_2D(frame, frame_joints_2d)\n","\n","        cv2.putText(frame, \"Predicted Sign Pose\", (180, 600), cv2.FONT_HERSHEY_SIMPLEX, 1,\n","                    (0, 0, 255), 2)\n","\n","        # If reference is provided, create and concatenate on the end\n","        if references is not None:\n","            # Extract the reference joints\n","            ref_joints = references[j]\n","            # Initialise frame of white\n","            ref_frame = np.ones((650, 650, 3), np.uint8) * 255\n","\n","            # Cut off the percent_tok and multiply each joint by 3 (as was reduced in training files)\n","            #ref_joints = ref_joints[:-1] * 3\n","            ref_joints = ref_joints[:-1]\n","            \n","            # Reduce the frame joints down to 2D- Frame joints 2d shape is (48,2)\n","            # ref_joints_2d = np.reshape(ref_joints, (50, 3))[:, :2]\n","            ref_joints_2d = np.reshape(ref_joints, (-1, 2))\n","\n","            # Draw these joints on the frame\n","            draw_frame_2D(ref_frame, ref_joints_2d)\n","\n","            cv2.putText(ref_frame, \"Ground Truth Pose\", (190, 600), cv2.FONT_HERSHEY_SIMPLEX, 1,\n","                        (0, 0, 0), 2)\n","\n","            frame = np.concatenate((frame, ref_frame), axis=1)\n","\n","            sequence_ID_write = \"Sequence ID: \" + sequence_ID\n","            cv2.putText(frame, sequence_ID_write, (700, 635), cv2.FONT_HERSHEY_SIMPLEX, 0.6,\n","                        (0, 0, 0), 2)\n","        # Write the video frame\n","        video.write(frame)\n","        num_frames += 1\n","    # Release the video\n","    video.release()\n","\n","# This is the format of the 3D data, outputted from the Inverse Kinematics model\n","def getSkeletalModelStructure():\n","    # Definition of skeleton model structure:\n","    #   The structure is an n-tuple of:\n","    #\n","    #   (index of a start point, index of an end point, index of a bone)\n","    #\n","    #   E.g., this simple skeletal model\n","    #\n","    #             (0)\n","    #              |\n","    #              |\n","    #              0\n","    #              |\n","    #              |\n","    #     (2)--1--(1)--1--(3)\n","    #      |               |\n","    #      |               |\n","    #      2               2\n","    #      |               |\n","    #      |               |\n","    #     (4)             (5)\n","    #\n","    #   has this structure:\n","    #\n","    #   (\n","    #     (0, 1, 0),\n","    #     (1, 2, 1),\n","    #     (1, 3, 1),\n","    #     (2, 4, 2),\n","    #     (3, 5, 2),\n","    #   )\n","    #\n","    #  Warning 1: The structure has to be a tree.\n","    #  Warning 2: The order isn't random. The order is from a root to lists.\n","    #\n","\n","    return (\n","        # head\n","        (0, 1, 0),\n","\n","        # left shoulder\n","        (1, 2, 1),\n","\n","        # left arm\n","        (2, 3, 2),\n","        # (3, 4, 3),\n","        # Changed to avoid wrist, go straight to hands\n","        (3, 29, 3),\n","\n","        # right shoulder\n","        (1, 5, 1),\n","\n","        # right arm\n","        (5, 6, 2),\n","        # (6, 7, 3),\n","        # Changed to avoid wrist, go straight to hands\n","        (6, 8, 3),\n","\n","        # left hand - wrist\n","        # (7, 8, 4),\n","\n","        # left hand - palm\n","        (8, 9, 5),\n","        (8, 13, 9),\n","        (8, 17, 13),\n","        (8, 21, 17),\n","        (8, 25, 21),\n","\n","        # left hand - 1st finger\n","        (9, 10, 6),\n","        (10, 11, 7),\n","        (11, 12, 8),\n","\n","        # left hand - 2nd finger\n","        (13, 14, 10),\n","        (14, 15, 11),\n","        (15, 16, 12),\n","\n","        # left hand - 3rd finger\n","        (17, 18, 14),\n","        (18, 19, 15),\n","        (19, 20, 16),\n","\n","        # left hand - 4th finger\n","        (21, 22, 18),\n","        (22, 23, 19),\n","        (23, 24, 20),\n","\n","        # left hand - 5th finger\n","        (25, 26, 22),\n","        (26, 27, 23),\n","        (27, 28, 24),\n","\n","        # right hand - wrist\n","        # (4, 29, 4),\n","\n","        # right hand - palm\n","        (29, 30, 5),\n","        (29, 34, 9),\n","        (29, 38, 13),\n","        (29, 42, 17),\n","        (29, 46, 21),\n","\n","        # right hand - 1st finger\n","        (30, 31, 6),\n","        (31, 32, 7),\n","        (32, 33, 8),\n","\n","        # right hand - 2nd finger\n","        (34, 35, 10),\n","        (35, 36, 11),\n","        (36, 37, 12),\n","\n","        # right hand - 3rd finger\n","        (38, 39, 14),\n","        (39, 40, 15),\n","        (40, 41, 16),\n","\n","        # right hand - 4th finger\n","        (42, 43, 18),\n","        (43, 44, 19),\n","        (44, 45, 20),\n","\n","        # right hand - 5th finger\n","        (46, 47, 22),\n","        (47, 48, 23),\n","        (48, 49, 24),\n","    )\n","\n","# Draw a line between two points, if they are positive points\n","def draw_line(im, joint1, joint2, c=(0, 0, 255),t=1, width=3):\n","    thresh = -100\n","    if joint1[0] > thresh and  joint1[1] > thresh and joint2[0] > thresh and joint2[1] > thresh:\n","\n","        center = (int((joint1[0] + joint2[0]) / 2), int((joint1[1] + joint2[1]) / 2))\n","\n","        length = int(math.sqrt(((joint1[0] - joint2[0]) ** 2) + ((joint1[1] - joint2[1]) ** 2))/2)\n","\n","        angle = math.degrees(math.atan2((joint1[0] - joint2[0]),(joint1[1] - joint2[1])))\n","\n","        cv2.ellipse(im, center, (width,length), -angle,0.0,360.0, c, -1)\n","\n","# Draw the frame given 2D joints that are in the Inverse Kinematics format\n","def draw_frame_2D(frame, joints):\n","    # Line to be between the stacked\n","    draw_line(frame, [1, 650], [1, 1], c=(0,0,0), t=1, width=1)\n","    # Give an offset to center the skeleton around\n","    offset = [350, 250]\n","\n","    # Get the skeleton structure details of each bone, and size\n","    skeleton = getSkeletalModelStructure()\n","    skeleton = np.array(skeleton)\n","\n","    number = skeleton.shape[0]\n","\n","    # Increase the size and position of the joints\n","    joints = joints * 10 * 12 * 2\n","    joints = joints + np.ones((joints.shape[0], 2)) * offset\n","\n","    # Loop through each of the bone structures, and plot the bone\n","    for j in range(number):\n","\n","        c = get_bone_colour(skeleton,j)\n","\n","        draw_line(frame, [joints[skeleton[j, 0]][0], joints[skeleton[j, 0]][1]],\n","                  [joints[skeleton[j, 1]][0], joints[skeleton[j, 1]][1]], c=c, t=1, width=1)\n","        \n","\n","# get bone colour given index\n","def get_bone_colour(skeleton,j):\n","    bone = skeleton[j, 2]\n","\n","    if bone == 0:  # head\n","        c = (0, 153, 0)\n","    elif bone == 1:  # Shoulder\n","        c = (0, 0, 255)\n","\n","    elif bone == 2 and skeleton[j, 1] == 3:  # left arm\n","        c = (0, 102, 204)\n","    elif bone == 3 and skeleton[j, 0] == 3:  # left lower arm\n","        c = (0, 204, 204)\n","\n","    elif bone == 2 and skeleton[j, 1] == 6:  # right arm\n","        c = (0, 153, 0)\n","    elif bone == 3 and skeleton[j, 0] == 6:  # right lower arm\n","        c = (0, 204, 0)\n","\n","    # Hands\n","    elif bone in [5, 6, 7, 8]:\n","        c = (0, 0, 255)\n","    elif bone in [9, 10, 11, 12]:\n","        c = (51, 255, 51)\n","    elif bone in [13, 14, 15, 16]:\n","        c = (255, 0, 0)\n","    elif bone in [17, 18, 19, 20]:\n","        c = (204, 153, 255)\n","    elif bone in [21, 22, 23, 24]:\n","        c = (51, 255, 255)\n","\n","    return c\n","\n","# Apply DTW to the produced sequence, so it can be visually compared to the reference sequence\n","def alter_DTW_timing(pred_seq,ref_seq):\n","\n","    # Define a cost function\n","    euclidean_norm = lambda x, y: np.sum(np.abs(x - y))\n","\n","    # Cut the reference down to the max count value\n","    _ , ref_max_idx = torch.max(ref_seq[:, -1], 0)\n","    if ref_max_idx == 0: ref_max_idx += 1\n","    # Cut down frames by counter\n","    ref_seq = ref_seq[:ref_max_idx,:].cpu().numpy()\n","\n","    # Cut the hypothesis down to the max count value\n","    _, hyp_max_idx = torch.max(pred_seq[:, -1], 0)\n","    if hyp_max_idx == 0: hyp_max_idx += 1\n","    # Cut down frames by counter\n","    pred_seq = pred_seq[:hyp_max_idx,:].cpu().numpy()\n","\n","    # Run DTW on the reference and predicted sequence\n","    d, cost_matrix, acc_cost_matrix, path = dtw(ref_seq[:,:-1], pred_seq[:,:-1], dist=euclidean_norm)\n","\n","    # Normalise the dtw cost by sequence length\n","    d = d / acc_cost_matrix.shape[0]\n","\n","    # Initialise new sequence\n","    new_pred_seq = np.zeros_like(ref_seq)\n","    # j tracks the position in the reference sequence\n","    j = 0\n","    skips = 0\n","    squeeze_frames = []\n","    for (i, pred_num) in enumerate(path[0]):\n","\n","        if i == len(path[0]) - 1:\n","            break\n","\n","        if path[1][i] == path[1][i + 1]:\n","            skips += 1\n","\n","        # If a double coming up\n","        if path[0][i] == path[0][i + 1]:\n","            squeeze_frames.append(pred_seq[i - skips])\n","            j += 1\n","        # Just finished a double\n","        elif path[0][i] == path[0][i - 1]:\n","            new_pred_seq[pred_num] = avg_frames(squeeze_frames)\n","            squeeze_frames = []\n","        else:\n","            new_pred_seq[pred_num] = pred_seq[i - skips]\n","\n","    return new_pred_seq, ref_seq, d\n","\n","# Find the average of the given frames\n","def avg_frames(frames):\n","    frames_sum = np.zeros_like(frames[0])\n","    for frame in frames:\n","        frames_sum += frame\n","\n","    avg_frame = frames_sum / len(frames)\n","    return avg_frame"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZEiw7aolTrBf"},"source":["### Run "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":511},"id":"0ARey4ccUH_D","executionInfo":{"status":"error","timestamp":1606795052615,"user_tz":480,"elapsed":161175,"user":{"displayName":"Juyoung Lee","photoUrl":"","userId":"16438012406002097348"}},"outputId":"162736af-d9ac-4dc3-d137-06964533d3e3"},"source":["version = \"v10\"\n","\n","if not os.path.isdir(os.path.join(MODEL_DIR, version)):\n","  os.mkdir(os.path.join(MODEL_DIR, version))\n","\n","cfg = load_config(os.path.join(CONF_DIR, 'Base.yaml'))\n","\n","cfg[\"training\"][\"use_cuda\"] = True\n","cfg[\"data\"][\"max_sent_length\"] = 1\n","cfg[\"data\"][\"skip_frames\"] = 2\n","cfg[\"model\"][\"trg_size\"] = 240 # size of skeleton (xy of face included)\n","cfg[\"training\"][\"logging_freq\"] = 40\n","cfg[\"training\"][\"validation_freq\"] = 300\n","cfg['training'][\"max_output_length\"] = 20\n","cfg[\"training\"][\"batch_size\"] = 32\n","cfg[\"model\"][\"encoder\"][\"num_layers\"] = 1\n","cfg[\"training\"][\"epochs\"] = 1000\n","cfg[\"model\"][\"gaussian_noise\"] = False\n","cfg[\"model\"][\"future_prediction\"] = 0\n","\n","set_seed(seed=cfg[\"training\"].get(\"random_seed\", 42))\n","train_data, dev_data, src_vocab, trg_vocab = load_data(cfg)\n","\n","model = build_model(cfg, src_vocab=src_vocab, trg_vocab=trg_vocab)\n","ckpt = os.path.join(MODEL_DIR, version, 'best.ckpt')\n","model_checkpoint = load_checkpoint(path=ckpt, use_cuda=True)\n","model.load_state_dict(model_checkpoint[\"model_state\"])\n","model.cuda()\n","\n","\n","trainer = TrainManager(model=model, config=cfg)\n","trainer.logger.info(\"Running validation\")\n","\n","score, loss, references, hypotheses, \\\n","inputs, all_dtw_scores, file_paths = validate_on_data(model=model, \n","                                                      data=dev_data,\n","                 batch_size=cfg[\"training\"][\"batch_size\"],\n","                 max_output_length=cfg['training'][\"max_output_length\"],\n","                 eval_metric='dtw',\n","                 loss_function=None,\n","                 batch_type='sentence',\n","                 type='val')\n","trainer.logger.info(\"Mean DTW: {}\".format(score))\n","display = list(range(len(hypotheses)))\n","\"\"\"\n","trainer.produce_validation_video(\n","    output_joints=hypotheses,\n","    inputs=inputs,\n","    references=references,\n","    model_dir=os.path.join(MODEL_DIR, version),\n","    display=display,\n","    type=\"test\",\n","    steps=\"final\",\n","    file_paths=file_paths,\n",")\n","\"\"\""],"execution_count":14,"outputs":[{"output_type":"stream","text":["2020-12-01 03:54:57,265 - __main__ - INFO - Total params: 3072768\n","2020-12-01 03:54:57,267 - __main__ - INFO - Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'src_embed.lut.bias', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']\n","2020-12-01 03:54:57,275 - __main__ - INFO - Running validation\n"],"name":"stderr"},{"output_type":"stream","text":["removing <StreamHandler stderr (NOTSET)>\n","removing <FileHandler /content/drive/My Drive/Colab Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/papertransformer/model/v10/inference.log (NOTSET)>\n"],"name":"stdout"},{"output_type":"stream","text":["2020-12-01 03:56:34,936 - __main__ - INFO - Mean DTW: 128.7590660120419\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:102: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-1f29e34125d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"final\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mfile_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_paths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m )\n","\u001b[0;32m<ipython-input-13-59388d99a516>\u001b[0m in \u001b[0;36mproduce_validation_video\u001b[0;34m(self, output_joints, inputs, references, display, model_dir, type, steps, file_paths)\u001b[0m\n\u001b[1;32m    488\u001b[0m                             \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref_seq_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                             \u001b[0mskip_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_frames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m                             sequence_ID=gloss_label)\n\u001b[0m","\u001b[0;32m<ipython-input-10-c64e7a68192c>\u001b[0m in \u001b[0;36mplot_video\u001b[0;34m(joints, file_path, video_name, references, skip_frames, sequence_ID)\u001b[0m\n\u001b[1;32m    146\u001b[0m                         (0, 0, 0), 2)\n\u001b[1;32m    147\u001b[0m         \u001b[0;31m# Write the video frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mnum_frames\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;31m# Release the video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"e87__Tze44k9","executionInfo":{"status":"ok","timestamp":1606795067832,"user_tz":480,"elapsed":2570,"user":{"displayName":"Juyoung Lee","photoUrl":"","userId":"16438012406002097348"}}},"source":["# save it\n","\n","ground_truth= []\n","for r in references:\n","  ground_truth.append(r.detach().to('cpu').numpy())\n","\n","predictions = []\n","for h in hypotheses:\n","  predictions.append(h.detach().to('cpu').numpy())\n","\n","# save everything\n","all_outputs ={\n","    \"mean_dtw\": score,\n","    \"dtw\": all_dtw_scores,\n","    'words': np.array(inputs).squeeze(),\n","    \"ground_truth\": np.array(ground_truth),\n","    \"predictions\": np.array(predictions)\n","}\n","\n","with open(os.path.join(MODEL_DIR, version, 'v10.predictions.pkl'), 'wb') as f:\n","  pickle.dump(all_outputs, f)"],"execution_count":15,"outputs":[]}]}