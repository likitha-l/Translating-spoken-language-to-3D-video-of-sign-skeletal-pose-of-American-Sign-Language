{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"evaluate.ipynb","provenance":[],"collapsed_sections":["B11XzleBaN48","MYX_AxySa32M","oRcA1qEfbNjz","vusJuNKkbPmu","rhlefd3IcmFi","Vmn1B0PBhdzk"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iAbSJf39YSnU","executionInfo":{"status":"ok","timestamp":1606800503102,"user_tz":480,"elapsed":1885,"user":{"displayName":"Amy Lee","photoUrl":"","userId":"14323350772356815607"}},"outputId":"455d0506-640b-4f1d-b851-2eda9c958f8e"},"source":["import os \n","from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"],"execution_count":52,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MNaKlJUeYXvv"},"source":["# Load the back translation model and run it for the given predicted skeletons\n"]},{"cell_type":"markdown","metadata":{"id":"VKLKGNixYuVQ"},"source":["### Imports "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jnc738sdYjKW","executionInfo":{"status":"ok","timestamp":1606800509918,"user_tz":480,"elapsed":6517,"user":{"displayName":"Amy Lee","photoUrl":"","userId":"14323350772356815607"}},"outputId":"47dfda22-9412-4939-ad20-61363add73e2"},"source":["import yaml\n","import random\n","import sys\n","from typing import Union, List, Dict\n","from logging import Logger\n","import logging\n","from sys import platform\n","import queue\n","import time\n","import glob\n","\n","\n","from itertools import groupby\n","import numpy as np\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import tensorflow as tf\n","tf.config.set_visible_devices([], \"GPU\")\n","from torchtext import data\n","from torchtext.data import Dataset, Iterator\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from torch import Tensor\n","\n","# some non path dependent python modules to import\n","!cp /content/drive/My\\ Drive/Colab\\ Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/backtranslation/dataset.py .\n","!cp /content/drive/My\\ Drive/Colab\\ Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/backtranslation/vocabulary.py .\n","!cp /content/drive/My\\ Drive/Colab\\ Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/backtranslation/initialization.py .\n","!cp /content/drive/My\\ Drive/Colab\\ Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/backtranslation/embeddings.py .\n","!cp /content/drive/My\\ Drive/Colab\\ Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/backtranslation/transformer_layers.py .\n","!cp /content/drive/My\\ Drive/Colab\\ Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/backtranslation/attention.py .\n","!cp /content/drive/My\\ Drive/Colab\\ Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/backtranslation/batch.py .\n","!cp /content/drive/My\\ Drive/Colab\\ Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/backtranslation/loss.py .\n","!cp /content/drive/My\\ Drive/Colab\\ Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/backtranslation/builders.py .\n","!cp /content/drive/My\\ Drive/Colab\\ Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/backtranslation/mscoco_rouge.py .\n","\n","!pip install sacrebleu==\"1.4.2\"\n","\n","from dataset import SignTranslationDataset\n","from vocabulary import (\n","    build_vocab,\n","    Vocabulary,\n","    TextVocabulary,\n","    GlossVocabulary,\n","    UNK_TOKEN,\n","    EOS_TOKEN,\n","    BOS_TOKEN,\n","    PAD_TOKEN,\n","    SIL_TOKEN\n",")\n","from initialization import initialize_model\n","from embeddings import Embeddings, SpatialEmbeddings\n","from transformer_layers import TransformerEncoderLayer, PositionalEncoding, TransformerDecoderLayer\n","from attention import BahdanauAttention, LuongAttention\n","from batch import Batch\n","from loss import XentLoss\n","from builders import build_optimizer, build_scheduler, build_gradient_clipper\n","\n","import mscoco_rouge\n","import sacrebleu"],"execution_count":53,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: sacrebleu==1.4.2 in /usr/local/lib/python3.6/dist-packages (1.4.2)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu==1.4.2) (2.0.0)\n","Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu==1.4.2) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B11XzleBaN48"},"source":["### Helper Methods"]},{"cell_type":"code","metadata":{"id":"rG0uC5bdZfzp","executionInfo":{"status":"ok","timestamp":1606800509919,"user_tz":480,"elapsed":3911,"user":{"displayName":"Amy Lee","photoUrl":"","userId":"14323350772356815607"}}},"source":["def load_config(path='sign.yaml') -> dict:\n","    \"\"\"\n","    Loads and parses a YAML configuration file.\n","\n","    :param path: path to YAML configuration file\n","    :return: configuration dictionary\n","    \"\"\"\n","    with open(path, \"r\", encoding=\"utf-8\") as ymlfile:\n","        cfg = yaml.safe_load(ymlfile)\n","    return cfg\n","def set_seed(seed: int):\n","    \"\"\"\n","    Set the random seed for modules torch, numpy and random.\n","\n","    :param seed: random seed\n","    \"\"\"\n","    torch.manual_seed(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","def subsequent_mask(size: int) -> Tensor:\n","    \"\"\"\n","    Mask out subsequent positions (to prevent attending to future positions)\n","    Transformer helper function.\n","\n","    :param size: size of mask (2nd and 3rd dim)\n","    :return: Tensor with 0s and 1s of shape (1, size, size)\n","    \"\"\"\n","    mask = np.triu(np.ones((1, size, size)), k=1).astype(\"uint8\")\n","    return torch.from_numpy(mask) == 0\n","def freeze_params(module: nn.Module):\n","    \"\"\"\n","    Freeze the parameters of this module,\n","    i.e. do not update them during training\n","\n","    :param module: freeze parameters of this module\n","    \"\"\"\n","    for _, p in module.named_parameters():\n","        p.requires_grad = False\n","# from onmt\n","def tile(x: Tensor, count: int, dim=0) -> Tensor:\n","    \"\"\"\n","    Tiles x on dimension dim count times. From OpenNMT. Used for beam search.\n","\n","    :param x: tensor to tile\n","    :param count: number of tiles\n","    :param dim: dimension along which the tensor is tiled\n","    :return: tiled tensor\n","    \"\"\"\n","    if isinstance(x, tuple):\n","        h, c = x\n","        return tile(h, count, dim=dim), tile(c, count, dim=dim)\n","\n","    perm = list(range(len(x.size())))\n","    if dim != 0:\n","        perm[0], perm[dim] = perm[dim], perm[0]\n","        x = x.permute(perm).contiguous()\n","    out_size = list(x.size())\n","    out_size[0] *= count\n","    batch = x.size(0)\n","    x = (\n","        x.view(batch, -1)\n","        .transpose(0, 1)\n","        .repeat(count, 1)\n","        .transpose(0, 1)\n","        .contiguous()\n","        .view(*out_size)\n","    )\n","    if dim != 0:\n","        x = x.permute(perm).contiguous()\n","    return x\n","def log_cfg(cfg: dict, logger: Logger, prefix: str = \"cfg\"):\n","    \"\"\"\n","    Write configuration to log.\n","\n","    :param cfg: configuration to log\n","    :param logger: logger that defines where log is written to\n","    :param prefix: prefix for logging\n","    \"\"\"\n","    for k, v in cfg.items():\n","        if isinstance(v, dict):\n","            p = \".\".join([prefix, k])\n","            log_cfg(v, logger, prefix=p)\n","        else:\n","            p = \".\".join([prefix, k])\n","            logger.info(\"{:34s} : {}\".format(p, v))\n","def make_logger(model_dir: str, log_file: str = \"inference.log\") -> Logger:\n","    \"\"\"\n","    Create a logger for logging the training process.\n","\n","    :param model_dir: path to logging directory\n","    :param log_file: path to logging file\n","    :return: logger object\n","    \"\"\"\n","    #if not logger.handlers:\n","    logger = logging.getLogger(__name__)\n","    while len(logger.handlers) > 0:\n","        h = logger.handlers[0]\n","        print('removing {}'.format(h))\n","        logger.removeHandler(h)\n","    logger.propagate = False\n","    logger.setLevel(logging.INFO)\n","    # Create handlers\n","    c_handler = logging.StreamHandler()\n","    f_handler = logging.FileHandler(os.path.join(MODEL_DIR, log_file), mode='w')\n","\n","    # Create formatters and add it to handlers\n","    c_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","    f_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","    c_handler.setFormatter(c_format)\n","    f_handler.setFormatter(f_format)\n","\n","    # Add handlers to the logger\n","    logger.addHandler(c_handler)\n","    logger.addHandler(f_handler)\n","\n","    return logger\n","\n","def load_checkpoint(path: str, use_cuda: bool = True) -> dict:\n","    \"\"\"\n","    Load model from saved checkpoint.\n","\n","    :param path: path to checkpoint\n","    :param use_cuda: using cuda or not\n","    :return: checkpoint (dict)\n","    \"\"\"\n","    assert os.path.isfile(path), \"Checkpoint %s not found\" % path\n","    checkpoint = torch.load(path, map_location=\"cuda\" if use_cuda else \"cpu\")\n","    return checkpoint\n","\n","def get_latest_checkpoint(ckpt_dir: str, version: str):\n","    \"\"\"\n","    Returns the latest checkpoint (by time) from the given directory.\n","    If there is no checkpoint in this directory, returns None\n","\n","    :param ckpt_dir:\n","    :return: latest checkpoint file\n","    \"\"\"\n","    list_of_files = glob.glob(\"{}/{}.*.ckpt\".format(ckpt_dir, version))\n","    latest_checkpoint = None\n","    if list_of_files:\n","        latest_checkpoint = max(list_of_files, key=os.path.getctime)\n","    return latest_checkpoint"],"execution_count":54,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MYX_AxySa32M"},"source":["### Dataset Creation"]},{"cell_type":"code","metadata":{"id":"bt0BRqt-a5Kn","executionInfo":{"status":"ok","timestamp":1606800509920,"user_tz":480,"elapsed":2464,"user":{"displayName":"Amy Lee","photoUrl":"","userId":"14323350772356815607"}}},"source":["# coding: utf-8\n","\"\"\"\n","Data module\n","\"\"\"\n","from torchtext import data\n","from torchtext.data import Field\n","from typing import Tuple\n","import pickle\n","\n","class SignTranslationDataset(data.Dataset):\n","    \"\"\"Defines a dataset for machine translation.\"\"\"\n","\n","    @staticmethod\n","    def sort_key(ex):\n","        return data.interleave_keys(len(ex.sgn), len(ex.txt))\n","\n","    def __init__(\n","        self,\n","        path: str,\n","        fields: Tuple[Field, Field],\n","        **kwargs\n","    ):\n","        \"\"\"Create a SignTranslationDataset given paths and fields.\n","\n","        Arguments:\n","            path: Common prefix of paths to the data files for both languages.\n","            exts: A tuple containing the extension to path for each language.\n","            fields: A tuple containing the fields that will be used for data\n","                in each language.\n","            Remaining keyword arguments: Passed to the constructor of\n","                data.Dataset.\n","        \"\"\"\n","        if not isinstance(fields[0], (tuple, list)):\n","            fields = [\n","                (\"sgn\", fields[0]),\n","                (\"txt\", fields[1]),\n","            ]\n","\n","\n","        # read in the data\n","        with open(path, 'rb') as f:\n","            dataset = pickle.load(f)\n","        \n","        word_key = \"gloss\"\n","        skeleton_key = \"skeleton\"\n","        if \"words\" in dataset.keys():\n","          word_key = \"words\"\n","        if \"predictions\" in dataset.keys():\n","          skeleton_key = \"predictions\"\n","          \n","        print(dataset.keys())\n","        examples = []\n","        for i, skeleton in enumerate(dataset[skeleton_key]):\n","            # do frame skip\n","            # skeleton_data = skeleton[0:skeleton.shape[0]:2]\n","\n","            # skip the start token\n","            normalize = skeleton[1:, 0:240] + 1e-8\n","\n","\n","            examples.append(\n","                data.Example.fromlist(\n","                    [\n","                        # This is for numerical stability\n","                        normalize,\n","                        dataset[word_key][i].strip(),\n","                    ],\n","                    fields,\n","                )\n","            )\n","            \n","        super().__init__(examples, fields, **kwargs)\n","\n","def load_data(data_cfg: dict) -> (Dataset, Dataset, Dataset, Vocabulary, Vocabulary):\n","    \"\"\"\n","    Load train, dev and optionally test data as specified in configuration.\n","    Vocabularies are created from the training set with a limit of `voc_limit`\n","    tokens and a minimum token frequency of `voc_min_freq`\n","    (specified in the configuration dictionary).\n","\n","    The training data is filtered to include sentences up to `max_sent_length`\n","    on source and target side.\n","\n","    If you set ``random_train_subset``, a random selection of this size is used\n","    from the training set instead of the full training set.\n","\n","    If you set ``random_dev_subset``, a random selection of this size is used\n","    from the dev development instead of the full development set.\n","\n","    :param data_cfg: configuration dictionary for data\n","        (\"data\" part of configuration file)\n","    :return:\n","        - train_data: training dataset\n","        - dev_data: development dataset\n","        - txt_vocab: spoken text vocabulary extracted from training data\n","    \"\"\"\n","\n","    test_path = os.path.join(DATA_DIR, data_cfg[\"data_file\"])\n","    train_path = os.path.join(DATA_DIR, 'train.pkl')\n","\n","    # this is the length of the skeleton output (240)\n","    pad_feature_size = data_cfg[\"feature_size\"]\n","    \n","    level = data_cfg[\"level\"] # word\n","    txt_lowercase = data_cfg[\"txt_lowercase\"]\n","    max_sent_length = data_cfg[\"max_sent_length\"] # this should be the max length of frame (176)\n","\n","    def tokenize_text(text):\n","        if level == \"char\":\n","            return list(text)\n","        else:\n","            return text.split()\n","\n","    sgn_field = data.Field(\n","        use_vocab=False,\n","        init_token=None,\n","        dtype=torch.float32,\n","        tokenize=lambda features: features,  # TODO (Cihan): is this necessary?\n","        batch_first=True,\n","        include_lengths=True,\n","        pad_token=torch.zeros((pad_feature_size,)),\n","    )\n","\n","    txt_field = data.Field(\n","        init_token=BOS_TOKEN,\n","        eos_token=EOS_TOKEN,\n","        pad_token=PAD_TOKEN,\n","        tokenize=tokenize_text,\n","        unk_token=UNK_TOKEN,\n","        batch_first=True,\n","        lower=txt_lowercase,\n","        include_lengths=True,\n","    )\n","\n","    test_data = SignTranslationDataset(\n","        path=test_path,\n","        fields=(sgn_field, txt_field),\n","        filter_pred=lambda x: len(vars(x)[\"sgn\"]) <= max_sent_length\n","        and len(vars(x)[\"txt\"]) <= 1,\n","    )\n","\n","    train_data = SignTranslationDataset(\n","        path=train_path,\n","        fields=(sgn_field, txt_field),\n","        filter_pred=lambda x: len(vars(x)[\"sgn\"]) <= max_sent_length\n","        and len(vars(x)[\"txt\"]) <= 1,\n","    )\n","\n","    txt_max_size = data_cfg.get(\"txt_voc_limit\", sys.maxsize)\n","    txt_min_freq = data_cfg.get(\"txt_voc_min_freq\", 1)\n","    txt_vocab_file = data_cfg.get(\"txt_vocab\", None)\n","    \n","    txt_vocab = build_vocab(\n","        field=\"txt\",\n","        min_freq=txt_min_freq,\n","        max_size=txt_max_size,\n","        dataset=train_data,\n","        vocab_file=txt_vocab_file,\n","    )\n","\n","    txt_field.vocab = txt_vocab\n","\n","    return test_data, txt_vocab\n","\n","\n","def make_data_iter(\n","    dataset: Dataset,\n","    batch_size: int,\n","    batch_type: str = \"sentence\",\n","    train: bool = False,\n","    shuffle: bool = False,\n",") -> Iterator:\n","    \"\"\"\n","    Returns a torchtext iterator for a torchtext dataset.\n","\n","    :param dataset: torchtext dataset containing sgn and optionally txt\n","    :param batch_size: size of the batches the iterator prepares\n","    :param batch_type: measure batch size by sentence count or by token count\n","    :param train: whether it's training time, when turned off,\n","        bucketing, sorting within batches and shuffling is disabled\n","    :param shuffle: whether to shuffle the data before each epoch\n","        (no effect if set to True for testing)\n","    :return: torchtext iterator\n","    \"\"\"\n","\n","    #batch_size_fn = token_batch_size_fn if batch_type == \"token\" else None\n","    batch_size_fn = None\n","\n","    if train:\n","        # optionally shuffle and sort during training\n","        data_iter = data.BucketIterator(\n","            repeat=False,\n","            sort=False,\n","            dataset=dataset,\n","            batch_size=batch_size,\n","            batch_size_fn=batch_size_fn,\n","            train=True,\n","            sort_within_batch=True,\n","            sort_key=lambda x: len(x.sgn),\n","            shuffle=shuffle,\n","        )\n","    else:\n","        # don't sort/shuffle for validation/inference\n","        data_iter = data.BucketIterator(\n","            repeat=False,\n","            dataset=dataset,\n","            batch_size=batch_size,\n","            batch_size_fn=batch_size_fn,\n","            train=False,\n","            sort=False,\n","        )\n","    return data_iter"],"execution_count":55,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WkUjrOTibCWE"},"source":["### Building The Model\n"]},{"cell_type":"markdown","metadata":{"id":"oRcA1qEfbNjz"},"source":["#### Building Blocks for the Transformer"]},{"cell_type":"code","metadata":{"id":"KK2jyr1ba-gA","executionInfo":{"status":"ok","timestamp":1606800510391,"user_tz":480,"elapsed":1896,"user":{"displayName":"Amy Lee","photoUrl":"","userId":"14323350772356815607"}}},"source":["# pylint: disable=abstract-method\n","class Encoder(nn.Module):\n","    \"\"\"\n","    Base encoder class\n","    \"\"\"\n","\n","    @property\n","    def output_size(self):\n","        \"\"\"\n","        Return the output size\n","\n","        :return:\n","        \"\"\"\n","        return self._output_size\n","        \n","class TransformerEncoder(Encoder):\n","    \"\"\"\n","    Transformer Encoder\n","    \"\"\"\n","\n","    # pylint: disable=unused-argument\n","    def __init__(\n","        self,\n","        hidden_size: int = 512,\n","        ff_size: int = 2048,\n","        num_layers: int = 8,\n","        num_heads: int = 4,\n","        dropout: float = 0.1,\n","        emb_dropout: float = 0.1,\n","        freeze: bool = False,\n","        **kwargs\n","    ):\n","        \"\"\"\n","        Initializes the Transformer.\n","        :param hidden_size: hidden size and size of embeddings\n","        :param ff_size: position-wise feed-forward layer size.\n","          (Typically this is 2*hidden_size.)\n","        :param num_layers: number of layers\n","        :param num_heads: number of heads for multi-headed attention\n","        :param dropout: dropout probability for Transformer layers\n","        :param emb_dropout: Is applied to the input (word embeddings).\n","        :param freeze: freeze the parameters of the encoder during training\n","        :param kwargs:\n","        \"\"\"\n","        super(TransformerEncoder, self).__init__()\n","\n","        # build all (num_layers) layers\n","        self.layers = nn.ModuleList(\n","            [\n","                TransformerEncoderLayer(\n","                    size=hidden_size,\n","                    ff_size=ff_size,\n","                    num_heads=num_heads,\n","                    dropout=dropout,\n","                )\n","                for _ in range(num_layers)\n","            ]\n","        )\n","\n","        self.layer_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n","        self.pe = PositionalEncoding(hidden_size)\n","        self.emb_dropout = nn.Dropout(p=emb_dropout)\n","\n","        self._output_size = hidden_size\n","\n","        if freeze:\n","            freeze_params(self)\n","\n","    # pylint: disable=arguments-differ\n","    def forward(\n","        self, embed_src: Tensor, src_length: Tensor, mask: Tensor\n","    ) -> (Tensor, Tensor):\n","        \"\"\"\n","        Pass the input (and mask) through each layer in turn.\n","        Applies a Transformer encoder to sequence of embeddings x.\n","        The input mini-batch x needs to be sorted by src length.\n","        x and mask should have the same dimensions [batch, time, dim].\n","\n","        :param embed_src: embedded src inputs,\n","            shape (batch_size, src_len, embed_size)\n","        :param src_length: length of src inputs\n","            (counting tokens before padding), shape (batch_size)\n","        :param mask: indicates padding areas (zeros where padding), shape\n","            (batch_size, src_len, embed_size)\n","        :return:\n","            - output: hidden states with\n","                shape (batch_size, max_length, directions*hidden),\n","            - hidden_concat: last hidden state with\n","                shape (batch_size, directions*hidden)\n","        \"\"\"\n","        x = self.pe(embed_src)  # add position encoding to word embeddings\n","        x = self.emb_dropout(x)\n","\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.layer_norm(x), None\n","\n","    def __repr__(self):\n","        return \"%s(num_layers=%r, num_heads=%r)\" % (\n","            self.__class__.__name__,\n","            len(self.layers),\n","            self.layers[0].src_src_att.num_heads,\n","        )\n","\n","# pylint: disable=abstract-method\n","class Decoder(nn.Module):\n","    \"\"\"\n","    Base decoder class\n","    \"\"\"\n","\n","    @property\n","    def output_size(self):\n","        \"\"\"\n","        Return the output size (size of the target vocabulary)\n","\n","        :return:\n","        \"\"\"\n","        return self._output_size\n","\n","class TransformerDecoder(Decoder):\n","    \"\"\"\n","    A transformer decoder with N masked layers.\n","    Decoder layers are masked so that an attention head cannot see the future.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        num_layers: int = 4,\n","        num_heads: int = 8,\n","        hidden_size: int = 512,\n","        ff_size: int = 2048,\n","        dropout: float = 0.1,\n","        emb_dropout: float = 0.1,\n","        vocab_size: int = 1,\n","        freeze: bool = False,\n","        **kwargs\n","    ):\n","        \"\"\"\n","        Initialize a Transformer decoder.\n","\n","        :param num_layers: number of Transformer layers\n","        :param num_heads: number of heads for each layer\n","        :param hidden_size: hidden size\n","        :param ff_size: position-wise feed-forward size\n","        :param dropout: dropout probability (1-keep)\n","        :param emb_dropout: dropout probability for embeddings\n","        :param vocab_size: size of the output vocabulary\n","        :param freeze: set to True keep all decoder parameters fixed\n","        :param kwargs:\n","        \"\"\"\n","        super(TransformerDecoder, self).__init__()\n","\n","        self._hidden_size = hidden_size\n","        self._output_size = vocab_size\n","\n","        # create num_layers decoder layers and put them in a list\n","        self.layers = nn.ModuleList(\n","            [\n","                TransformerDecoderLayer(\n","                    size=hidden_size,\n","                    ff_size=ff_size,\n","                    num_heads=num_heads,\n","                    dropout=dropout,\n","                )\n","                for _ in range(num_layers)\n","            ]\n","        )\n","\n","        self.pe = PositionalEncoding(hidden_size)\n","        self.layer_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n","\n","        self.emb_dropout = nn.Dropout(p=emb_dropout)\n","        self.output_layer = nn.Linear(hidden_size, vocab_size, bias=False)\n","\n","        if freeze:\n","            freeze_params(self)\n","\n","    def forward(\n","        self,\n","        trg_embed: Tensor = None,\n","        encoder_output: Tensor = None,\n","        encoder_hidden: Tensor = None,\n","        src_mask: Tensor = None,\n","        unroll_steps: int = None,\n","        hidden: Tensor = None,\n","        trg_mask: Tensor = None,\n","        **kwargs\n","    ):\n","        \"\"\"\n","        Transformer decoder forward pass.\n","\n","        :param trg_embed: embedded targets\n","        :param encoder_output: source representations\n","        :param encoder_hidden: unused\n","        :param src_mask:\n","        :param unroll_steps: unused\n","        :param hidden: unused\n","        :param trg_mask: to mask out target paddings\n","                         Note that a subsequent mask is applied here.\n","        :param kwargs:\n","        :return:\n","        \"\"\"\n","        assert trg_mask is not None, \"trg_mask required for Transformer\"\n","\n","        x = self.pe(trg_embed)  # add position encoding to word embedding\n","        x = self.emb_dropout(x)\n","\n","        trg_mask = trg_mask & subsequent_mask(trg_embed.size(1)).type_as(trg_mask)\n","\n","        for layer in self.layers:\n","            x = layer(x=x, memory=encoder_output, src_mask=src_mask, trg_mask=trg_mask)\n","\n","        x = self.layer_norm(x)\n","        output = self.output_layer(x)\n","\n","        return output, x, None, None\n","\n","    def __repr__(self):\n","        return \"%s(num_layers=%r, num_heads=%r)\" % (\n","            self.__class__.__name__,\n","            len(self.layers),\n","            self.layers[0].trg_trg_att.num_heads,\n","        )\n"],"execution_count":56,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vusJuNKkbPmu"},"source":["#### Main Training Model"]},{"cell_type":"code","metadata":{"id":"rNXi94vgbZk0","executionInfo":{"status":"ok","timestamp":1606800515035,"user_tz":480,"elapsed":5207,"user":{"displayName":"Amy Lee","photoUrl":"","userId":"14323350772356815607"}}},"source":["def greedy(\n","    src_mask: Tensor,\n","    embed: Embeddings,\n","    bos_index: int,\n","    eos_index: int,\n","    max_output_length: int,\n","    decoder,\n","    encoder_output: Tensor,\n","    encoder_hidden: Tensor,\n",") -> (np.array, np.array):\n","    \"\"\"\n","    Greedy decoding. Select the token word highest probability at each time\n","    step. This function is a wrapper that calls recurrent_greedy for\n","    recurrent decoders and transformer_greedy for transformer decoders.\n","\n","    :param src_mask: mask for source inputs, 0 for positions after </s>\n","    :param embed: target embedding\n","    :param bos_index: index of <s> in the vocabulary\n","    :param eos_index: index of </s> in the vocabulary\n","    :param max_output_length: maximum length for the hypotheses\n","    :param decoder: decoder to use for greedy decoding\n","    :param encoder_output: encoder hidden states for attention\n","    :param encoder_hidden: encoder last state for decoder initialization\n","    :return:\n","    \"\"\"\n","\n","    if isinstance(decoder, TransformerDecoder):\n","        # Transformer greedy decoding\n","        greedy_fun = transformer_greedy\n","    else:\n","        # Recurrent greedy decoding\n","        greedy_fun = recurrent_greedy\n","\n","    return greedy_fun(\n","        src_mask=src_mask,\n","        embed=embed,\n","        bos_index=bos_index,\n","        eos_index=eos_index,\n","        max_output_length=max_output_length,\n","        decoder=decoder,\n","        encoder_output=encoder_output,\n","        encoder_hidden=encoder_hidden,\n","    )\n","\n","def transformer_greedy(\n","    src_mask: Tensor,\n","    embed: Embeddings,\n","    bos_index: int,\n","    eos_index: int,\n","    max_output_length: int,\n","    decoder,\n","    encoder_output: Tensor,\n","    encoder_hidden: Tensor,\n",") -> (np.array, None):\n","    \"\"\"\n","    Special greedy function for transformer, since it works differently.\n","    The transformer remembers all previous states and attends to them.\n","\n","    :param src_mask: mask for source inputs, 0 for positions after </s>\n","    :param embed: target embedding layer\n","    :param bos_index: index of <s> in the vocabulary\n","    :param eos_index: index of </s> in the vocabulary\n","    :param max_output_length: maximum length for the hypotheses\n","    :param decoder: decoder to use for greedy decoding\n","    :param encoder_output: encoder hidden states for attention\n","    :param encoder_hidden: encoder final state (unused in Transformer)\n","    :return:\n","        - stacked_output: output hypotheses (2d array of indices),\n","        - stacked_attention_scores: attention scores (3d array)\n","    \"\"\"\n","\n","    batch_size = src_mask.size(0)\n","\n","    # start with BOS-symbol for each sentence in the batch\n","    ys = encoder_output.new_full([batch_size, 1], bos_index, dtype=torch.long)\n","\n","    # a subsequent mask is intersected with this in decoder forward pass\n","    trg_mask = src_mask.new_ones([1, 1, 1])\n","    finished = src_mask.new_zeros((batch_size)).byte()\n","\n","    for _ in range(max_output_length):\n","\n","        trg_embed = embed(ys)  # embed the previous tokens\n","\n","        # pylint: disable=unused-variable\n","        with torch.no_grad():\n","            logits, out, _, _ = decoder(\n","                trg_embed=trg_embed,\n","                encoder_output=encoder_output,\n","                encoder_hidden=None,\n","                src_mask=src_mask,\n","                unroll_steps=None,\n","                hidden=None,\n","                trg_mask=trg_mask,\n","            )\n","\n","            logits = logits[:, -1]\n","            _, next_word = torch.max(logits, dim=1)\n","            next_word = next_word.data\n","            ys = torch.cat([ys, next_word.unsqueeze(-1)], dim=1)\n","\n","        # check if previous symbol was <eos>\n","        is_eos = torch.eq(next_word, eos_index)\n","        finished += is_eos\n","        # stop predicting if <eos> reached for all elements in batch\n","        if (finished >= 1).sum() == batch_size:\n","            break\n","\n","    ys = ys[:, 1:]  # remove BOS-symbol\n","    return ys.detach().cpu().numpy(), None\n","    \n","class SignModel(nn.Module):\n","    \"\"\"\n","    Base Model class\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        encoder: Encoder,\n","        gloss_output_layer: nn.Module,\n","        decoder: Decoder,\n","        sgn_embed: SpatialEmbeddings,\n","        txt_embed: Embeddings,\n","        gls_vocab: GlossVocabulary,\n","        txt_vocab: TextVocabulary,\n","        do_recognition: bool = True,\n","        do_translation: bool = True,\n","    ):\n","        \"\"\"\n","        Create a new encoder-decoder model\n","\n","        :param encoder: encoder\n","        :param decoder: decoder\n","        :param sgn_embed: spatial feature frame embeddings\n","        :param txt_embed: spoken language word embedding\n","        :param gls_vocab: gls vocabulary\n","        :param txt_vocab: spoken language vocabulary\n","        :param do_recognition: flag to build the model with recognition output.\n","        :param do_translation: flag to build the model with translation decoder.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","        self.sgn_embed = sgn_embed\n","        self.txt_embed = txt_embed\n","\n","        self.gls_vocab = gls_vocab\n","        self.txt_vocab = txt_vocab\n","\n","        self.txt_bos_index = self.txt_vocab.stoi[BOS_TOKEN]\n","        self.txt_pad_index = self.txt_vocab.stoi[PAD_TOKEN]\n","        self.txt_eos_index = self.txt_vocab.stoi[EOS_TOKEN]\n","\n","        self.gloss_output_layer = gloss_output_layer\n","        self.do_recognition = do_recognition\n","        self.do_translation = do_translation\n","\n","    # pylint: disable=arguments-differ\n","    def forward(\n","        self,\n","        sgn: Tensor,\n","        sgn_mask: Tensor,\n","        sgn_lengths: Tensor,\n","        txt_input: Tensor,\n","        txt_mask: Tensor = None,\n","    ) -> (Tensor, Tensor, Tensor, Tensor):\n","        \"\"\"\n","        First encodes the source sentence.\n","        Then produces the target one word at a time.\n","\n","        :param sgn: source input\n","        :param sgn_mask: source mask\n","        :param sgn_lengths: length of source inputs\n","        :param txt_input: target input\n","        :param txt_mask: target mask\n","        :return: decoder outputs\n","        \"\"\"\n","        encoder_output, encoder_hidden = self.encode(\n","            sgn=sgn, sgn_mask=sgn_mask, sgn_length=sgn_lengths\n","        )\n","\n","        if self.do_recognition:\n","            # Gloss Recognition Part\n","            # N x T x C\n","            gloss_scores = self.gloss_output_layer(encoder_output)\n","            # N x T x C\n","            gloss_probabilities = gloss_scores.log_softmax(2)\n","            # Turn it into T x N x C\n","            gloss_probabilities = gloss_probabilities.permute(1, 0, 2)\n","        else:\n","            gloss_probabilities = None\n","\n","        if self.do_translation:\n","            unroll_steps = txt_input.size(1)\n","            decoder_outputs = self.decode(\n","                encoder_output=encoder_output,\n","                encoder_hidden=encoder_hidden,\n","                sgn_mask=sgn_mask,\n","                txt_input=txt_input,\n","                unroll_steps=unroll_steps,\n","                txt_mask=txt_mask,\n","            )\n","        else:\n","            decoder_outputs = None\n","\n","        return decoder_outputs, gloss_probabilities\n","\n","    def encode(\n","        self, sgn: Tensor, sgn_mask: Tensor, sgn_length: Tensor\n","    ) -> (Tensor, Tensor):\n","        \"\"\"\n","        Encodes the source sentence.\n","\n","        :param sgn:\n","        :param sgn_mask:\n","        :param sgn_length:\n","        :return: encoder outputs (output, hidden_concat)\n","        \"\"\"\n","        return self.encoder(\n","            embed_src=self.sgn_embed(x=sgn, mask=sgn_mask),\n","            src_length=sgn_length,\n","            mask=sgn_mask,\n","        )\n","\n","    def decode(\n","        self,\n","        encoder_output: Tensor,\n","        encoder_hidden: Tensor,\n","        sgn_mask: Tensor,\n","        txt_input: Tensor,\n","        unroll_steps: int,\n","        decoder_hidden: Tensor = None,\n","        txt_mask: Tensor = None,\n","    ) -> (Tensor, Tensor, Tensor, Tensor):\n","        \"\"\"\n","        Decode, given an encoded source sentence.\n","\n","        :param encoder_output: encoder states for attention computation\n","        :param encoder_hidden: last encoder state for decoder initialization\n","        :param sgn_mask: sign sequence mask, 1 at valid tokens\n","        :param txt_input: spoken language sentence inputs\n","        :param unroll_steps: number of steps to unroll the decoder for\n","        :param decoder_hidden: decoder hidden state (optional)\n","        :param txt_mask: mask for spoken language words\n","        :return: decoder outputs (outputs, hidden, att_probs, att_vectors)\n","        \"\"\"\n","        return self.decoder(\n","            encoder_output=encoder_output,\n","            encoder_hidden=encoder_hidden,\n","            src_mask=sgn_mask,\n","            trg_embed=self.txt_embed(x=txt_input, mask=txt_mask),\n","            trg_mask=txt_mask,\n","            unroll_steps=unroll_steps,\n","            hidden=decoder_hidden,\n","        )\n","\n","    def get_loss_for_batch(\n","        self,\n","        batch: Batch,\n","        recognition_loss_function: nn.Module,\n","        translation_loss_function: nn.Module,\n","        recognition_loss_weight: float,\n","        translation_loss_weight: float,\n","    ) -> (Tensor, Tensor):\n","        \"\"\"\n","        Compute non-normalized loss and number of tokens for a batch\n","\n","        :param batch: batch to compute loss for\n","        :param recognition_loss_function: Sign Language Recognition Loss Function (CTC)\n","        :param translation_loss_function: Sign Language Translation Loss Function (XEntropy)\n","        :param recognition_loss_weight: Weight for recognition loss\n","        :param translation_loss_weight: Weight for translation loss\n","        :return: recognition_loss: sum of losses over sequences in the batch\n","        :return: translation_loss: sum of losses over non-pad elements in the batch\n","        \"\"\"\n","        # pylint: disable=unused-variable\n","\n","        # Do a forward pass\n","        decoder_outputs, gloss_probabilities = self.forward(\n","            sgn=batch.sgn,\n","            sgn_mask=batch.sgn_mask,\n","            sgn_lengths=batch.sgn_lengths,\n","            txt_input=batch.txt_input,\n","            txt_mask=batch.txt_mask,\n","        )\n","\n","        if self.do_recognition:\n","            assert gloss_probabilities is not None\n","            # Calculate Recognition Loss\n","            recognition_loss = (\n","                recognition_loss_function(\n","                    gloss_probabilities,\n","                    batch.gls,\n","                    batch.sgn_lengths.long(),\n","                    batch.gls_lengths.long(),\n","                )\n","                * recognition_loss_weight\n","            )\n","        else:\n","            recognition_loss = None\n","\n","        if self.do_translation:\n","            assert decoder_outputs is not None\n","            word_outputs, _, _, _ = decoder_outputs\n","            # Calculate Translation Loss\n","            txt_log_probs = F.log_softmax(word_outputs, dim=-1)\n","            translation_loss = (\n","                translation_loss_function(txt_log_probs, batch.txt)\n","                * translation_loss_weight\n","            )\n","        else:\n","            translation_loss = None\n","\n","        return recognition_loss, translation_loss\n","\n","    def run_batch(\n","        self,\n","        batch: Batch,\n","        recognition_beam_size: int = 1,\n","        translation_beam_size: int = 1,\n","        translation_beam_alpha: float = -1,\n","        translation_max_output_length: int = 100,\n","    ) -> (np.array, np.array, np.array):\n","        \"\"\"\n","        Get outputs and attentions scores for a given batch\n","\n","        :param batch: batch to generate hypotheses for\n","        :param recognition_beam_size: size of the beam for CTC beam search\n","            if 1 use greedy\n","        :param translation_beam_size: size of the beam for translation beam search\n","            if 1 use greedy\n","        :param translation_beam_alpha: alpha value for beam search\n","        :param translation_max_output_length: maximum length of translation hypotheses\n","        :return: stacked_output: hypotheses for batch,\n","            stacked_attention_scores: attention scores for batch\n","        \"\"\"\n","\n","        encoder_output, encoder_hidden = self.encode(\n","            sgn=batch.sgn, sgn_mask=batch.sgn_mask, sgn_length=batch.sgn_lengths\n","        )\n","\n","        if self.do_recognition:\n","            # Gloss Recognition Part\n","            # N x T x C\n","            gloss_scores = self.gloss_output_layer(encoder_output)\n","            # N x T x C\n","            gloss_probabilities = gloss_scores.log_softmax(2)\n","            # Turn it into T x N x C\n","            gloss_probabilities = gloss_probabilities.permute(1, 0, 2)\n","            gloss_probabilities = gloss_probabilities.cpu().detach().numpy()\n","            tf_gloss_probabilities = np.concatenate(\n","                (gloss_probabilities[:, :, 1:], gloss_probabilities[:, :, 0, None]),\n","                axis=-1,\n","            )\n","\n","            assert recognition_beam_size > 0\n","            ctc_decode, _ = tf.nn.ctc_beam_search_decoder(\n","                inputs=tf_gloss_probabilities,\n","                sequence_length=batch.sgn_lengths.cpu().detach().numpy(),\n","                beam_width=recognition_beam_size,\n","                top_paths=1,\n","            )\n","            ctc_decode = ctc_decode[0]\n","            # Create a decoded gloss list for each sample\n","            tmp_gloss_sequences = [[] for i in range(gloss_scores.shape[0])]\n","            for (value_idx, dense_idx) in enumerate(ctc_decode.indices):\n","                tmp_gloss_sequences[dense_idx[0]].append(\n","                    ctc_decode.values[value_idx].numpy() + 1\n","                )\n","            decoded_gloss_sequences = []\n","            for seq_idx in range(0, len(tmp_gloss_sequences)):\n","                decoded_gloss_sequences.append(\n","                    [x[0] for x in groupby(tmp_gloss_sequences[seq_idx])]\n","                )\n","        else:\n","            decoded_gloss_sequences = None\n","\n","        if self.do_translation:\n","            # greedy decoding\n","            if translation_beam_size < 2:\n","                stacked_txt_output, stacked_attention_scores = greedy(\n","                    encoder_hidden=encoder_hidden,\n","                    encoder_output=encoder_output,\n","                    src_mask=batch.sgn_mask,\n","                    embed=self.txt_embed,\n","                    bos_index=self.txt_bos_index,\n","                    eos_index=self.txt_eos_index,\n","                    decoder=self.decoder,\n","                    max_output_length=translation_max_output_length,\n","                )\n","                # batch, time, max_sgn_length\n","            else:  # beam size\n","                stacked_txt_output, stacked_attention_scores = beam_search(\n","                    size=translation_beam_size,\n","                    encoder_hidden=encoder_hidden,\n","                    encoder_output=encoder_output,\n","                    src_mask=batch.sgn_mask,\n","                    embed=self.txt_embed,\n","                    max_output_length=translation_max_output_length,\n","                    alpha=translation_beam_alpha,\n","                    eos_index=self.txt_eos_index,\n","                    pad_index=self.txt_pad_index,\n","                    bos_index=self.txt_bos_index,\n","                    decoder=self.decoder,\n","                )\n","        else:\n","            stacked_txt_output = stacked_attention_scores = None\n","\n","        return decoded_gloss_sequences, stacked_txt_output, stacked_attention_scores\n","\n","    def __repr__(self) -> str:\n","        \"\"\"\n","        String representation: a description of encoder, decoder and embeddings\n","\n","        :return: string representation\n","        \"\"\"\n","        return (\n","            \"%s(\\n\"\n","            \"\\tencoder=%s,\\n\"\n","            \"\\tdecoder=%s,\\n\"\n","            \"\\tsgn_embed=%s,\\n\"\n","            \"\\ttxt_embed=%s)\"\n","            % (\n","                self.__class__.__name__,\n","                self.encoder,\n","                self.decoder,\n","                self.sgn_embed,\n","                self.txt_embed,\n","            )\n","        )\n","\n","def build_model(\n","    cfg: dict,\n","    sgn_dim: int,\n","    gls_vocab: GlossVocabulary,\n","    txt_vocab: TextVocabulary,\n","    do_recognition: bool = True,\n","    do_translation: bool = True,\n",") -> SignModel:\n","    \"\"\"\n","    Build and initialize the model according to the configuration.\n","\n","    :param cfg: dictionary configuration containing model specifications\n","    :param sgn_dim: feature dimension of the sign frame representation, i.e. 2560 for EfficientNet-7.\n","    :param gls_vocab: sign gloss vocabulary\n","    :param txt_vocab: spoken language word vocabulary\n","    :return: built and initialized model\n","    :param do_recognition: flag to build the model with recognition output.\n","    :param do_translation: flag to build the model with translation decoder.\n","    \"\"\"\n","\n","    txt_padding_idx = txt_vocab.stoi[PAD_TOKEN]\n","\n","    sgn_embed: SpatialEmbeddings = SpatialEmbeddings(\n","        **cfg[\"encoder\"][\"embeddings\"],\n","        num_heads=cfg[\"encoder\"][\"num_heads\"],\n","        input_size=sgn_dim,\n","    )\n","\n","    # build encoder\n","    enc_dropout = cfg[\"encoder\"].get(\"dropout\", 0.0)\n","    enc_emb_dropout = cfg[\"encoder\"][\"embeddings\"].get(\"dropout\", enc_dropout)\n","    if cfg[\"encoder\"].get(\"type\", \"recurrent\") == \"transformer\":\n","        assert (\n","            cfg[\"encoder\"][\"embeddings\"][\"embedding_dim\"]\n","            == cfg[\"encoder\"][\"hidden_size\"]\n","        ), \"for transformer, emb_size must be hidden_size\"\n","\n","        encoder = TransformerEncoder(\n","            **cfg[\"encoder\"],\n","            emb_size=sgn_embed.embedding_dim,\n","            emb_dropout=enc_emb_dropout,\n","        )\n","    else:\n","        encoder = RecurrentEncoder(\n","            **cfg[\"encoder\"],\n","            emb_size=sgn_embed.embedding_dim,\n","            emb_dropout=enc_emb_dropout,\n","        )\n","\n","    if do_recognition:\n","        gloss_output_layer = nn.Linear(encoder.output_size, len(gls_vocab))\n","        if cfg[\"encoder\"].get(\"freeze\", False):\n","            freeze_params(gloss_output_layer)\n","    else:\n","        gloss_output_layer = None\n","\n","    # build decoder and word embeddings\n","    if do_translation:\n","        txt_embed: Union[Embeddings, None] = Embeddings(\n","            **cfg[\"decoder\"][\"embeddings\"],\n","            num_heads=cfg[\"decoder\"][\"num_heads\"],\n","            vocab_size=len(txt_vocab),\n","            padding_idx=txt_padding_idx,\n","        )\n","        dec_dropout = cfg[\"decoder\"].get(\"dropout\", 0.0)\n","        dec_emb_dropout = cfg[\"decoder\"][\"embeddings\"].get(\"dropout\", dec_dropout)\n","        if cfg[\"decoder\"].get(\"type\", \"recurrent\") == \"transformer\":\n","            decoder = TransformerDecoder(\n","                **cfg[\"decoder\"],\n","                encoder=encoder,\n","                vocab_size=len(txt_vocab),\n","                emb_size=txt_embed.embedding_dim,\n","                emb_dropout=dec_emb_dropout,\n","            )\n","        else:\n","            decoder = RecurrentDecoder(\n","                **cfg[\"decoder\"],\n","                encoder=encoder,\n","                vocab_size=len(txt_vocab),\n","                emb_size=txt_embed.embedding_dim,\n","                emb_dropout=dec_emb_dropout,\n","            )\n","    else:\n","        txt_embed = None\n","        decoder = None\n","\n","    model: SignModel = SignModel(\n","        encoder=encoder,\n","        gloss_output_layer=gloss_output_layer,\n","        decoder=decoder,\n","        sgn_embed=sgn_embed,\n","        txt_embed=txt_embed,\n","        gls_vocab=gls_vocab,\n","        txt_vocab=txt_vocab,\n","        do_recognition=do_recognition,\n","        do_translation=do_translation,\n","    )\n","\n","    if do_translation:\n","        # tie softmax layer with txt embeddings\n","        if cfg.get(\"tied_softmax\", False):\n","            # noinspection PyUnresolvedReferences\n","            if txt_embed.lut.weight.shape == model.decoder.output_layer.weight.shape:\n","                # (also) share txt embeddings and softmax layer:\n","                # noinspection PyUnresolvedReferences\n","                model.decoder.output_layer.weight = txt_embed.lut.weight\n","            else:\n","                raise ValueError(\n","                    \"For tied_softmax, the decoder embedding_dim and decoder \"\n","                    \"hidden_size must be the same.\"\n","                    \"The decoder must be a Transformer.\"\n","                )\n","\n","    # custom initialization of model parameters\n","    initialize_model(model, cfg, txt_padding_idx)\n","\n","    return model\n","\n","class TrainManager:\n","    \"\"\" Manages training loop, validations, learning rate scheduling\n","    and early stopping.\"\"\"\n","\n","    def __init__(self, model: SignModel, config: dict, logger) -> None:\n","        \"\"\"\n","        Creates a new TrainManager for a model, specified as in configuration.\n","\n","        :param model: torch module defining the model\n","        :param config: dictionary containing the training configurations\n","        \"\"\"\n","        train_config = config[\"training\"]\n","\n","        # files for logging and storing\n","        self.model_dir = MODEL_DIR\n","        self.logger = logger\n","\n","        self.logging_freq = train_config.get(\"logging_freq\", 100)\n","        self.valid_report_file = \"{}/validations.txt\".format(self.model_dir)\n","\n","        # input\n","        self.feature_size = (\n","            sum(config[\"data\"][\"feature_size\"])\n","            if isinstance(config[\"data\"][\"feature_size\"], list)\n","            else config[\"data\"][\"feature_size\"]\n","        )\n","        self.dataset_version = config[\"data\"].get(\"version\", \"phoenix_2014_trans\")\n","\n","        # model\n","        self.model = model\n","        self.txt_pad_index = self.model.txt_pad_index\n","        self.txt_bos_index = self.model.txt_bos_index\n","        self._log_parameters_list()\n","        # Check if we are doing only recognition or only translation or both\n","        self.do_recognition = (\n","            config[\"training\"].get(\"recognition_loss_weight\", 1.0) > 0.0\n","        )\n","        self.do_translation = (\n","            config[\"training\"].get(\"translation_loss_weight\", 1.0) > 0.0\n","        )\n","\n","        # Get Recognition and Translation specific parameters\n","        if self.do_recognition:\n","            self._get_recognition_params(train_config=train_config)\n","        if self.do_translation:\n","            self._get_translation_params(train_config=train_config)\n","\n","        # optimization\n","        self.last_best_lr = train_config.get(\"learning_rate\", -1)\n","        self.learning_rate_min = train_config.get(\"learning_rate_min\", 1.0e-8)\n","        self.clip_grad_fun = build_gradient_clipper(config=train_config)\n","        self.optimizer = build_optimizer(\n","            config=train_config, parameters=model.parameters()\n","        )\n","        self.batch_multiplier = train_config.get(\"batch_multiplier\", 1)\n","\n","        # validation & early stopping\n","        self.validation_freq = train_config.get(\"validation_freq\", 100)\n","        self.num_valid_log = train_config.get(\"num_valid_log\", 5)\n","        self.ckpt_queue = queue.Queue(maxsize=train_config.get(\"keep_last_ckpts\", 5))\n","        self.eval_metric = train_config.get(\"eval_metric\", \"bleu\")\n","        if self.eval_metric not in [\"bleu\", \"chrf\", \"wer\", \"rouge\"]:\n","            raise ValueError(\n","                \"Invalid setting for 'eval_metric': {}\".format(self.eval_metric)\n","            )\n","        self.early_stopping_metric = train_config.get(\n","            \"early_stopping_metric\", \"eval_metric\"\n","        )\n","\n","        # if we schedule after BLEU/chrf, we want to maximize it, else minimize\n","        # early_stopping_metric decides on how to find the early stopping point:\n","        # ckpts are written when there's a new high/low score for this metric\n","        if self.early_stopping_metric in [\n","            \"ppl\",\n","            \"translation_loss\",\n","            \"recognition_loss\",\n","        ]:\n","            self.minimize_metric = True\n","        elif self.early_stopping_metric == \"eval_metric\":\n","            if self.eval_metric in [\"bleu\", \"chrf\", \"rouge\"]:\n","                assert self.do_translation\n","                self.minimize_metric = False\n","            else:  # eval metric that has to get minimized (not yet implemented)\n","                self.minimize_metric = True\n","        else:\n","            raise ValueError(\n","                \"Invalid setting for 'early_stopping_metric': {}\".format(\n","                    self.early_stopping_metric\n","                )\n","            )\n","\n","        # data_augmentation parameters\n","        self.frame_subsampling_ratio = config[\"data\"].get(\n","            \"frame_subsampling_ratio\", None\n","        )\n","        self.random_frame_subsampling = config[\"data\"].get(\n","            \"random_frame_subsampling\", None\n","        )\n","        self.random_frame_masking_ratio = config[\"data\"].get(\n","            \"random_frame_masking_ratio\", None\n","        )\n","\n","        # learning rate scheduling\n","        self.scheduler, self.scheduler_step_at = build_scheduler(\n","            config=train_config,\n","            scheduler_mode=\"min\" if self.minimize_metric else \"max\",\n","            optimizer=self.optimizer,\n","            hidden_size=config[\"model\"][\"encoder\"][\"hidden_size\"],\n","        )\n","\n","        # data & batch handling\n","        self.level = config[\"data\"][\"level\"]\n","        if self.level not in [\"word\", \"bpe\", \"char\"]:\n","            raise ValueError(\"Invalid segmentation level': {}\".format(self.level))\n","\n","        self.shuffle = train_config.get(\"shuffle\", True)\n","        self.epochs = train_config[\"epochs\"]\n","        self.batch_size = train_config[\"batch_size\"]\n","        self.batch_type = train_config.get(\"batch_type\", \"sentence\")\n","        self.eval_batch_size = train_config.get(\"eval_batch_size\", self.batch_size)\n","        self.eval_batch_type = train_config.get(\"eval_batch_type\", self.batch_type)\n","\n","        self.use_cuda = train_config[\"use_cuda\"]\n","        if self.use_cuda:\n","            self.model.cuda()\n","            if self.do_translation:\n","                self.translation_loss_function.cuda()\n","            if self.do_recognition:\n","                self.recognition_loss_function.cuda()\n","\n","        # initialize training statistics\n","        self.steps = 0\n","        # stop training if this flag is True by reaching learning rate minimum\n","        self.stop = False\n","        self.total_txt_tokens = 0\n","        self.total_gls_tokens = 0\n","        self.best_ckpt_iteration = 0\n","        # initial values for best scores\n","        self.best_ckpt_score = np.inf if self.minimize_metric else -np.inf\n","        self.best_all_ckpt_scores = {}\n","        # comparison function for scores\n","        self.is_best = (\n","            lambda score: score < self.best_ckpt_score\n","            if self.minimize_metric\n","            else score > self.best_ckpt_score\n","        )\n","\n","        # model parameters\n","        if \"load_model\" in train_config.keys():\n","            model_load_path = train_config[\"load_model\"]\n","            self.logger.info(\"Loading model from %s\", model_load_path)\n","            reset_best_ckpt = train_config.get(\"reset_best_ckpt\", False)\n","            reset_scheduler = train_config.get(\"reset_scheduler\", False)\n","            reset_optimizer = train_config.get(\"reset_optimizer\", False)\n","            self.init_from_checkpoint(\n","                model_load_path,\n","                reset_best_ckpt=reset_best_ckpt,\n","                reset_scheduler=reset_scheduler,\n","                reset_optimizer=reset_optimizer,\n","            )\n","\n","    def _get_recognition_params(self, train_config) -> None:\n","        # NOTE (Cihan): The blank label is the silence index in the gloss vocabulary.\n","        #   There is an assertion in the GlossVocabulary class's __init__.\n","        #   This is necessary to do TensorFlow decoding, as it is hardcoded\n","        #   Currently it is hardcoded as 0.\n","        self.gls_silence_token = self.model.gls_vocab.stoi[SIL_TOKEN]\n","        assert self.gls_silence_token == 0\n","\n","        self.recognition_loss_function = torch.nn.CTCLoss(\n","            blank=self.gls_silence_token, zero_infinity=True\n","        )\n","        self.recognition_loss_weight = train_config.get(\"recognition_loss_weight\", 1.0)\n","        self.eval_recognition_beam_size = train_config.get(\n","            \"eval_recognition_beam_size\", 1\n","        )\n","\n","    def _get_translation_params(self, train_config) -> None:\n","        self.label_smoothing = train_config.get(\"label_smoothing\", 0.0)\n","        self.translation_loss_function = XentLoss(\n","            pad_index=self.txt_pad_index, smoothing=self.label_smoothing\n","        )\n","        self.translation_normalization_mode = train_config.get(\n","            \"translation_normalization\", \"batch\"\n","        )\n","        if self.translation_normalization_mode not in [\"batch\", \"tokens\"]:\n","            raise ValueError(\n","                \"Invalid normalization {}.\".format(self.translation_normalization_mode)\n","            )\n","        self.translation_loss_weight = train_config.get(\"translation_loss_weight\", 1.0)\n","        self.eval_translation_beam_size = train_config.get(\n","            \"eval_translation_beam_size\", 1\n","        )\n","        self.eval_translation_beam_alpha = train_config.get(\n","            \"eval_translation_beam_alpha\", -1\n","        )\n","        self.translation_max_output_length = train_config.get(\n","            \"translation_max_output_length\", None\n","        )\n","\n","    def _save_checkpoint(self) -> None:\n","        \"\"\"\n","        Save the model's current parameters and the training state to a\n","        checkpoint.\n","\n","        The training state contains the total number of training steps,\n","        the total number of training tokens,\n","        the best checkpoint score and iteration so far,\n","        and optimizer and scheduler states.\n","\n","        \"\"\"\n","        model_path = \"{}/{}.ckpt\".format(self.model_dir, self.steps)\n","        state = {\n","            \"steps\": self.steps,\n","            \"total_txt_tokens\": self.total_txt_tokens if self.do_translation else 0,\n","            \"total_gls_tokens\": self.total_gls_tokens if self.do_recognition else 0,\n","            \"best_ckpt_score\": self.best_ckpt_score,\n","            \"best_all_ckpt_scores\": self.best_all_ckpt_scores,\n","            \"best_ckpt_iteration\": self.best_ckpt_iteration,\n","            \"model_state\": self.model.state_dict(),\n","            \"optimizer_state\": self.optimizer.state_dict(),\n","            \"scheduler_state\": self.scheduler.state_dict()\n","            if self.scheduler is not None\n","            else None,\n","        }\n","        torch.save(state, model_path)\n","        if self.ckpt_queue.full():\n","            to_delete = self.ckpt_queue.get()  # delete oldest ckpt\n","            try:\n","                os.remove(to_delete)\n","            except FileNotFoundError:\n","                self.logger.warning(\n","                    \"Wanted to delete old checkpoint %s but \" \"file does not exist.\",\n","                    to_delete,\n","                )\n","\n","        self.ckpt_queue.put(model_path)\n","\n","        # create/modify symbolic link for best checkpoint\n","        #symlink_update(\n","        #    \"{}.ckpt\".format(self.steps), \"{}/best.ckpt\".format(self.model_dir)\n","        #)\n","\n","    def init_from_checkpoint(\n","        self,\n","        path: str,\n","        reset_best_ckpt: bool = False,\n","        reset_scheduler: bool = False,\n","        reset_optimizer: bool = False,\n","    ) -> None:\n","        \"\"\"\n","        Initialize the trainer from a given checkpoint file.\n","\n","        This checkpoint file contains not only model parameters, but also\n","        scheduler and optimizer states, see `self._save_checkpoint`.\n","\n","        :param path: path to checkpoint\n","        :param reset_best_ckpt: reset tracking of the best checkpoint,\n","                                use for domain adaptation with a new dev\n","                                set or when using a new metric for fine-tuning.\n","        :param reset_scheduler: reset the learning rate scheduler, and do not\n","                                use the one stored in the checkpoint.\n","        :param reset_optimizer: reset the optimizer, and do not use the one\n","                                stored in the checkpoint.\n","        \"\"\"\n","        model_checkpoint = load_checkpoint(path=path, use_cuda=self.use_cuda)\n","\n","        # restore model and optimizer parameters\n","        self.model.load_state_dict(model_checkpoint[\"model_state\"])\n","\n","        if not reset_optimizer:\n","            self.optimizer.load_state_dict(model_checkpoint[\"optimizer_state\"])\n","        else:\n","            self.logger.info(\"Reset optimizer.\")\n","\n","        if not reset_scheduler:\n","            if (\n","                model_checkpoint[\"scheduler_state\"] is not None\n","                and self.scheduler is not None\n","            ):\n","                self.scheduler.load_state_dict(model_checkpoint[\"scheduler_state\"])\n","        else:\n","            self.logger.info(\"Reset scheduler.\")\n","\n","        # restore counts\n","        self.steps = model_checkpoint[\"steps\"]\n","        self.total_txt_tokens = model_checkpoint[\"total_txt_tokens\"]\n","        self.total_gls_tokens = model_checkpoint[\"total_gls_tokens\"]\n","\n","        if not reset_best_ckpt:\n","            self.best_ckpt_score = model_checkpoint[\"best_ckpt_score\"]\n","            self.best_all_ckpt_scores = model_checkpoint[\"best_all_ckpt_scores\"]\n","            self.best_ckpt_iteration = model_checkpoint[\"best_ckpt_iteration\"]\n","        else:\n","            self.logger.info(\"Reset tracking of the best checkpoint.\")\n","\n","        # move parameters to cuda\n","        if self.use_cuda:\n","            self.model.cuda()\n","\n","    def train_and_validate(self, train_data: Dataset, valid_data: Dataset) -> None:\n","        \"\"\"\n","        Train the model and validate it from time to time on the validation set.\n","\n","        :param train_data: training data\n","        :param valid_data: validation data\n","        \"\"\"\n","        train_iter = make_data_iter(\n","            train_data,\n","            batch_size=self.batch_size,\n","            batch_type=self.batch_type,\n","            train=True,\n","            shuffle=self.shuffle,\n","        )\n","        epoch_no = None\n","        for epoch_no in range(self.epochs):\n","            self.logger.info(\"EPOCH %d\", epoch_no + 1)\n","\n","            if self.scheduler is not None and self.scheduler_step_at == \"epoch\":\n","                self.scheduler.step(epoch=epoch_no)\n","\n","            self.model.train()\n","            start = time.time()\n","            total_valid_duration = 0\n","            count = self.batch_multiplier - 1\n","\n","            if self.do_recognition:\n","                processed_gls_tokens = self.total_gls_tokens\n","                epoch_recognition_loss = 0\n","            if self.do_translation:\n","                processed_txt_tokens = self.total_txt_tokens\n","                epoch_translation_loss = 0\n","\n","            for batch in iter(train_iter):\n","                # reactivate training\n","                # create a Batch object from torchtext batch\n","                batch = Batch(\n","                    is_train=True,\n","                    torch_batch=batch,\n","                    txt_pad_index=self.txt_pad_index,\n","                    sgn_dim=self.feature_size,\n","                    use_cuda=self.use_cuda,\n","                    frame_subsampling_ratio=self.frame_subsampling_ratio,\n","                    random_frame_subsampling=self.random_frame_subsampling,\n","                    random_frame_masking_ratio=self.random_frame_masking_ratio,\n","                )\n","\n","                # only update every batch_multiplier batches\n","                # see https://medium.com/@davidlmorton/\n","                # increasing-mini-batch-size-without-increasing-\n","                # memory-6794e10db672\n","                update = count == 0\n","\n","                recognition_loss, translation_loss = self._train_batch(\n","                    batch, update=update\n","                )\n","\n","                if self.do_recognition:\n","                    epoch_recognition_loss += recognition_loss.detach().cpu().numpy()\n","\n","                if self.do_translation:\n","                    epoch_translation_loss += translation_loss.detach().cpu().numpy()\n","\n","                count = self.batch_multiplier if update else count\n","                count -= 1\n","\n","                if (\n","                    self.scheduler is not None\n","                    and self.scheduler_step_at == \"step\"\n","                    and update\n","                ):\n","                    self.scheduler.step()\n","\n","                # log learning progress\n","                if self.steps % self.logging_freq == 0 and update:\n","                    elapsed = time.time() - start - total_valid_duration\n","\n","                    log_out = \"[Epoch: {:03d} Step: {:08d}] \".format(\n","                        epoch_no + 1, self.steps,\n","                    )\n","\n","                    if self.do_recognition:\n","                        elapsed_gls_tokens = (\n","                            self.total_gls_tokens - processed_gls_tokens\n","                        )\n","                        processed_gls_tokens = self.total_gls_tokens\n","                        log_out += \"Batch Recognition Loss: {:10.6f} => \".format(\n","                            recognition_loss\n","                        )\n","                        log_out += \"Gls Tokens per Sec: {:8.0f} || \".format(\n","                            elapsed_gls_tokens / elapsed\n","                        )\n","                    if self.do_translation:\n","                        elapsed_txt_tokens = (\n","                            self.total_txt_tokens - processed_txt_tokens\n","                        )\n","                        processed_txt_tokens = self.total_txt_tokens\n","                        log_out += \"Batch Translation Loss: {:10.6f} => \".format(\n","                            translation_loss\n","                        )\n","                        log_out += \"Txt Tokens per Sec: {:8.0f} || \".format(\n","                            elapsed_txt_tokens / elapsed\n","                        )\n","                    log_out += \"Lr: {:.6f}\".format(self.optimizer.param_groups[0][\"lr\"])\n","                    self.logger.info(log_out)\n","                    start = time.time()\n","                    total_valid_duration = 0\n","\n","                # validate on the entire dev set\n","                if self.steps % self.validation_freq == 0 and update:\n","                    valid_start_time = time.time()\n","                    # TODO (Cihan): There must be a better way of passing\n","                    #   these recognition only and translation only parameters!\n","                    #   Maybe have a NamedTuple with optional fields?\n","                    #   Hmm... Future Cihan's problem.\n","                    val_res = validate_on_data(\n","                        model=self.model,\n","                        data=valid_data,\n","                        batch_size=self.eval_batch_size,\n","                        use_cuda=self.use_cuda,\n","                        batch_type=self.eval_batch_type,\n","                        dataset_version=self.dataset_version,\n","                        sgn_dim=self.feature_size,\n","                        txt_pad_index=self.txt_pad_index,\n","                        # Recognition Parameters\n","                        do_recognition=self.do_recognition,\n","                        recognition_loss_function=self.recognition_loss_function\n","                        if self.do_recognition\n","                        else None,\n","                        recognition_loss_weight=self.recognition_loss_weight\n","                        if self.do_recognition\n","                        else None,\n","                        recognition_beam_size=self.eval_recognition_beam_size\n","                        if self.do_recognition\n","                        else None,\n","                        # Translation Parameters\n","                        do_translation=self.do_translation,\n","                        translation_loss_function=self.translation_loss_function\n","                        if self.do_translation\n","                        else None,\n","                        translation_max_output_length=self.translation_max_output_length\n","                        if self.do_translation\n","                        else None,\n","                        level=self.level if self.do_translation else None,\n","                        translation_loss_weight=self.translation_loss_weight\n","                        if self.do_translation\n","                        else None,\n","                        translation_beam_size=self.eval_translation_beam_size\n","                        if self.do_translation\n","                        else None,\n","                        translation_beam_alpha=self.eval_translation_beam_alpha\n","                        if self.do_translation\n","                        else None,\n","                        frame_subsampling_ratio=self.frame_subsampling_ratio,\n","                    )\n","                    self.model.train()\n","\n","                    if self.early_stopping_metric == \"recognition_loss\":\n","                        assert self.do_recognition\n","                        ckpt_score = val_res[\"valid_recognition_loss\"]\n","                    elif self.early_stopping_metric == \"translation_loss\":\n","                        assert self.do_translation\n","                        ckpt_score = val_res[\"valid_translation_loss\"]\n","                    elif self.early_stopping_metric in [\"ppl\", \"perplexity\"]:\n","                        assert self.do_translation\n","                        ckpt_score = val_res[\"valid_ppl\"]\n","                    else:\n","                        ckpt_score = val_res[\"valid_scores\"][self.eval_metric]\n","\n","                    new_best = False\n","                    if self.is_best(ckpt_score):\n","                        self.best_ckpt_score = ckpt_score\n","                        self.best_all_ckpt_scores = val_res[\"valid_scores\"]\n","                        self.best_ckpt_iteration = self.steps\n","                        self.logger.info(\n","                            \"Hooray! New best validation result [%s]!\",\n","                            self.early_stopping_metric,\n","                        )\n","                        if self.ckpt_queue.maxsize > 0:\n","                            self.logger.info(\"Saving new checkpoint.\")\n","                            new_best = True\n","                            self._save_checkpoint()\n","\n","                    if (\n","                        self.scheduler is not None\n","                        and self.scheduler_step_at == \"validation\"\n","                    ):\n","                        prev_lr = self.scheduler.optimizer.param_groups[0][\"lr\"]\n","                        self.scheduler.step(ckpt_score)\n","                        now_lr = self.scheduler.optimizer.param_groups[0][\"lr\"]\n","\n","                        if prev_lr != now_lr:\n","                            if self.last_best_lr != prev_lr:\n","                                self.stop = True\n","\n","                    # append to validation report\n","                    self._add_report(\n","                        valid_scores=val_res[\"valid_scores\"],\n","                        valid_recognition_loss=val_res[\"valid_recognition_loss\"]\n","                        if self.do_recognition\n","                        else None,\n","                        valid_translation_loss=val_res[\"valid_translation_loss\"]\n","                        if self.do_translation\n","                        else None,\n","                        valid_ppl=val_res[\"valid_ppl\"] if self.do_translation else None,\n","                        eval_metric=self.eval_metric,\n","                        new_best=new_best,\n","                    )\n","                    valid_duration = time.time() - valid_start_time\n","                    total_valid_duration += valid_duration\n","                    self.logger.info(\n","                        \"Validation result at epoch %3d, step %8d: duration: %.4fs\\n\\t\"\n","                        \"Recognition Beam Size: %d\\t\"\n","                        \"Translation Beam Size: %d\\t\"\n","                        \"Translation Beam Alpha: %d\\n\\t\"\n","                        \"Recognition Loss: %4.5f\\t\"\n","                        \"Translation Loss: %4.5f\\t\"\n","                        \"PPL: %4.5f\\n\\t\"\n","                        \"Eval Metric: %s\\n\\t\"\n","                        \"WER %3.2f\\t(DEL: %3.2f,\\tINS: %3.2f,\\tSUB: %3.2f)\\n\\t\"\n","                        \"BLEU-1 %.2f\\n\\t\"\n","                        \"CHRF %.2f\\t\"\n","                        \"ROUGE %.2f\",\n","                        epoch_no + 1,\n","                        self.steps,\n","                        valid_duration,\n","                        self.eval_recognition_beam_size if self.do_recognition else -1,\n","                        self.eval_translation_beam_size if self.do_translation else -1,\n","                        self.eval_translation_beam_alpha if self.do_translation else -1,\n","                        val_res[\"valid_recognition_loss\"]\n","                        if self.do_recognition\n","                        else -1,\n","                        val_res[\"valid_translation_loss\"]\n","                        if self.do_translation\n","                        else -1,\n","                        val_res[\"valid_ppl\"] if self.do_translation else -1,\n","                        self.eval_metric.upper(),\n","                        # WER\n","                        val_res[\"valid_scores\"][\"wer\"] if self.do_recognition else -1,\n","                        val_res[\"valid_scores\"][\"wer_scores\"][\"del_rate\"]\n","                        if self.do_recognition\n","                        else -1,\n","                        val_res[\"valid_scores\"][\"wer_scores\"][\"ins_rate\"]\n","                        if self.do_recognition\n","                        else -1,\n","                        val_res[\"valid_scores\"][\"wer_scores\"][\"sub_rate\"]\n","                        if self.do_recognition\n","                        else -1,\n","                        # BLEU\n","                        val_res[\"valid_scores\"][\"bleu\"] if self.do_translation else -1,\n","                        # Other\n","                        val_res[\"valid_scores\"][\"chrf\"] if self.do_translation else -1,\n","                        val_res[\"valid_scores\"][\"rouge\"] if self.do_translation else -1,\n","                    )\n","\n","                if self.stop:\n","                    break\n","            if self.stop:\n","                if (\n","                    self.scheduler is not None\n","                    and self.scheduler_step_at == \"validation\"\n","                    and self.last_best_lr != prev_lr\n","                ):\n","                    self.logger.info(\n","                        \"Training ended since there were no improvements in\"\n","                        \"the last learning rate step: %f\",\n","                        prev_lr,\n","                    )\n","                else:\n","                    self.logger.info(\n","                        \"Training ended since minimum lr %f was reached.\",\n","                        self.learning_rate_min,\n","                    )\n","                break\n","\n","            self.logger.info(\n","                \"Epoch %3d: Total Training Recognition Loss %.2f \"\n","                \" Total Training Translation Loss %.2f \",\n","                epoch_no + 1,\n","                epoch_recognition_loss if self.do_recognition else -1,\n","                epoch_translation_loss if self.do_translation else -1,\n","            )\n","        else:\n","            self.logger.info(\"Training ended after %3d epochs.\", epoch_no + 1)\n","        self.logger.info(\n","            \"Best validation result at step %8d: %6.2f %s.\",\n","            self.best_ckpt_iteration,\n","            self.best_ckpt_score,\n","            self.early_stopping_metric,\n","        )\n","\n","    def _train_batch(self, batch: Batch, update: bool = True) -> (Tensor, Tensor):\n","        \"\"\"\n","        Train the model on one batch: Compute the loss, make a gradient step.\n","\n","        :param batch: training batch\n","        :param update: if False, only store gradient. if True also make update\n","        :return normalized_recognition_loss: Normalized recognition loss\n","        :return normalized_translation_loss: Normalized translation loss\n","        \"\"\"\n","\n","        recognition_loss, translation_loss = self.model.get_loss_for_batch(\n","            batch=batch,\n","            recognition_loss_function=self.recognition_loss_function\n","            if self.do_recognition\n","            else None,\n","            translation_loss_function=self.translation_loss_function\n","            if self.do_translation\n","            else None,\n","            recognition_loss_weight=self.recognition_loss_weight\n","            if self.do_recognition\n","            else None,\n","            translation_loss_weight=self.translation_loss_weight\n","            if self.do_translation\n","            else None,\n","        )\n","\n","        # normalize translation loss\n","        if self.do_translation:\n","            if self.translation_normalization_mode == \"batch\":\n","                txt_normalization_factor = batch.num_seqs\n","            elif self.translation_normalization_mode == \"tokens\":\n","                txt_normalization_factor = batch.num_txt_tokens\n","            else:\n","                raise NotImplementedError(\"Only normalize by 'batch' or 'tokens'\")\n","\n","            # division needed since loss.backward sums the gradients until updated\n","            normalized_translation_loss = translation_loss / (\n","                txt_normalization_factor * self.batch_multiplier\n","            )\n","        else:\n","            normalized_translation_loss = 0\n","\n","        # TODO (Cihan): Add Gloss Token normalization (?)\n","        #   I think they are already being normalized by batch\n","        #   I need to think about if I want to normalize them by token.\n","        if self.do_recognition:\n","            normalized_recognition_loss = recognition_loss / self.batch_multiplier\n","        else:\n","            normalized_recognition_loss = 0\n","\n","        total_loss = normalized_recognition_loss + normalized_translation_loss\n","        # compute gradients\n","        total_loss.backward()\n","\n","        if self.clip_grad_fun is not None:\n","            # clip gradients (in-place)\n","            self.clip_grad_fun(params=self.model.parameters())\n","\n","        if update:\n","            # make gradient step\n","            self.optimizer.step()\n","            self.optimizer.zero_grad()\n","\n","            # increment step counter\n","            self.steps += 1\n","\n","        # increment token counter\n","        if self.do_recognition:\n","            self.total_gls_tokens += batch.num_gls_tokens\n","        if self.do_translation:\n","            self.total_txt_tokens += batch.num_txt_tokens\n","\n","        return normalized_recognition_loss, normalized_translation_loss\n","\n","    def _add_report(\n","        self,\n","        valid_scores: Dict,\n","        valid_recognition_loss: float,\n","        valid_translation_loss: float,\n","        valid_ppl: float,\n","        eval_metric: str,\n","        new_best: bool = False,\n","    ) -> None:\n","        \"\"\"\n","        Append a one-line report to validation logging file.\n","\n","        :param valid_scores: Dictionary of validation scores\n","        :param valid_recognition_loss: validation loss (sum over whole validation set)\n","        :param valid_translation_loss: validation loss (sum over whole validation set)\n","        :param valid_ppl: validation perplexity\n","        :param eval_metric: evaluation metric, e.g. \"bleu\"\n","        :param new_best: whether this is a new best model\n","        \"\"\"\n","        current_lr = -1\n","        # ignores other param groups for now\n","        for param_group in self.optimizer.param_groups:\n","            current_lr = param_group[\"lr\"]\n","\n","        if new_best:\n","            self.last_best_lr = current_lr\n","\n","        if current_lr < self.learning_rate_min:\n","            self.stop = True\n","\n","        with open(self.valid_report_file, \"a\", encoding=\"utf-8\") as opened_file:\n","            opened_file.write(\n","                \"Steps: {}\\t\"\n","                \"Recognition Loss: {:.5f}\\t\"\n","                \"Translation Loss: {:.5f}\\t\"\n","                \"PPL: {:.5f}\\t\"\n","                \"Eval Metric: {}\\t\"\n","                \"WER {:.2f}\\t(DEL: {:.2f},\\tINS: {:.2f},\\tSUB: {:.2f})\\t\"\n","                \"BLEU {:.2f}\\t\"\n","                \"CHRF {:.2f}\\t\"\n","                \"ROUGE {:.2f}\\t\"\n","                \"LR: {:.8f}\\t{}\\n\".format(\n","                    self.steps,\n","                    valid_recognition_loss if self.do_recognition else -1,\n","                    valid_translation_loss if self.do_translation else -1,\n","                    valid_ppl if self.do_translation else -1,\n","                    eval_metric,\n","                    # WER\n","                    valid_scores[\"wer\"] if self.do_recognition else -1,\n","                    valid_scores[\"wer_scores\"][\"del_rate\"]\n","                    if self.do_recognition\n","                    else -1,\n","                    valid_scores[\"wer_scores\"][\"ins_rate\"]\n","                    if self.do_recognition\n","                    else -1,\n","                    valid_scores[\"wer_scores\"][\"sub_rate\"]\n","                    if self.do_recognition\n","                    else -1,\n","                    # BLEU\n","                    valid_scores[\"bleu\"] if self.do_translation else -1,\n","                    # Other\n","                    valid_scores[\"chrf\"] if self.do_translation else -1,\n","                    valid_scores[\"rouge\"] if self.do_translation else -1,\n","                    current_lr,\n","                    \"*\" if new_best else \"\",\n","                )\n","            )\n","\n","    def _log_parameters_list(self) -> None:\n","        \"\"\"\n","        Write all model parameters (name, shape) to the log.\n","        \"\"\"\n","        model_parameters = filter(lambda p: p.requires_grad, self.model.parameters())\n","        n_params = sum([np.prod(p.size()) for p in model_parameters])\n","        self.logger.info(\"Total params: %d\", n_params)\n","        trainable_params = [\n","            n for (n, p) in self.model.named_parameters() if p.requires_grad\n","        ]\n","        self.logger.info(\"Trainable parameters: %s\", sorted(trainable_params))\n","        assert trainable_params\n","\n","    def _log_examples(\n","        self,\n","        sequences: List[str],\n","        gls_references: List[str],\n","        gls_hypotheses: List[str],\n","        txt_references: List[str],\n","        txt_hypotheses: List[str],\n","    ) -> None:\n","        \"\"\"\n","        Log `self.num_valid_log` number of samples from valid.\n","\n","        :param sequences: sign video sequence names (list of strings)\n","        :param txt_hypotheses: decoded txt hypotheses (list of strings)\n","        :param txt_references: decoded txt references (list of strings)\n","        :param gls_hypotheses: decoded gls hypotheses (list of strings)\n","        :param gls_references: decoded gls references (list of strings)\n","        \"\"\"\n","\n","        if self.do_recognition:\n","            assert len(gls_references) == len(gls_hypotheses)\n","            num_sequences = len(gls_hypotheses)\n","        if self.do_translation:\n","            assert len(txt_references) == len(txt_hypotheses)\n","            num_sequences = len(txt_hypotheses)\n","\n","        rand_idx = np.sort(np.random.permutation(num_sequences)[: self.num_valid_log])\n","        self.logger.info(\"Logging Recognition and Translation Outputs\")\n","        self.logger.info(\"=\" * 120)\n","        for ri in rand_idx:\n","            self.logger.info(\"Logging Sequence: %s\", sequences[ri])\n","            if self.do_recognition:\n","                gls_res = wer_single(r=gls_references[ri], h=gls_hypotheses[ri])\n","                self.logger.info(\n","                    \"\\tGloss Reference :\\t%s\", gls_res[\"alignment_out\"][\"align_ref\"]\n","                )\n","                self.logger.info(\n","                    \"\\tGloss Hypothesis:\\t%s\", gls_res[\"alignment_out\"][\"align_hyp\"]\n","                )\n","                self.logger.info(\n","                    \"\\tGloss Alignment :\\t%s\", gls_res[\"alignment_out\"][\"alignment\"]\n","                )\n","            if self.do_recognition and self.do_translation:\n","                self.logger.info(\"\\t\" + \"-\" * 116)\n","            if self.do_translation:\n","                txt_res = wer_single(r=txt_references[ri], h=txt_hypotheses[ri])\n","                self.logger.info(\n","                    \"\\tText Reference  :\\t%s\", txt_res[\"alignment_out\"][\"align_ref\"]\n","                )\n","                self.logger.info(\n","                    \"\\tText Hypothesis :\\t%s\", txt_res[\"alignment_out\"][\"align_hyp\"]\n","                )\n","                self.logger.info(\n","                    \"\\tText Alignment  :\\t%s\", txt_res[\"alignment_out\"][\"alignment\"]\n","                )\n","            self.logger.info(\"=\" * 120)\n","\n","    def _store_outputs(\n","        self, tag: str, sequence_ids: List[str], hypotheses: List[str], sub_folder=None\n","    ) -> None:\n","        \"\"\"\n","        Write current validation outputs to file in `self.model_dir.`\n","\n","        :param hypotheses: list of strings\n","        \"\"\"\n","        if sub_folder:\n","            out_folder = os.path.join(self.model_dir, sub_folder)\n","            if not os.path.exists(out_folder):\n","                os.makedirs(out_folder)\n","            current_valid_output_file = \"{}/{}.{}\".format(out_folder, self.steps, tag)\n","        else:\n","            out_folder = self.model_dir\n","            current_valid_output_file = \"{}/{}\".format(out_folder, tag)\n","\n","        with open(current_valid_output_file, \"w\", encoding=\"utf-8\") as opened_file:\n","            for seq, hyp in zip(sequence_ids, hypotheses):\n","                opened_file.write(\"{}|{}\\n\".format(seq, hyp))\n"],"execution_count":57,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i4MXrKz0cXlp"},"source":["### Validation Code"]},{"cell_type":"code","metadata":{"id":"QGFLRL02cfxu","executionInfo":{"status":"ok","timestamp":1606800515043,"user_tz":480,"elapsed":4343,"user":{"displayName":"Amy Lee","photoUrl":"","userId":"14323350772356815607"}}},"source":["def validate_on_data(\n","    model: SignModel,\n","    data: Dataset,\n","    batch_size: int,\n","    use_cuda: bool,\n","    sgn_dim: int,\n","    do_recognition: bool,\n","    recognition_loss_function: torch.nn.Module,\n","    recognition_loss_weight: int,\n","    do_translation: bool,\n","    translation_loss_function: torch.nn.Module,\n","    translation_loss_weight: int,\n","    translation_max_output_length: int,\n","    level: str,\n","    txt_pad_index: int,\n","    recognition_beam_size: int = 1,\n","    translation_beam_size: int = 1,\n","    translation_beam_alpha: int = -1,\n","    batch_type: str = \"sentence\",\n","    dataset_version: str = \"phoenix_2014_trans\",\n","    frame_subsampling_ratio: int = None,\n",") -> (\n","    float,\n","    float,\n","    float,\n","    List[str],\n","    List[List[str]],\n","    List[str],\n","    List[str],\n","    List[List[str]],\n","    List[np.array],\n","):\n","    \"\"\"\n","    Generate translations for the given data.\n","    If `loss_function` is not None and references are given,\n","    also compute the loss.\n","\n","    :return:\n","        - current_valid_score: current validation score [eval_metric],\n","        - valid_loss: validation loss,\n","        - valid_ppl:, validation perplexity,\n","        - valid_sources: validation sources,\n","        - valid_sources_raw: raw validation sources (before post-processing),\n","        - valid_references: validation references,\n","        - valid_hypotheses: validation_hypotheses,\n","        - decoded_valid: raw validation hypotheses (before post-processing),\n","        - valid_attention_scores: attention scores for validation hypotheses\n","    \"\"\"\n","    valid_iter = make_data_iter(\n","        dataset=data,\n","        batch_size=batch_size,\n","        batch_type=batch_type,\n","        shuffle=False,\n","        train=False,\n","    )\n","\n","    # disable dropout\n","    model.eval()\n","    # don't track gradients during validation\n","    with torch.no_grad():\n","        all_gls_outputs = []\n","        all_txt_outputs = []\n","        all_attention_scores = []\n","        total_recognition_loss = 0\n","        total_translation_loss = 0\n","        total_num_txt_tokens = 0\n","        total_num_gls_tokens = 0\n","        total_num_seqs = 0\n","        for valid_batch in iter(valid_iter):\n","            batch = Batch(\n","                is_train=False,\n","                torch_batch=valid_batch,\n","                txt_pad_index=txt_pad_index,\n","                sgn_dim=sgn_dim,\n","                use_cuda=use_cuda,\n","                frame_subsampling_ratio=frame_subsampling_ratio,\n","            )\n","            sort_reverse_index = batch.sort_by_sgn_lengths()\n","\n","            batch_recognition_loss, batch_translation_loss = model.get_loss_for_batch(\n","                batch=batch,\n","                recognition_loss_function=recognition_loss_function\n","                if do_recognition\n","                else None,\n","                translation_loss_function=translation_loss_function\n","                if do_translation\n","                else None,\n","                recognition_loss_weight=recognition_loss_weight\n","                if do_recognition\n","                else None,\n","                translation_loss_weight=translation_loss_weight\n","                if do_translation\n","                else None,\n","            )\n","\n","            if do_translation:\n","                total_translation_loss += batch_translation_loss\n","                total_num_txt_tokens += batch.num_txt_tokens\n","            total_num_seqs += batch.num_seqs\n","            \n","            (\n","                batch_gls_predictions,\n","                batch_txt_predictions,\n","                batch_attention_scores,\n","            ) = model.run_batch(\n","                batch=batch,\n","                recognition_beam_size=recognition_beam_size if do_recognition else None,\n","                translation_beam_size=translation_beam_size if do_translation else None,\n","                translation_beam_alpha=translation_beam_alpha\n","                if do_translation\n","                else None,\n","                translation_max_output_length=translation_max_output_length\n","                if do_translation\n","                else None,\n","            )\n","\n","            # sort outputs back to original order\n","            if do_recognition:\n","                all_gls_outputs.extend(\n","                    [batch_gls_predictions[sri] for sri in sort_reverse_index]\n","                )\n","            if do_translation:\n","                all_txt_outputs.extend(batch_txt_predictions[sort_reverse_index])\n","            all_attention_scores.extend(\n","                batch_attention_scores[sort_reverse_index]\n","                if batch_attention_scores is not None\n","                else []\n","            )\n","\n","        if do_translation:\n","            assert len(all_txt_outputs) == len(data)\n","            if (\n","                translation_loss_function is not None\n","                and translation_loss_weight != 0\n","                and total_num_txt_tokens > 0\n","            ):\n","                # total validation translation loss\n","                valid_translation_loss = total_translation_loss\n","                # exponent of token-level negative log prob\n","                valid_ppl = torch.exp(total_translation_loss / total_num_txt_tokens)\n","            else:\n","                valid_translation_loss = -1\n","                valid_ppl = -1\n","            # decode back to symbols\n","            decoded_txt = model.txt_vocab.arrays_to_sentences(arrays=all_txt_outputs)\n","            # evaluate with metric on full dataset\n","            join_char = \" \" if level in [\"word\", \"bpe\"] else \"\"\n","            # Construct text sequences for metrics\n","            txt_ref = [join_char.join(t) for t in data.txt]\n","            txt_hyp = [join_char.join(t) for t in decoded_txt]\n","\n","            # post-process\n","            if level == \"bpe\":\n","                txt_ref = [bpe_postprocess(v) for v in txt_ref]\n","                txt_hyp = [bpe_postprocess(v) for v in txt_hyp]\n","            assert len(txt_ref) == len(txt_hyp)\n","\n","            # TXT Metrics\n","            txt_bleu = bleu(references=txt_ref, hypotheses=txt_hyp)\n","            txt_chrf = chrf(references=txt_ref, hypotheses=txt_hyp)\n","            txt_rouge = rouge(references=txt_ref, hypotheses=txt_hyp)\n","\n","        valid_scores = {}\n","        if do_translation:\n","            valid_scores[\"bleu\"] = txt_bleu[\"bleu1\"]\n","            valid_scores[\"bleu_scores\"] = txt_bleu\n","            valid_scores[\"chrf\"] = txt_chrf\n","            valid_scores[\"rouge\"] = txt_rouge\n","\n","    results = {\n","        \"valid_scores\": valid_scores,\n","        \"all_attention_scores\": all_attention_scores,\n","    }\n","\n","    if do_translation:\n","        results[\"valid_translation_loss\"] = valid_translation_loss\n","        results[\"valid_ppl\"] = valid_ppl\n","        results[\"decoded_txt\"] = decoded_txt\n","        results[\"txt_ref\"] = txt_ref\n","        results[\"txt_hyp\"] = txt_hyp\n","\n","    return results"],"execution_count":58,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rhlefd3IcmFi"},"source":["### Metrics"]},{"cell_type":"code","metadata":{"id":"sSzJ_NuacpAk","executionInfo":{"status":"ok","timestamp":1606798491586,"user_tz":480,"elapsed":763,"user":{"displayName":"Amy Lee","photoUrl":"","userId":"14323350772356815607"}}},"source":["# metrics\n","def chrf(references, hypotheses):\n","    \"\"\"\n","    Character F-score from sacrebleu\n","\n","    :param hypotheses: list of hypotheses (strings)\n","    :param references: list of references (strings)\n","    :return:\n","    \"\"\"\n","    return (\n","        sacrebleu.corpus_chrf(hypotheses=hypotheses, references=references).score * 100\n","    )\n","\n","\n","def bleu(references, hypotheses):\n","    \"\"\"\n","    Raw corpus BLEU from sacrebleu (without tokenization)\n","\n","    :param hypotheses: list of hypotheses (strings)\n","    :param references: list of references (strings)\n","    :return:\n","    \"\"\"\n","    bleu_scores = sacrebleu.raw_corpus_bleu(\n","        sys_stream=hypotheses, ref_streams=[references]\n","    ).score\n","    scores = {}\n","    scores[\"bleu1\"] = bleu_scores\n","    return scores\n","    \n","def rouge(references, hypotheses):\n","    rouge_score = 0\n","    n_seq = len(hypotheses)\n","\n","    for h, r in zip(hypotheses, references):\n","        rouge_score += mscoco_rouge.calc_score(hypotheses=[h], references=[r]) / n_seq\n","\n","    return rouge_score * 100"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vmn1B0PBhdzk"},"source":["### Beam Search (not using - doing greedy)"]},{"cell_type":"code","metadata":{"id":"SpLhGu0Nhowo","executionInfo":{"status":"ok","timestamp":1606798498748,"user_tz":480,"elapsed":3522,"user":{"displayName":"Amy Lee","photoUrl":"","userId":"14323350772356815607"}}},"source":["import math\n","def beam_search(\n","    decoder: Decoder,\n","    size: int,\n","    bos_index: int,\n","    eos_index: int,\n","    pad_index: int,\n","    encoder_output: Tensor,\n","    encoder_hidden: Tensor,\n","    src_mask: Tensor,\n","    max_output_length: int,\n","    alpha: float,\n","    embed: Embeddings,\n","    n_best: int = 1,\n",") -> (np.array, np.array):\n","    \"\"\"\n","    Beam search with size k.\n","    Inspired by OpenNMT-py, adapted for Transformer.\n","\n","    In each decoding step, find the k most likely partial hypotheses.\n","\n","    :param decoder:\n","    :param size: size of the beam\n","    :param bos_index:\n","    :param eos_index:\n","    :param pad_index:\n","    :param encoder_output:\n","    :param encoder_hidden:\n","    :param src_mask:\n","    :param max_output_length:\n","    :param alpha: `alpha` factor for length penalty\n","    :param embed:\n","    :param n_best: return this many hypotheses, <= beam (currently only 1)\n","    :return:\n","        - stacked_output: output hypotheses (2d array of indices),\n","        - stacked_attention_scores: attention scores (3d array)\n","    \"\"\"\n","    assert size > 0, \"Beam size must be >0.\"\n","    assert n_best <= size, \"Can only return {} best hypotheses.\".format(size)\n","\n","    # init\n","    transformer = isinstance(decoder, TransformerDecoder)\n","    batch_size = src_mask.size(0)\n","    att_vectors = None  # not used for Transformer\n","\n","    # Recurrent models only: initialize RNN hidden state\n","    # pylint: disable=protected-access\n","    if not transformer:\n","        hidden = decoder._init_hidden(encoder_hidden)\n","    else:\n","        hidden = None\n","\n","    # tile encoder states and decoder initial states beam_size times\n","    if hidden is not None:\n","        hidden = tile(hidden, size, dim=1)  # layers x batch*k x dec_hidden_size\n","\n","    encoder_output = tile(\n","        encoder_output.contiguous(), size, dim=0\n","    )  # batch*k x src_len x enc_hidden_size\n","    src_mask = tile(src_mask, size, dim=0)  # batch*k x 1 x src_len\n","\n","    # Transformer only: create target mask\n","    if transformer:\n","        trg_mask = src_mask.new_ones([1, 1, 1])  # transformer only\n","    else:\n","        trg_mask = None\n","\n","    # numbering elements in the batch\n","    batch_offset = torch.arange(\n","        batch_size, dtype=torch.long, device=encoder_output.device\n","    )\n","\n","    # numbering elements in the extended batch, i.e. beam size copies of each\n","    # batch element\n","    beam_offset = torch.arange(\n","        0, batch_size * size, step=size, dtype=torch.long, device=encoder_output.device\n","    )\n","\n","    # keeps track of the top beam size hypotheses to expand for each element\n","    # in the batch to be further decoded (that are still \"alive\")\n","    alive_seq = torch.full(\n","        [batch_size * size, 1],\n","        bos_index,\n","        dtype=torch.long,\n","        device=encoder_output.device,\n","    )\n","    # Give full probability to the first beam on the first step.\n","    topk_log_probs = torch.zeros(batch_size, size, device=encoder_output.device)\n","    topk_log_probs[:, 1:] = float('-inf')\n","\n","    # Structure that holds finished hypotheses.\n","    hypotheses = [[] for _ in range(batch_size)]\n","\n","    results = {\n","        \"predictions\": [[] for _ in range(batch_size)],\n","        \"scores\": [[] for _ in range(batch_size)],\n","        \"gold_score\": [0] * batch_size,\n","    }\n","\n","    for step in range(max_output_length):\n","\n","        # This decides which part of the predicted sentence we feed to the\n","        # decoder to make the next prediction.\n","        # For Transformer, we feed the complete predicted sentence so far.\n","        # For Recurrent models, only feed the previous target word prediction\n","        if transformer:  # Transformer\n","            decoder_input = alive_seq  # complete prediction so far\n","        else:  # Recurrent\n","            decoder_input = alive_seq[:, -1].view(-1, 1)  # only the last word\n","\n","        # expand current hypotheses\n","        # decode one single step\n","        # logits: logits for final softmax\n","        # pylint: disable=unused-variable\n","        trg_embed = embed(decoder_input)\n","        logits, hidden, att_scores, att_vectors = decoder(\n","            encoder_output=encoder_output,\n","            encoder_hidden=encoder_hidden,\n","            src_mask=src_mask,\n","            trg_embed=trg_embed,\n","            hidden=hidden,\n","            prev_att_vector=att_vectors,\n","            unroll_steps=1,\n","            trg_mask=trg_mask,  # subsequent mask for Transformer only\n","        )\n","\n","        # For the Transformer we made predictions for all time steps up to\n","        # this point, so we only want to know about the last time step.\n","        if transformer:\n","            logits = logits[:, -1]  # keep only the last time step\n","            hidden = None  # we don't need to keep it for transformer\n","\n","        # batch*k x trg_vocab\n","        log_probs = F.log_softmax(logits, dim=-1).squeeze(1)\n","\n","        # multiply probs by the beam probability (=add logprobs)\n","        log_probs += topk_log_probs.view(-1).unsqueeze(1)\n","        curr_scores = log_probs.clone()\n","\n","        # compute length penalty\n","        if alpha > -1:\n","            length_penalty = ((5.0 + (step + 1)) / 6.0) ** alpha\n","            curr_scores /= length_penalty\n","\n","        # flatten log_probs into a list of possibilities\n","        curr_scores = curr_scores.reshape(-1, size * decoder.output_size)\n","\n","        # pick currently best top k hypotheses (flattened order)\n","        topk_scores, topk_ids = curr_scores.topk(size, dim=-1)\n","\n","        if alpha > -1:\n","            # recover original log probs\n","            topk_log_probs = topk_scores * length_penalty\n","        else:\n","            topk_log_probs = topk_scores.clone()\n","\n","        # reconstruct beam origin and true word ids from flattened order\n","        topk_beam_index = topk_ids.div(decoder.output_size)\n","        topk_ids = topk_ids.fmod(decoder.output_size)\n","\n","        # map beam_index to batch_index in the flat representation\n","        batch_index = topk_beam_index + beam_offset[\n","            : topk_beam_index.size(0)\n","        ].unsqueeze(1)\n","        select_indices = batch_index.view(-1)\n","\n","        # append latest prediction\n","\n","        alive_seq = torch.cat(\n","            [alive_seq.index_select(0, select_indices.type(torch.LongTensor).to(encoder_output.device)), topk_ids.view(-1, 1)], -1\n","        )  # batch_size*k x hyp_len\n","\n","        is_finished = topk_ids.eq(eos_index)\n","        if step + 1 == max_output_length:\n","            is_finished.fill_(True)\n","        # end condition is whether the top beam is finished\n","        end_condition = is_finished[:, 0].eq(True)\n","\n","        # save finished hypotheses\n","        if is_finished.any():\n","            predictions = alive_seq.view(-1, size, alive_seq.size(-1))\n","            for i in range(is_finished.size(0)):\n","                b = batch_offset[i]\n","                if end_condition[i]:\n","                    is_finished[i].fill_(True)\n","                finished_hyp = is_finished[i].nonzero().view(-1)\n","                # store finished hypotheses for this batch\n","                for j in finished_hyp:\n","                    # Check if the prediction has more than one EOS.\n","                    # If it has more than one EOS, it means that the prediction should have already\n","                    # been added to the hypotheses, so you don't have to add them again.\n","                    if (predictions[i, j, 1:] == eos_index).nonzero().numel() < 2:\n","                        hypotheses[b].append(\n","                            (\n","                                topk_scores[i, j],\n","                                predictions[i, j, 1:],\n","                            )  # ignore start_token\n","                        )\n","                # if the batch reached the end, save the n_best hypotheses\n","                if end_condition[i]:\n","                    best_hyp = sorted(hypotheses[b], key=lambda x: x[0], reverse=True)\n","                    for n, (score, pred) in enumerate(best_hyp):\n","                        if n >= n_best:\n","                            break\n","                        results[\"scores\"][b].append(score)\n","                        results[\"predictions\"][b].append(pred)\n","            non_finished = end_condition.eq(False).nonzero().view(-1)\n","            # if all sentences are translated, no need to go further\n","            # pylint: disable=len-as-condition\n","            if len(non_finished) == 0:\n","                break\n","            # remove finished batches for the next step\n","            topk_log_probs = topk_log_probs.index_select(0, non_finished)\n","            batch_index = batch_index.index_select(0, non_finished)\n","            batch_offset = batch_offset.index_select(0, non_finished)\n","            alive_seq = predictions.index_select(0, non_finished).view(\n","                -1, alive_seq.size(-1)\n","            )\n","\n","        # reorder indices, outputs and masks\n","        select_indices = batch_index.view(-1)\n","        encoder_output = encoder_output.index_select(0, select_indices)\n","        src_mask = src_mask.index_select(0, select_indices)\n","\n","        if hidden is not None and not transformer:\n","            if isinstance(hidden, tuple):\n","                # for LSTMs, states are tuples of tensors\n","                h, c = hidden\n","                h = h.index_select(1, select_indices)\n","                c = c.index_select(1, select_indices)\n","                hidden = (h, c)\n","            else:\n","                # for GRUs, states are single tensors\n","                hidden = hidden.index_select(1, select_indices)\n","\n","        if att_vectors is not None:\n","            att_vectors = att_vectors.index_select(0, select_indices)\n","\n","    def pad_and_stack_hyps(hyps, pad_value):\n","        filled = (\n","            np.ones((len(hyps), max([h.shape[0] for h in hyps])), dtype=int) * pad_value\n","        )\n","        for j, h in enumerate(hyps):\n","            for k, i in enumerate(h):\n","                filled[j, k] = i\n","        return filled\n","\n","    # from results to stacked outputs\n","    assert n_best == 1\n","    # only works for n_best=1 for now\n","    final_outputs = pad_and_stack_hyps(\n","        [r[0].cpu().numpy() for r in results[\"predictions\"]], pad_value=pad_index\n","    )\n","\n","    return final_outputs, None\n","\n","def validate_on_beam(translation_beam_sizes, \n","                     model,\n","                     test_data,\n","                     batch_size,\n","                     use_cuda,\n","                     batch_type,\n","                     translation_loss_function,\n","                     logger):\n","    frame_subsampling_ratio = None\n","    do_recognition = False\n","    do_translation = True\n","    logger.info(\"=\" * 60)\n","    dev_translation_results = {}\n","    dev_best_bleu_score = float(\"-inf\")\n","    dev_best_translation_beam_size = 1\n","    dev_best_translation_alpha = 1\n","    for tbw in translation_beam_sizes:\n","        dev_translation_results[tbw] = {}\n","        for ta in translation_beam_alphas:\n","            dev_translation_results[tbw][ta] = validate_on_data(\n","                model=model,\n","                data=test_data,\n","                batch_size=batch_size,\n","                use_cuda=use_cuda,\n","                level=level,\n","                sgn_dim=sum(cfg[\"data\"][\"feature_size\"])\n","                if isinstance(cfg[\"data\"][\"feature_size\"], list)\n","                else cfg[\"data\"][\"feature_size\"],\n","                batch_type=batch_type,\n","                do_recognition=do_recognition,\n","                recognition_loss_function=None\n","                if do_recognition\n","                else None,\n","                recognition_loss_weight=1 if do_recognition else None,\n","                recognition_beam_size=1 if do_recognition else None,\n","                do_translation=do_translation,\n","                translation_loss_function=translation_loss_function,\n","                translation_loss_weight=1,\n","                translation_max_output_length=translation_max_output_length,\n","                txt_pad_index=txt_vocab.stoi[PAD_TOKEN],\n","                translation_beam_size=tbw,\n","                translation_beam_alpha=ta,\n","                frame_subsampling_ratio=frame_subsampling_ratio,\n","            )\n","\n","            if (\n","                dev_translation_results[tbw][ta][\"valid_scores\"][\"bleu\"]\n","                > dev_best_bleu_score\n","            ):\n","                dev_best_bleu_score = dev_translation_results[tbw][ta][\n","                    \"valid_scores\"\n","                ][\"bleu\"]\n","                dev_best_translation_beam_size = tbw\n","                dev_best_translation_alpha = ta\n","                dev_best_translation_result = dev_translation_results[tbw][ta]\n","                logger.info(\n","                    \"[DEV] partition [Translation] results:\\n\\t\"\n","                    \"New Best Translation Beam Size: %d and Alpha: %d\\n\\t\"\n","                    \"BLEU %.2f\\t\"\n","                    \"CHRF %.2f\\t\"\n","                    \"ROUGE %.2f\",\n","                    dev_best_translation_beam_size,\n","                    dev_best_translation_alpha,\n","                    dev_best_translation_result[\"valid_scores\"][\"bleu\"],\n","                    dev_best_translation_result[\"valid_scores\"][\"chrf\"],\n","                    dev_best_translation_result[\"valid_scores\"][\"rouge\"],\n","                )\n","                logger.info(\"-\" * 60)\n","    return dev_best_translation_beam_size, dev_best_translation_alpha, dev_best_translation_result, dev_translation_results \n","\n","def _write_to_file(file_path: str, sequence_ids: List[str], hypotheses: List[str]):\n","    with open(file_path, mode=\"w\", encoding=\"utf-8\") as out_file:\n","        for seq, hyp in zip(sequence_ids, hypotheses):\n","            out_file.write(seq + \"|\" + hyp + \"\\n\")"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BuUi8uBFj3CV"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"j44AEifdcq-Q"},"source":["### Run the Evaluation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X1-ee2le69st","executionInfo":{"status":"ok","timestamp":1606719137202,"user_tz":480,"elapsed":889,"user":{"displayName":"Amy Lee","photoUrl":"","userId":"14323350772356815607"}},"outputId":"fcc99ec0-bfe6-4b26-fbe9-175cd418482a"},"source":["with open(os.path.join(DATA_DIR, cfg[\"data\"][\"data_file\"]), 'rb') as f:\n","  testing_this = pickle.load(f)\n","testing_this[\"predictions\"][0].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(68, 241)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r1lay0_XctmT","executionInfo":{"status":"ok","timestamp":1606798522012,"user_tz":480,"elapsed":23215,"user":{"displayName":"Amy Lee","photoUrl":"","userId":"14323350772356815607"}},"outputId":"b2960303-8ddb-4eb1-d7f4-570484ea43bf"},"source":["MAIN_DIR = '/content/drive/My Drive/Colab Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/backtranslation'\n","DATA_DIR = os.path.join(MAIN_DIR, 'data')\n","MODEL_DIR = os.path.join(MAIN_DIR, 'model')\n","CONF_DIR = os.path.join(MAIN_DIR, 'conf')\n","\n","# TODO: set whre you want the translation to be saved\n","version = 'v3.tf'\n","use_cuda = True\n","\n","# load the model\n","cfg = load_config(os.path.join(CONF_DIR, 'sign.yaml'))\n","\n","# TODO: set data path\n","cfg[\"data\"][\"data_file\"] = \"v3.predictions.pkl\"\n","\n","\n","# these were the settings used for the best model version = v6\n","cfg[\"training\"][\"use_cuda\"] = use_cuda\n","cfg[\"training\"][\"batch_size\"] = 32\n","cfg[\"training\"][\"translation_max_output_length\"] = 1\n","cfg[\"model\"][\"decoder\"][\"num_layers\"] = 1\n","cfg[\"model\"][\"encoder\"][\"num_layers\"] = 2\n","cfg[\"model\"][\"encoder\"][\"num_heads\"] = 4\n","cfg[\"model\"][\"decoder\"][\"num_heads\"] = 4\n","\n","set_seed(cfg[\"training\"].get(\"random_seed\", 42))\n","logger = make_logger(model_dir=MODEL_DIR, log_file=f\"inference.{version}.log\")\n","log_cfg(cfg, logger)\n","\n","batch_size = cfg[\"training\"][\"batch_size\"]\n","batch_type = cfg[\"training\"].get(\"batch_type\", \"sentence\")\n","use_cuda = cfg[\"training\"].get(\"use_cuda\", use_cuda)\n","level = cfg[\"data\"][\"level\"]\n","translation_max_output_length = cfg[\"training\"].get(\n","    \"translation_max_output_length\", None\n",")\n","frame_subsampling_ratio = cfg[\"data\"].get(\"frame_subsampling_ratio\", None)\n","do_recognition = False\n","do_translation = True\n","\n","# load the data\n","test_data, txt_vocab = load_data(data_cfg=cfg[\"data\"])\n","ckpt = get_latest_checkpoint(MODEL_DIR, 'v6')\n","model_checkpoint = load_checkpoint(ckpt, use_cuda=use_cuda)\n","\n","model = build_model(\n","      cfg=cfg[\"model\"],\n","      gls_vocab=None,\n","      txt_vocab=txt_vocab,\n","      sgn_dim=sum(cfg[\"data\"][\"feature_size\"])\n","      if isinstance(cfg[\"data\"][\"feature_size\"], list)\n","      else cfg[\"data\"][\"feature_size\"],\n","      do_recognition=False,\n","      do_translation=True,\n","  )\n","model.load_state_dict(model_checkpoint[\"model_state\"])\n","if use_cuda:\n","    model.cuda()\n","\n","\"\"\"\n","if \"testing\" in cfg.keys():\n","    recognition_beam_sizes = cfg[\"testing\"].get(\"recognition_beam_sizes\", [1])\n","    translation_beam_sizes = cfg[\"testing\"].get(\"translation_beam_sizes\", [1])\n","    translation_beam_alphas = cfg[\"testing\"].get(\"translation_beam_alphas\", [-1])\n","else:\n","\"\"\"\n","# setting greedy search\n","recognition_beam_sizes = [1]\n","translation_beam_sizes = [1]\n","translation_beam_alphas = [-1]\n","\n","if \"testing\" in cfg.keys():\n","    max_recognition_beam_size = cfg[\"testing\"].get(\n","        \"max_recognition_beam_size\", None\n","    )\n","    if max_recognition_beam_size is not None:\n","        recognition_beam_sizes = list(range(1, max_recognition_beam_size + 1))\n","\n","translation_loss_function = XentLoss(\n","    pad_index=txt_vocab.stoi[PAD_TOKEN], smoothing=0.0\n",")\n","if use_cuda:\n","    translation_loss_function.cuda()\n","\n","\n","dev_best_translation_beam_size, dev_best_translation_alpha, dev_best_translation_result, dev_translation_results = validate_on_beam(translation_beam_sizes, \n","                                                                                                            model,\n","                                                                                                            test_data,\n","                                                                                                            batch_size,\n","                                                                                                            use_cuda,\n","                                                                                                            batch_type,\n","                                                                                                            translation_loss_function,\n","                                                                                                            logger)\n","\n","\n","logger.info(\"*\" * 60)\n","logger.info(\n","    \"[ALL] partition [Recognition & Translation] results:\\n\\t\"\n","    \"Best CTC Decode Beam Size: %d\\n\\t\"\n","    \"Best Translation Beam Size: %d and Alpha: %d\\n\\t\"\n","    \"WER %3.2f\\t(DEL: %3.2f,\\tINS: %3.2f,\\tSUB: %3.2f)\\n\\t\"\n","    \"BLEU %.2f\\t\"\n","    \"CHRF %.2f\\t\"\n","    \"ROUGE %.2f\",\n","    dev_best_recognition_beam_size if do_recognition else -1,\n","    dev_best_translation_beam_size if do_translation else -1,\n","    dev_best_translation_alpha if do_translation else -1,\n","    dev_best_recognition_result[\"valid_scores\"][\"wer\"] if do_recognition else -1,\n","    dev_best_recognition_result[\"valid_scores\"][\"wer_scores\"][\"del_rate\"]\n","    if do_recognition\n","    else -1,\n","    dev_best_recognition_result[\"valid_scores\"][\"wer_scores\"][\"ins_rate\"]\n","    if do_recognition\n","    else -1,\n","    dev_best_recognition_result[\"valid_scores\"][\"wer_scores\"][\"sub_rate\"]\n","    if do_recognition\n","    else -1,\n","    dev_best_translation_result[\"valid_scores\"][\"bleu\"] if do_translation else -1,\n","    dev_best_translation_result[\"valid_scores\"][\"chrf\"] if do_translation else -1,\n","    dev_best_translation_result[\"valid_scores\"][\"rouge\"] if do_translation else -1,\n",")\n","logger.info(\"*\" * 60)\n","\n","\n","# write to file\n","output_path = os.path.join(MODEL_DIR, f\"inference.{version}\")\n","\n","if not os.path.isdir(output_path):\n","  os.mkdir(output_path)\n","\n","with open(os.path.join(output_path, \"all_results.pkl\"), \"wb\") as out:\n","    pickle.dump(\n","        {\n","            \"recognition_results\": dev_recognition_results\n","            if do_recognition\n","            else None,\n","            \"translation_results\": dev_translation_results\n","            if do_translation\n","            else None,\n","            \"best_beam_size\": dev_best_translation_beam_size,\n","            \"translation_alpha\": dev_best_translation_alpha\n","        },\n","        out,\n","    )\n"],"execution_count":17,"outputs":[{"output_type":"stream","text":["2020-12-01 04:54:59,155 - __main__ - INFO - cfg.name                           : sign_experiment\n","2020-12-01 04:54:59,159 - __main__ - INFO - cfg.data.data_path                 : ../data/\n","2020-12-01 04:54:59,161 - __main__ - INFO - cfg.data.version                   : phoenix_2014_trans\n","2020-12-01 04:54:59,163 - __main__ - INFO - cfg.data.sgn                       : sign\n","2020-12-01 04:54:59,166 - __main__ - INFO - cfg.data.txt                       : text\n","2020-12-01 04:54:59,171 - __main__ - INFO - cfg.data.gls                       : gloss\n","2020-12-01 04:54:59,172 - __main__ - INFO - cfg.data.train                     : PHOENIX2014T/phoenix14t.pami0.train\n","2020-12-01 04:54:59,174 - __main__ - INFO - cfg.data.dev                       : PHOENIX2014T/phoenix14t.pami0.dev\n","2020-12-01 04:54:59,176 - __main__ - INFO - cfg.data.test                      : PHOENIX2014T/phoenix14t.pami0.test\n","2020-12-01 04:54:59,177 - __main__ - INFO - cfg.data.feature_size              : 240\n","2020-12-01 04:54:59,179 - __main__ - INFO - cfg.data.level                     : word\n","2020-12-01 04:54:59,182 - __main__ - INFO - cfg.data.txt_lowercase             : True\n","2020-12-01 04:54:59,184 - __main__ - INFO - cfg.data.max_sent_length           : 176\n","2020-12-01 04:54:59,185 - __main__ - INFO - cfg.data.random_train_subset       : -1\n","2020-12-01 04:54:59,186 - __main__ - INFO - cfg.data.random_dev_subset         : -1\n","2020-12-01 04:54:59,188 - __main__ - INFO - cfg.data.data_file                 : v3.predictions.pkl\n","2020-12-01 04:54:59,190 - __main__ - INFO - cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","2020-12-01 04:54:59,194 - __main__ - INFO - cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","2020-12-01 04:54:59,197 - __main__ - INFO - cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]\n","2020-12-01 04:54:59,197 - __main__ - INFO - cfg.training.reset_best_ckpt       : False\n","2020-12-01 04:54:59,199 - __main__ - INFO - cfg.training.reset_scheduler       : False\n","2020-12-01 04:54:59,201 - __main__ - INFO - cfg.training.reset_optimizer       : False\n","2020-12-01 04:54:59,203 - __main__ - INFO - cfg.training.random_seed           : 42\n","2020-12-01 04:54:59,204 - __main__ - INFO - cfg.training.model_dir             : ./sign_sample_model\n","2020-12-01 04:54:59,205 - __main__ - INFO - cfg.training.recognition_loss_weight : 0.0\n","2020-12-01 04:54:59,206 - __main__ - INFO - cfg.training.translation_loss_weight : 1.0\n","2020-12-01 04:54:59,208 - __main__ - INFO - cfg.training.eval_metric           : bleu\n","2020-12-01 04:54:59,210 - __main__ - INFO - cfg.training.optimizer             : adam\n","2020-12-01 04:54:59,211 - __main__ - INFO - cfg.training.learning_rate         : 0.001\n","2020-12-01 04:54:59,213 - __main__ - INFO - cfg.training.batch_size            : 32\n","2020-12-01 04:54:59,214 - __main__ - INFO - cfg.training.num_valid_log         : 5\n","2020-12-01 04:54:59,216 - __main__ - INFO - cfg.training.epochs                : 5000000\n","2020-12-01 04:54:59,218 - __main__ - INFO - cfg.training.early_stopping_metric : eval_metric\n","2020-12-01 04:54:59,220 - __main__ - INFO - cfg.training.batch_type            : sentence\n","2020-12-01 04:54:59,222 - __main__ - INFO - cfg.training.translation_normalization : batch\n","2020-12-01 04:54:59,223 - __main__ - INFO - cfg.training.eval_recognition_beam_size : 1\n","2020-12-01 04:54:59,224 - __main__ - INFO - cfg.training.eval_translation_beam_size : 1\n","2020-12-01 04:54:59,225 - __main__ - INFO - cfg.training.eval_translation_beam_alpha : -1\n","2020-12-01 04:54:59,226 - __main__ - INFO - cfg.training.overwrite             : True\n","2020-12-01 04:54:59,227 - __main__ - INFO - cfg.training.shuffle               : True\n","2020-12-01 04:54:59,228 - __main__ - INFO - cfg.training.use_cuda              : True\n","2020-12-01 04:54:59,229 - __main__ - INFO - cfg.training.translation_max_output_length : 1\n","2020-12-01 04:54:59,230 - __main__ - INFO - cfg.training.keep_last_ckpts       : 1\n","2020-12-01 04:54:59,231 - __main__ - INFO - cfg.training.batch_multiplier      : 1\n","2020-12-01 04:54:59,232 - __main__ - INFO - cfg.training.logging_freq          : 1\n","2020-12-01 04:54:59,232 - __main__ - INFO - cfg.training.validation_freq       : 1\n","2020-12-01 04:54:59,233 - __main__ - INFO - cfg.training.betas                 : [0.9, 0.998]\n","2020-12-01 04:54:59,236 - __main__ - INFO - cfg.training.scheduling            : plateau\n","2020-12-01 04:54:59,243 - __main__ - INFO - cfg.training.learning_rate_min     : 1e-07\n","2020-12-01 04:54:59,244 - __main__ - INFO - cfg.training.weight_decay          : 0.001\n","2020-12-01 04:54:59,246 - __main__ - INFO - cfg.training.patience              : 8\n","2020-12-01 04:54:59,247 - __main__ - INFO - cfg.training.decrease_factor       : 0.7\n","2020-12-01 04:54:59,248 - __main__ - INFO - cfg.training.label_smoothing       : 0.0\n","2020-12-01 04:54:59,250 - __main__ - INFO - cfg.model.initializer              : xavier\n","2020-12-01 04:54:59,251 - __main__ - INFO - cfg.model.bias_initializer         : zeros\n","2020-12-01 04:54:59,253 - __main__ - INFO - cfg.model.init_gain                : 1.0\n","2020-12-01 04:54:59,254 - __main__ - INFO - cfg.model.embed_initializer        : xavier\n","2020-12-01 04:54:59,256 - __main__ - INFO - cfg.model.embed_init_gain          : 1.0\n","2020-12-01 04:54:59,258 - __main__ - INFO - cfg.model.tied_softmax             : False\n","2020-12-01 04:54:59,259 - __main__ - INFO - cfg.model.encoder.type             : transformer\n","2020-12-01 04:54:59,260 - __main__ - INFO - cfg.model.encoder.num_layers       : 2\n","2020-12-01 04:54:59,261 - __main__ - INFO - cfg.model.encoder.num_heads        : 4\n","2020-12-01 04:54:59,264 - __main__ - INFO - cfg.model.encoder.embeddings.embedding_dim : 512\n","2020-12-01 04:54:59,265 - __main__ - INFO - cfg.model.encoder.embeddings.scale : False\n","2020-12-01 04:54:59,265 - __main__ - INFO - cfg.model.encoder.embeddings.dropout : 0.1\n","2020-12-01 04:54:59,266 - __main__ - INFO - cfg.model.encoder.embeddings.norm_type : batch\n","2020-12-01 04:54:59,267 - __main__ - INFO - cfg.model.encoder.embeddings.activation_type : softsign\n","2020-12-01 04:54:59,268 - __main__ - INFO - cfg.model.encoder.hidden_size      : 512\n","2020-12-01 04:54:59,269 - __main__ - INFO - cfg.model.encoder.ff_size          : 2048\n","2020-12-01 04:54:59,269 - __main__ - INFO - cfg.model.encoder.dropout          : 0.1\n","2020-12-01 04:54:59,270 - __main__ - INFO - cfg.model.decoder.type             : transformer\n","2020-12-01 04:54:59,271 - __main__ - INFO - cfg.model.decoder.num_layers       : 1\n","2020-12-01 04:54:59,271 - __main__ - INFO - cfg.model.decoder.num_heads        : 4\n","2020-12-01 04:54:59,272 - __main__ - INFO - cfg.model.decoder.embeddings.embedding_dim : 512\n","2020-12-01 04:54:59,273 - __main__ - INFO - cfg.model.decoder.embeddings.scale : False\n","2020-12-01 04:54:59,274 - __main__ - INFO - cfg.model.decoder.embeddings.dropout : 0.1\n","2020-12-01 04:54:59,274 - __main__ - INFO - cfg.model.decoder.embeddings.norm_type : batch\n","2020-12-01 04:54:59,275 - __main__ - INFO - cfg.model.decoder.embeddings.activation_type : softsign\n","2020-12-01 04:54:59,276 - __main__ - INFO - cfg.model.decoder.hidden_size      : 512\n","2020-12-01 04:54:59,277 - __main__ - INFO - cfg.model.decoder.ff_size          : 2048\n","2020-12-01 04:54:59,278 - __main__ - INFO - cfg.model.decoder.dropout          : 0.1\n"],"name":"stderr"},{"output_type":"stream","text":["removing <StreamHandler stderr (NOTSET)>\n","removing <FileHandler /content/drive/My Drive/Colab Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/backtranslation/model/inference.v3.tf.log (NOTSET)>\n","dict_keys(['mean_dtw', 'dtw', 'words', 'ground_truth', 'predictions'])\n","dict_keys(['skeleton', 'gloss', 'embedding', 'video'])\n"],"name":"stdout"},{"output_type":"stream","text":["2020-12-01 04:55:16,163 - __main__ - INFO - ============================================================\n","2020-12-01 04:55:21,616 - __main__ - INFO - [DEV] partition [Translation] results:\n","\tNew Best Translation Beam Size: 1 and Alpha: -1\n","\tBLEU 19.44\tCHRF 24.78\tROUGE 19.44\n","2020-12-01 04:55:21,618 - __main__ - INFO - ------------------------------------------------------------\n","2020-12-01 04:55:21,621 - __main__ - INFO - ************************************************************\n","2020-12-01 04:55:21,623 - __main__ - INFO - [ALL] partition [Recognition & Translation] results:\n","\tBest CTC Decode Beam Size: -1\n","\tBest Translation Beam Size: 1 and Alpha: -1\n","\tWER -1.00\t(DEL: -1.00,\tINS: -1.00,\tSUB: -1.00)\n","\tBLEU 19.44\tCHRF 24.78\tROUGE 19.44\n","2020-12-01 04:55:21,625 - __main__ - INFO - ************************************************************\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3qL7wXMU67n-","executionInfo":{"status":"ok","timestamp":1606795454360,"user_tz":480,"elapsed":695,"user":{"displayName":"Amy Lee","photoUrl":"","userId":"14323350772356815607"}},"outputId":"ab0419b0-d745-40d9-cacc-8063910ac314"},"source":["dev_translation_results[1][-1][\"valid_scores\"]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'bleu': 0.07936507936507935,\n"," 'bleu_scores': {'bleu1': 0.07936507936507935},\n"," 'chrf': 5.108356544554804,\n"," 'rouge': 0.07936507936507936}"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uTFMvgKU9fOv","executionInfo":{"status":"ok","timestamp":1606794241449,"user_tz":480,"elapsed":603,"user":{"displayName":"Amy Lee","photoUrl":"","userId":"14323350772356815607"}},"outputId":"a459e860-7a5b-4e50-99eb-54086b46c23e"},"source":["indices = []\n","for i, w in enumerate(dev_translation_results[1][-1][\"txt_ref\"]):\n","  if w == \"microwave\":\n","    print(dev_translation_results[1][-1][\"txt_hyp\"][i])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["microwave\n","microwave\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LQKU6QFk-_fN","executionInfo":{"status":"ok","timestamp":1606794243149,"user_tz":480,"elapsed":858,"user":{"displayName":"Amy Lee","photoUrl":"","userId":"14323350772356815607"}},"outputId":"7b31c137-5b11-4de8-fc41-df66d001acaf"},"source":["dev_translation_results[1][-1][\"txt_ref\"][0:20]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['degree',\n"," 'price',\n"," 'spider',\n"," 'concern',\n"," 'carry',\n"," 'bed',\n"," 'call',\n"," 'microwave',\n"," 'dancer',\n"," 'door',\n"," 'lion',\n"," 'sandwich',\n"," 'much',\n"," 'stretch',\n"," 'computer',\n"," 'disagree',\n"," 'admit',\n"," 'sentence',\n"," 'secret',\n"," 'can']"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uPvrfXQg-gfl","executionInfo":{"status":"ok","timestamp":1606794245564,"user_tz":480,"elapsed":558,"user":{"displayName":"Amy Lee","photoUrl":"","userId":"14323350772356815607"}},"outputId":"fdd37e4c-4c9e-4279-bce0-ced6ce671da2"},"source":["dev_translation_results[1][-1][\"txt_hyp\"][0:20]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['children',\n"," 'group',\n"," 'spider',\n"," 'heart',\n"," 'heavy',\n"," 'traffic',\n"," 'why',\n"," 'microwave',\n"," 'screwdriver',\n"," 'door',\n"," 'soon',\n"," 'list',\n"," 'large',\n"," 'mechanic',\n"," 'vocabulary',\n"," 'corn',\n"," 'march',\n"," 'value',\n"," 'strawberry',\n"," 'test']"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"_SIeZOLYrUyl","executionInfo":{"status":"ok","timestamp":1606798891151,"user_tz":480,"elapsed":746,"user":{"displayName":"Amy Lee","photoUrl":"","userId":"14323350772356815607"}},"outputId":"efcd94bf-a2ab-4515-9c2f-dda473606175"},"source":["\n","MODEL_DIR"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/Colab Notebooks/openpose/DLF2020/dlf2020/src/AmyExperiments/backtranslation/model'"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"8qkguSXK-nTe","executionInfo":{"status":"ok","timestamp":1606799455518,"user_tz":480,"elapsed":809,"user":{"displayName":"Amy Lee","photoUrl":"","userId":"14323350772356815607"}}},"source":["file_location = os.path.join(MODEL_DIR, 'inference.v10.tf.base')\n","with open(os.path.join(file_location, 'v10.all_results.pkl'), 'rb') as f:\n","  inference = pickle.load(f)"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"beMTYVgQr_u6","executionInfo":{"status":"ok","timestamp":1606799457846,"user_tz":480,"elapsed":630,"user":{"displayName":"Amy Lee","photoUrl":"","userId":"14323350772356815607"}}},"source":["result = {\n","    \"bleu\": inference[\"translation_results\"][1][-1][\"valid_scores\"][\"bleu\"],\n","    \"txt_hyp\": inference[\"translation_results\"][1][-1][\"txt_hyp\"],\n","    \"txt_ref\": inference[\"translation_results\"][1][-1][\"txt_ref\"],\n","}\n","with open(os.path.join(file_location, 'v10.results.pkl'), 'wb') as f:\n","  pickle.dump(result, f)"],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z_wU0vfMrvVk","executionInfo":{"status":"ok","timestamp":1606799368934,"user_tz":480,"elapsed":488,"user":{"displayName":"Amy Lee","photoUrl":"","userId":"14323350772356815607"}}},"source":[""],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zH5IhG-Dr2HU","executionInfo":{"status":"ok","timestamp":1606799152869,"user_tz":480,"elapsed":575,"user":{"displayName":"Amy Lee","photoUrl":"","userId":"14323350772356815607"}},"outputId":"0dcaa6a8-43a5-47de-de9f-8cb0f74493fc"},"source":["result[\"txt_hyp\"][0:10]"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['computer',\n"," 'tranquil',\n"," 'spider',\n"," 'depressed',\n"," 'heavy',\n"," 'book',\n"," 'hearing',\n"," 'much',\n"," 'butter',\n"," 'door']"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"CHyzGgdvsVNs"},"source":[""],"execution_count":null,"outputs":[]}]}