{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Collection Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmyyyJ4gmjGW",
        "outputId": "30816f98-7157-4fca-d94e-9689496907e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8k7_DUWsj3-"
      },
      "source": [
        "Before running the following cell create a folder in Shared Drives named DLF2020 and add the DLF2020 (shared by Dhvani) folder's shortcut to it.\n",
        "Also change runtime to GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ochszNoyrATl",
        "outputId": "415eede4-ceeb-4430-fb2f-1cc384dfaf95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd '/content/drive/Shared drives/DLF2020/DLF2020/Data Collection Module'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1XCqSJ4wOA_0YGBeiVFwWnOq2NUCpBEMO/DLF2020/Data Collection Module\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76B2XgFM9sDx"
      },
      "source": [
        "The following code downloads and builds open pose for current session you need to run it only first time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL9QNzHI8y7l",
        "outputId": "b35afe2d-5a2d-4940-9f3f-9cb66f2188a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from os.path import exists, join, basename, splitext\n",
        "\n",
        "git_repo_url = 'https://github.com/CMU-Perceptual-Computing-Lab/openpose.git'\n",
        "# git_repo_url = 'https://github.com/dinatih/openpose.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "\n",
        "if 1 or not exists(project_name):\n",
        "  !rm -rf openpose\n",
        "  # see: https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/949\n",
        "\n",
        "  print(\"install new CMake becaue of CUDA10\")\n",
        "  if not exists('cmake-3.13.0-Linux-x86_64.tar.gz'):\n",
        "    !wget -q https://cmake.org/files/v3.13/cmake-3.13.0-Linux-x86_64.tar.gz\n",
        "  !tar xfz cmake-3.13.0-Linux-x86_64.tar.gz --strip-components=1 -C /usr/local\n",
        "\n",
        "  print(\"clone openpose\")\n",
        "  !git clone -q --depth 1 $git_repo_url\n",
        "  print(\"CMakelist.txt's caffe fix\")\n",
        "  !sed -i 's/execute_process(COMMAND git checkout master WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\\/3rdparty\\/caffe)/execute_process(COMMAND git checkout f019d0dfe86f49d1140961f8c7dec22130c83154 WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\\/3rdparty\\/caffe)/g' openpose/CMakeLists.txt\n",
        "  print(\"install system dependencies\")\n",
        "  !apt-get -qq install -y libatlas-base-dev libprotobuf-dev libleveldb-dev libsnappy-dev libhdf5-serial-dev protobuf-compiler libgflags-dev libgoogle-glog-dev liblmdb-dev opencl-headers ocl-icd-opencl-dev libviennacl-dev\n",
        "  print(\"build openpose\")\n",
        "  !cd openpose && rm -rf build || true && mkdir build && cd build && cmake .. && make -j`nproc`\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "install new CMake becaue of CUDA10\n",
            "clone openpose\n",
            "CMakelist.txt's caffe fix\n",
            "install system dependencies\n",
            "build openpose\n",
            "-- The C compiler identification is GNU 7.5.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- GCC detected, adding compile flags\n",
            "-- GCC detected, adding compile flags\n",
            "-- Looking for pthread.h\n",
            "-- Looking for pthread.h - found\n",
            "-- Looking for pthread_create\n",
            "-- Looking for pthread_create - not found\n",
            "-- Looking for pthread_create in pthreads\n",
            "-- Looking for pthread_create in pthreads - not found\n",
            "-- Looking for pthread_create in pthread\n",
            "-- Looking for pthread_create in pthread - found\n",
            "-- Found Threads: TRUE  \n",
            "-- Found CUDA: /usr/local/cuda (found version \"10.1\") \n",
            "-- Building with CUDA.\n",
            "-- CUDA detected: 10.1\n",
            "-- Found cuDNN: ver. 7.6.5 found (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libcudnn.so)\n",
            "-- Added CUDA NVCC flags for: sm_61\n",
            "-- Found cuDNN: ver. 7.6.5 found (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libcudnn.so)\n",
            "-- Found GFlags: /usr/include  \n",
            "-- Found gflags  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libgflags.so)\n",
            "-- Found Glog: /usr/include  \n",
            "-- Found glog    (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libglog.so)\n",
            "-- Found Protobuf: /usr/lib/x86_64-linux-gnu/libprotobuf.so;-lpthread (found version \"3.0.0\") \n",
            "-- Found OpenCV: /usr (found version \"3.2.0\") \n",
            "-- Caffe will be downloaded from source now. NOTE: This process might take several minutes depending\n",
            "        on your internet connection.\n",
            "Submodule '3rdparty/caffe' (https://github.com/CMU-Perceptual-Computing-Lab/caffe.git) registered for path '../3rdparty/caffe'\n",
            "Cloning into '/content/drive/.shortcut-targets-by-id/1XCqSJ4wOA_0YGBeiVFwWnOq2NUCpBEMO/DLF2020/Data Collection Module/openpose/3rdparty/caffe'...\n",
            "Submodule path '../3rdparty/caffe': checked out 'b5ede488952e40861e84e51a9f9fd8fe2395cc8a'\n",
            "Previous HEAD position was b5ede488 Added TX2 JetPack3.3 support\n",
            "HEAD is now at c95002fb Added support for newer GPUs\n",
            "M\tdata/cifar10/get_cifar10.sh\n",
            "M\tdata/ilsvrc12/get_ilsvrc_aux.sh\n",
            "M\tdata/mnist/get_mnist.sh\n",
            "M\texamples/cifar10/create_cifar10.sh\n",
            "M\texamples/cifar10/train_full.sh\n",
            "M\texamples/cifar10/train_full_sigmoid.sh\n",
            "M\texamples/cifar10/train_full_sigmoid_bn.sh\n",
            "M\texamples/cifar10/train_quick.sh\n",
            "M\texamples/finetune_flickr_style/assemble_data.py\n",
            "M\texamples/imagenet/create_imagenet.sh\n",
            "M\texamples/imagenet/make_imagenet_mean.sh\n",
            "M\texamples/imagenet/resume_training.sh\n",
            "M\texamples/imagenet/train_caffenet.sh\n",
            "M\texamples/mnist/create_mnist.sh\n",
            "M\texamples/mnist/train_lenet.sh\n",
            "M\texamples/mnist/train_lenet_adam.sh\n",
            "M\texamples/mnist/train_lenet_consolidated.sh\n",
            "M\texamples/mnist/train_lenet_docker.sh\n",
            "M\texamples/mnist/train_lenet_rmsprop.sh\n",
            "M\texamples/mnist/train_mnist_autoencoder.sh\n",
            "M\texamples/mnist/train_mnist_autoencoder_adadelta.sh\n",
            "M\texamples/mnist/train_mnist_autoencoder_adagrad.sh\n",
            "M\texamples/mnist/train_mnist_autoencoder_nesterov.sh\n",
            "M\texamples/siamese/create_mnist_siamese.sh\n",
            "M\texamples/siamese/train_mnist_siamese.sh\n",
            "M\tinstall_caffe_JetsonTX2_JetPack3.1.sh\n",
            "M\tinstall_caffe_if_cuda8.sh\n",
            "M\tmodels/bvlc_googlenet/train_val.prototxt\n",
            "M\tpython/classify.py\n",
            "M\tpython/detect.py\n",
            "M\tpython/draw_net.py\n",
            "M\tscripts/build_docs.sh\n",
            "M\tscripts/copy_notebook.py\n",
            "M\tscripts/cpp_lint.py\n",
            "M\tscripts/deploy_docs.sh\n",
            "M\tscripts/download_model_binary.py\n",
            "M\tscripts/download_model_from_gist.sh\n",
            "M\tscripts/gather_examples.sh\n",
            "M\tscripts/split_caffe_proto.py\n",
            "M\tscripts/travis/build.sh\n",
            "M\tscripts/travis/configure.sh\n",
            "M\tscripts/travis/defaults.sh\n",
            "M\tscripts/travis/install-deps.sh\n",
            "M\tscripts/travis/install-python-deps.sh\n",
            "M\tscripts/travis/setup-venv.sh\n",
            "M\tscripts/travis/test.sh\n",
            "M\tscripts/upload_model_to_gist.sh\n",
            "M\ttools/extra/extract_seconds.py\n",
            "M\ttools/extra/launch_resize_and_crop_images.sh\n",
            "M\ttools/extra/parse_log.py\n",
            "M\ttools/extra/parse_log.sh\n",
            "M\ttools/extra/plot_training_log.py.example\n",
            "M\ttools/extra/resize_and_crop_images.py\n",
            "M\ttools/extra/summarize.py\n",
            "-- Caffe will be built from source now.\n",
            "-- Download the models.\n",
            "-- Downloading BODY_25 model...\n",
            "-- NOTE: This process might take several minutes depending on your internet connection.\n",
            "-- Not downloading body (COCO) model\n",
            "-- Not downloading body (MPI) model\n",
            "-- Downloading face model...\n",
            "-- NOTE: This process might take several minutes depending on your internet connection.\n",
            "-- Downloading hand model...\n",
            "-- NOTE: This process might take several minutes depending on your internet connection.\n",
            "-- Models Downloaded.\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/drive/.shortcut-targets-by-id/1XCqSJ4wOA_0YGBeiVFwWnOq2NUCpBEMO/DLF2020/Data Collection Module/openpose/build\n",
            "\u001b[35m\u001b[1mScanning dependencies of target openpose_lib\u001b[0m\n",
            "[-12%] \u001b[34m\u001b[1mCreating directories for 'openpose_lib'\u001b[0m\n",
            "[  0%] \u001b[34m\u001b[1mNo download step for 'openpose_lib'\u001b[0m\n",
            "[ 12%] \u001b[34m\u001b[1mNo patch step for 'openpose_lib'\u001b[0m\n",
            "[ 25%] \u001b[34m\u001b[1mNo update step for 'openpose_lib'\u001b[0m\n",
            "[ 37%] \u001b[34m\u001b[1mPerforming configure step for 'openpose_lib'\u001b[0m\n",
            "-- The C compiler identification is GNU 7.5.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Looking for pthread.h\n",
            "-- Looking for pthread.h - found\n",
            "-- Looking for pthread_create\n",
            "-- Looking for pthread_create - not found\n",
            "-- Looking for pthread_create in pthreads\n",
            "-- Looking for pthread_create in pthreads - not found\n",
            "-- Looking for pthread_create in pthread\n",
            "-- Looking for pthread_create in pthread - found\n",
            "-- Found Threads: TRUE  \n",
            "-- Boost version: 1.65.1\n",
            "-- Found the following Boost libraries:\n",
            "--   system\n",
            "--   thread\n",
            "--   filesystem\n",
            "--   chrono\n",
            "--   date_time\n",
            "--   atomic\n",
            "-- Found GFlags: /usr/include  \n",
            "-- Found gflags  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libgflags.so)\n",
            "-- Found Glog: /usr/include  \n",
            "-- Found glog    (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libglog.so)\n",
            "-- Found Protobuf: /usr/lib/x86_64-linux-gnu/libprotobuf.so;-lpthread (found version \"3.0.0\") \n",
            "-- Found PROTOBUF Compiler: /usr/bin/protoc\n",
            "-- HDF5: Using hdf5 compiler wrapper to determine C configuration\n",
            "-- HDF5: Using hdf5 compiler wrapper to determine CXX configuration\n",
            "-- Found HDF5: /usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5_cpp.so;/usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5.so;/usr/lib/x86_64-linux-gnu/libpthread.so;/usr/lib/x86_64-linux-gnu/libsz.so;/usr/lib/x86_64-linux-gnu/libz.so;/usr/lib/x86_64-linux-gnu/libdl.so;/usr/lib/x86_64-linux-gnu/libm.so (found version \"1.10.0.1\") found components:  HL \n",
            "-- CUDA detected: 10.1\n",
            "-- Found cuDNN: ver. 7.6.5 found (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libcudnn.so)\n",
            "-- Added CUDA NVCC flags for: sm_61\n",
            "-- Found Atlas: /usr/include/x86_64-linux-gnu  \n",
            "-- Found Atlas (include: /usr/include/x86_64-linux-gnu library: /usr/lib/x86_64-linux-gnu/libatlas.so lapack: /usr/lib/x86_64-linux-gnu/liblapack.so\n",
            "-- Python interface is disabled or not all required dependencies found. Building without it...\n",
            "-- Found Git: /usr/bin/git (found version \"2.17.1\") \n",
            "-- \n",
            "-- ******************* Caffe Configuration Summary *******************\n",
            "-- General:\n",
            "--   Version           :   1.0.0\n",
            "--   Git               :   1.0-147-gc95002fb-dirty\n",
            "--   System            :   Linux\n",
            "--   C++ compiler      :   /usr/bin/c++\n",
            "--   Release CXX flags :   -O3 -DNDEBUG -fPIC -Wall -std=c++11 -Wno-sign-compare -Wno-uninitialized\n",
            "--   Debug CXX flags   :   -g -fPIC -Wall -std=c++11 -Wno-sign-compare -Wno-uninitialized\n",
            "--   Build type        :   Release\n",
            "-- \n",
            "--   BUILD_SHARED_LIBS :   ON\n",
            "--   BUILD_python      :   OFF\n",
            "--   BUILD_matlab      :   OFF\n",
            "--   BUILD_docs        :   OFF\n",
            "--   CPU_ONLY          :   OFF\n",
            "--   USE_OPENCV        :   OFF\n",
            "--   USE_LEVELDB       :   OFF\n",
            "--   USE_LMDB          :   OFF\n",
            "--   USE_NCCL          :   OFF\n",
            "--   ALLOW_LMDB_NOLOCK :   OFF\n",
            "--   USE_HDF5          :   ON\n",
            "-- \n",
            "-- Dependencies:\n",
            "--   BLAS              :   Yes (Atlas)\n",
            "--   Boost             :   Yes (ver. 1.65)\n",
            "--   glog              :   Yes\n",
            "--   gflags            :   Yes\n",
            "--   protobuf          :   Yes (ver. 3.0.0)\n",
            "--   CUDA              :   Yes (ver. 10.1)\n",
            "-- \n",
            "-- NVIDIA CUDA:\n",
            "--   Target GPU(s)     :   Auto\n",
            "--   GPU arch(s)       :   sm_61\n",
            "--   cuDNN             :   Yes (ver. 7.6.5)\n",
            "-- \n",
            "-- Install:\n",
            "--   Install path      :   /content/drive/.shortcut-targets-by-id/1XCqSJ4wOA_0YGBeiVFwWnOq2NUCpBEMO/DLF2020/Data Collection Module/openpose/build/caffe\n",
            "-- \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "CMake Warning:\n",
            "  Manually-specified variables were not used by the project:\n",
            "\n",
            "    CUDA_ARCH_BIN\n",
            "\n",
            "\n",
            "-- Build files have been written to: /content/drive/.shortcut-targets-by-id/1XCqSJ4wOA_0YGBeiVFwWnOq2NUCpBEMO/DLF2020/Data Collection Module/openpose/build/caffe/src/openpose_lib-build\n",
            "[ 50%] \u001b[34m\u001b[1mPerforming build step for 'openpose_lib'\u001b[0m\n",
            "[ -1%] \u001b[34m\u001b[1mRunning C++/Python protocol buffer compiler on /content/drive/.shortcut-targets-by-id/1XCqSJ4wOA_0YGBeiVFwWnOq2NUCpBEMO/DLF2020/Data Collection Module/openpose/3rdparty/caffe/src/caffe/proto/caffe.proto\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target caffeproto\u001b[0m\n",
            "[ -1%] \u001b[32mBuilding CXX object src/caffe/CMakeFiles/caffeproto.dir/__/__/include/caffe/proto/caffe.pb.cc.o\u001b[0m\n",
            "[ -1%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libcaffeproto.a\u001b[0m\n",
            "[ -1%] Built target caffeproto\n",
            "[ -1%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/util/cuda_compile_1_generated_math_functions.cu.o\u001b[0m\n",
            "[ -1%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_absval_layer.cu.o\u001b[0m\n",
            "In file included from /content/drive/.shortcut-targets-by-id/1XCqSJ4wOA_0YGBeiVFwWnOq2NUCpBEMO/DLF2020/Data Collection Module/openpose/3rdparty/caffe/src/caffe/util/math_functions.cu:1:0:\n",
            "/usr/local/cuda/include/math_functions.h:54:2: warning: #warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\" [-Wcpp]\n",
            " #warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\"\n",
            "  ^~~~~~~\n",
            "In file included from /content/drive/.shortcut-targets-by-id/1XCqSJ4wOA_0YGBeiVFwWnOq2NUCpBEMO/DLF2020/Data Collection Module/openpose/3rdparty/caffe/src/caffe/util/math_functions.cu:1:0:\n",
            "/usr/local/cuda/include/math_functions.h:54:2: warning: #warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\" [-Wcpp]\n",
            " #warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\"\n",
            "  ^~~~~~~\n",
            "In file included from /content/drive/.shortcut-targets-by-id/1XCqSJ4wOA_0YGBeiVFwWnOq2NUCpBEMO/DLF2020/Data Collection Module/openpose/3rdparty/caffe/src/caffe/util/math_functions.cu:1:0:\n",
            "/usr/local/cuda/include/math_functions.h:54:2: warning: #warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\" [-Wcpp]\n",
            " #warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\"\n",
            "  ^~~~~~~\n",
            "In file included from /content/drive/.shortcut-targets-by-id/1XCqSJ4wOA_0YGBeiVFwWnOq2NUCpBEMO/DLF2020/Data Collection Module/openpose/3rdparty/caffe/src/caffe/util/math_functions.cu:1:0:\n",
            "/usr/local/cuda/include/math_functions.h:54:2: warning: #warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\" [-Wcpp]\n",
            " #warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\"\n",
            "  ^~~~~~~\n",
            "[ -1%] \u001b[34m\u001b[1mBuilding NVCC (Device) object src/caffe/CMakeFiles/cuda_compile_1.dir/layers/cuda_compile_1_generated_accuracy_layer.cu.o\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YwTnny_mmOD"
      },
      "source": [
        "The following functions download videos according to a json config file named WLASL_v3.0.json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3KnjnS4kcKL"
      },
      "source": [
        "import os, shutil\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "import urllib.request\n",
        "from multiprocessing.dummy import Pool\n",
        "import cv2\n",
        "\n",
        "import random\n",
        "\n",
        "!pip install youtube-dl\n",
        "\n",
        "# import logging\n",
        "# logging.basicConfig(filename='download_{}.log'.format(int(time.time())), filemode='w', level=logging.DEBUG)\n",
        "# logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
        "\n",
        "\n",
        "def request_video(url, referer=''):\n",
        "    user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
        "\n",
        "    headers = {'User-Agent': user_agent,\n",
        "               }\n",
        "    \n",
        "    if referer:\n",
        "        headers['Referer'] = referer\n",
        "\n",
        "    request = urllib.request.Request(url, None, headers)  # The assembled request\n",
        "\n",
        "    # logging.info('Requesting {}'.format(url))\n",
        "    response = urllib.request.urlopen(request)\n",
        "    data = response.read()  # The data you need\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def save_video(data, saveto):\n",
        "    with open(saveto, 'wb+') as f:\n",
        "        f.write(data)\n",
        "\n",
        "    # please be nice to the host - take pauses and avoid spamming\n",
        "    time.sleep(random.uniform(0.5, 1.5))\n",
        "\n",
        "\n",
        "def download_youtube(url, dirname, video_id):\n",
        "    raise NotImplementedError(\"Urllib cannot deal with YouTube links.\")\n",
        "\n",
        "\n",
        "def download_aslpro(url, dirname, video_id):\n",
        "    saveto = os.path.join(dirname, '{}.swf'.format(video_id))\n",
        "    if os.path.exists(saveto):\n",
        "        # logging.info('{} exists at {}'.format(video_id, saveto))\n",
        "        return \n",
        "\n",
        "    data = request_video(url, referer='http://www.aslpro.com/cgi-bin/aslpro/aslpro.cgi')\n",
        "    save_video(data, saveto)\n",
        "\n",
        "\n",
        "def download_others(url, dirname, video_id):\n",
        "    saveto = os.path.join(dirname, '{}.mp4'.format(video_id))\n",
        "    if os.path.exists(saveto):\n",
        "        # logging.info('{} exists at {}'.format(video_id, saveto))\n",
        "        return \n",
        "    \n",
        "    data = request_video(url)\n",
        "    save_video(data, saveto)\n",
        "\n",
        "\n",
        "def select_download_method(url):\n",
        "    if 'aslpro' in url:\n",
        "        return download_aslpro\n",
        "    elif 'youtube' in url or 'youtu.be' in url:\n",
        "        return download_youtube\n",
        "    else:\n",
        "        return download_others\n",
        "\n",
        "def video_to_frames(video_path, size=None):\n",
        "    \"\"\"\n",
        "    video_path -> str, path to video.\n",
        "    size -> (int, int), width, height.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            if size:\n",
        "                frame = cv2.resize(frame, size)\n",
        "            frames.append(frame)\n",
        "        else:\n",
        "            break\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "\n",
        "def convert_frames_to_video(frame_array, path_out, size, fps=25):\n",
        "    out = cv2.VideoWriter(path_out, cv2.VideoWriter_fourcc(*'mp4v'), fps, size)\n",
        "    for i in range(len(frame_array)):\n",
        "        # writing to a image array\n",
        "        out.write(frame_array[i])\n",
        "    out.release()\n",
        "\n",
        "\n",
        "def extract_frame_as_video(src_video_path, start_frame, end_frame):\n",
        "    frames = video_to_frames(src_video_path)\n",
        "    return frames[start_frame: end_frame+1]\n",
        "\n",
        "\n",
        "def extract_videos(inst, pre_preprocessed, post_preprocessed):\n",
        "  # we will call preprcess.py that came with WLASL\n",
        "  !sh swf2mp4_new.sh          \n",
        "  url = inst['url']\n",
        "  video_id = inst['video_id']\n",
        "  if 'youtube' in url or 'youtu.be' in url:\n",
        "      yt_identifier = url[-11:]\n",
        "      src_video_path = os.path.join(pre_preprocessed, yt_identifier + '.mp4')\n",
        "      dst_video_path = os.path.join(post_preprocessed, video_id + '.mp4')\n",
        "      if not os.path.exists(src_video_path):\n",
        "          return\n",
        "      if os.path.exists(dst_video_path):\n",
        "          return\n",
        "      # because the JSON file indexes from 1.\n",
        "      start_frame = inst['frame_start'] - 1\n",
        "      end_frame = inst['frame_end'] - 1\n",
        "      if end_frame <= 0:\n",
        "          shutil.copyfile(src_video_path, dst_video_path)\n",
        "          return\n",
        "      selected_frames = extract_frame_as_video(src_video_path, start_frame, end_frame)\n",
        "      # when OpenCV reads an image, it returns size in (h, w, c)\n",
        "      # when OpenCV creates a writer, it requres size in (w, h).\n",
        "      size = selected_frames[0].shape[:2][::-1]\n",
        "      convert_frames_to_video(selected_frames, dst_video_path, size)\n",
        "      \n",
        "  else:\n",
        "      src_video_path = os.path.join(pre_preprocessed, video_id + '.mp4')\n",
        "      dst_video_path = os.path.join(post_preprocessed, video_id + '.mp4')\n",
        "      if os.path.exists(dst_video_path):\n",
        "          return\n",
        "      if not os.path.exists(src_video_path):\n",
        "          return\n",
        "      shutil.copyfile(src_video_path, dst_video_path)\n",
        "\n",
        "def download_nonyt_videos(inst, pre_preprocessed, post_preprocessed, saveto='raw_videos'):\n",
        "  video_url = inst['url']\n",
        "  video_id = inst['video_id']\n",
        "  # logging.info('gloss: {}, video: {}.'.format(gloss, video_id))\n",
        "\n",
        "  download_method = select_download_method(video_url)    \n",
        "  \n",
        "  if download_method != download_youtube:\n",
        "    try:\n",
        "        download_method(video_url, saveto, video_id)\n",
        "        extract_videos(inst,pre_preprocessed,post_preprocessed)\n",
        "    except Exception as e:\n",
        "        # logging.error('Unsuccessful downloading - video {}'.format(video_id))\n",
        "        return\n",
        "\n",
        "def check_youtube_dl_version():\n",
        "    ver = os.popen('youtube-dl --version').read()\n",
        "\n",
        "    assert ver, \"youtube-dl cannot be found in PATH. Please verify your installation.\"\n",
        "    assert ver >= '2020.03.08', \"Please update youtube-dl to newest version.\"\n",
        "\n",
        "\n",
        "def download_yt_videos(inst, pre_preprocessed, post_preprocessed,saveto='raw_videos'):\n",
        "    video_url = inst['url']\n",
        "    video_id = inst['video_id']\n",
        "\n",
        "    if 'youtube' not in video_url and 'youtu.be' not in video_url:\n",
        "        return\n",
        "\n",
        "    if os.path.exists(os.path.join(saveto, video_url[-11:] + '.mp4')) or os.path.exists(os.path.join(saveto, video_url[-11:] + '.mkv')):\n",
        "        # logging.info('YouTube videos {} already exists.'.format(video_url))\n",
        "        return\n",
        "    else:\n",
        "        cmd = \"youtube-dl \\\"{}\\\" -o \\\"{}%(id)s.%(ext)s\\\"\"\n",
        "        cmd = cmd.format(video_url, saveto + os.path.sep)\n",
        "\n",
        "        rv = os.system(cmd)\n",
        "        extract_videos(inst,pre_preprocessed,post_preprocessed)\n",
        "        if not rv:\n",
        "            return\n",
        "            # logging.info('Finish downloading youtube video url {}'.format(video_url))\n",
        "        else:\n",
        "            return\n",
        "            # logging.error('Unsuccessful downloading - youtube video url {}'.format(video_url))\n",
        "\n",
        "        # please be nice to the host - take pauses and avoid spamming\n",
        "        time.sleep(random.uniform(1.0, 1.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmRrpaLoCmLM"
      },
      "source": [
        "For each word in the json file, the code first clears the directory where videos are stored, download corresponding videos and then calls openpose on all the videos in that directory "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXppJo6dtaQa"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import json\n",
        "import preprocess\n",
        "indexfile = 'WLASL_v0.3.json'\n",
        "save_to = 'raw_videos'\n",
        "pre_preprocessed = 'raw_videos_mp4'\n",
        "post_preprocessed = 'videos'\n",
        "character = 'a'\n",
        "# we keep a list of already processed words from the log file\n",
        "processed_words = []\n",
        "with open(\"log.txt\",'r') as log:\n",
        "  processed_words = set(log.read().split())\n",
        "\n",
        "# I am keeping a log file to keep track of the last word that was processed\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    content = json.load(open(indexfile))\n",
        "    for entry in content:\n",
        "        # first we will delete existing videos in a folder\n",
        "        if not os.path.exists(save_to):\n",
        "          os.mkdir(save_to)\n",
        "        else:  \n",
        "          try:\n",
        "            if os.path.isfile(save_to) or os.path.islink(save_to):\n",
        "                os.unlink(save_to)\n",
        "            elif os.path.isdir(save_to):\n",
        "                shutil.rmtree(save_to)\n",
        "          except Exception as e:\n",
        "            pass\n",
        "        if os.path.exists(post_preprocessed):\n",
        "          try:\n",
        "            if os.path.isfile(post_preprocessed) or os.path.islink(post_preprocessed):\n",
        "                os.unlink(post_preprocessed)\n",
        "            elif os.path.isdir(post_preprocessed):\n",
        "                shutil.rmtree(post_preprocessed)\n",
        "          except Exception as e:\n",
        "            pass\n",
        "        os.mkdir(post_preprocessed)\n",
        "        gloss = entry['gloss']\n",
        "        if gloss[0] == character and gloss not in processed_words:  \n",
        "          print(gloss)\n",
        "          with open('log.txt','a')as f:\n",
        "            f.write(gloss + '\\n')\n",
        "          instances = entry['instances']\n",
        "          for inst in instances:  \n",
        "            # code to download all kinds of videos\n",
        "            download_nonyt_videos(inst,pre_preprocessed,post_preprocessed,save_to)\n",
        "            check_youtube_dl_version()\n",
        "            download_yt_videos(inst,pre_preprocessed,post_preprocessed,save_to)\n",
        "            # code to go through the downloaded videos and call openpose\n",
        "            colab_video_path = os.path.join(post_preprocessed, inst['video_id'] + '.mp4')\n",
        "            colab_openpose_video_path = colab_video_path.replace('.mp4', '') + '-openpose.mp4'\n",
        "            if not os.path.exists(colab_openpose_video_path):\n",
        "              #todo see output file location for json and video\n",
        "              !cd openpose && ./build/examples/openpose/openpose.bin --hand --face --number_people_max 12 --video '../{colab_video_path}'  --display 0 --write_video '../{colab_openpose_video_path}' --write_json '../pipelineDemo/2DJSON_Keypoints/{inst[\"video_id\"]}' # --net_resolution \"-1x736\" --scale_number 4 --scale_gap 0.25\n",
        "              !cd ..\n",
        "              # after processing a single instances of a single word we will create a h5 for it\n",
        "              !python3 \"./pipelineDemo/pipeline_demo_01_openpose2h5.py\"\n",
        "              # finally we create 3d skeletal structures for all the frames in one video video and store it in a single txt file\n",
        "              video_id = inst['video_id']\n",
        "              !python3 \"./pipelineDemo/demo.py\" $video_id\n",
        "              data = {}\n",
        "              data[inst['video_id']] = inst\n",
        "              with open('Training_Data.json', 'a+') as outfile:\n",
        "                  json.dump(data, outfile)\n",
        "          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgiKD-telGx0"
      },
      "source": [
        "# code to merge 2D features of a video into a single json file and adding a counter to it and convert text files containing 3D keypoints to a single json file\n",
        "# all the collected data will be stored in the training data Folder in DLF2020\n",
        "# \n",
        "import os\n",
        "import json\n",
        "\n",
        "%cd '/content/drive/Shared drives/DLF2020/DLF2020'\n",
        "\n",
        "# # code to merge multiple 2d json files into one json file\n",
        "directory = r'./Data Collection Module/pipelineDemo/2DJSON_Keypoints'\n",
        "count_frames = 0\n",
        "lines_written = 0\n",
        "for video_name in os.listdir(directory):\n",
        "    dir = os.path.join(directory, video_name)\n",
        "    temp = {}\n",
        "    for frame_name in os.listdir(dir):\n",
        "      frame_number = frame_name.split('_')[1]\n",
        "      f = open(os.path.join(dir,frame_name))\n",
        "      try:\n",
        "        temp[int(frame_number)] = json.load(f)\n",
        "      except:\n",
        "        continue\n",
        "      count_frames += 1\n",
        "    outfile = open(\"./Training Data/2D Data/\"+video_name + '.json','w')\n",
        "    print(video_name,count_frames,len(temp.keys()))\n",
        "    json.dump(temp, outfile, sort_keys=True)\n",
        "    \n",
        "# code to convert 3d txt file to one json file with counter\n",
        "# I am assuming that the 2d to 3d conversion maintains the order of the frames and so while converting from 2d to 3d I am directly taking line number to frame number\n",
        "directory = r'./Data Collection Module/Training_Data_3D'\n",
        "lines_written = 0\n",
        "for video_name in os.listdir(directory):\n",
        "    count_frames = 0\n",
        "    file_name = os.path.join(directory, video_name)\n",
        "    temp = {}\n",
        "    f = open(file_name)\n",
        "    for line in f.readlines():\n",
        "      temp[count_frames] = line.replace('\\t',',').replace('\\n','')\n",
        "      count_frames += 1\n",
        "      video_name = video_name.replace('txt','json')\n",
        "    outfile = open(\"./Training Data/3D Data/\"+ video_name,'w')\n",
        "    json.dump(temp, outfile, sort_keys=True)\n",
        "    print(video_name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpgFE_TXJe9a"
      },
      "source": [
        "No need to run following code for each cell but won't harm to check few videos and their corresponding visualizations randomly just change file/folder names\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEicNBMiOVxy"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBoPwnb9OV23"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YYVUOPQOV8x"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRidhTRGOV7T"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsMpbru3OV1T"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir-EL5D_OVvz"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShtA5gjadHIP"
      },
      "source": [
        "The following code just creates a visualization of final 3d data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1lKnk7j9FDL"
      },
      "source": [
        "# todo need to clean json file so that it only contains uniqueu values after each word is processed\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import numpy as np\n",
        "import pipelineDemo.skeletalModel as skeletalModel\n",
        "file_name = \"01995\"\n",
        "f = open(\"Training_Data_3D/\"+file_name+\".txt\",'r')\n",
        "l = []\n",
        "for line in f:\n",
        "  l.append([float(x) for x in (line.split())])\n",
        "l = np.array(l)\n",
        "print(l.shape)\n",
        "for pose in l[0::2]:\n",
        "  fig = plt.figure()\n",
        "  ax  = fig.add_subplot(111, projection = '3d')\n",
        "  #ax  = fig.add_subplot(111)\n",
        "  def drawLine(i,j,pose,ax):\n",
        "    ax.plot([pose[i*3],pose[j*3]],[pose[i*3+2],pose[j*3+2]],[-pose[i*3+1],-pose[j*3+1]], color = 'b')\n",
        "    #ax.plot([pose[i*3],pose[j*3]],[-pose[i*3+1],-pose[j*3+1]], color = 'b')\n",
        "  # structure = skeletalModel.getSkeletalModelStructure()\n",
        "  lines4disp = ((0,1),(1,2),(2,3),(3,4),(1,5),(5,6),(6,7),(7,8),(8,9),(9,10),(10,11),(11,12),(8,13),(13,14),(14,15),(15,16),(8,17),(7,18),(18,19),(19,20),(8,21),(21,22),(22,23),(23,24),(8,25),(25,26),(26,27,(27,28),(4,29),(29,30),(30,31),(31,32),(32,33),29,34),(34,35),(35,36),(36,37),(29,38),(38,39),(39,40),(40,41),(29,42),(42,43),(43,44),(44,45),(29,46),(46,47),(47,48),(48,49))\n",
        "\n",
        "  for s in lines4disp:\n",
        "    drawLine(s[0],s[1],pose,ax)\n",
        "  #ax.scatter(pose[150::2],pose[151::2],[0.2]* 105)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcGUEAtGC4xN"
      },
      "source": [
        "The following code visualizes 2d data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yxWpbSLD0RG"
      },
      "source": [
        "# following code plots 2d keypoints to check that they are correct\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import numpy as np\n",
        "import pipelineDemo.pipelineDemo.skeletalModel as skeletalModel\n",
        "import json\n",
        "file_name = \"./pipelineDemo/pipelineDemo/2DJSON_Keypoints/65043/65043_000000000026_keypoints.json\"\n",
        "f = open(file_name,'r')\n",
        "data = json.load(f)\n",
        "theTallest = data[\"people\"][0]\n",
        "pointsP = []\n",
        "pointsLH = []\n",
        "pointsRH = []\n",
        "pointsFace = []\n",
        "for i in range(0,len(theTallest[\"pose_keypoints_2d\"]),3):\n",
        "  pointsP.append(theTallest[\"pose_keypoints_2d\"][i])\n",
        "  pointsP.append(theTallest[\"pose_keypoints_2d\"][i+1])\n",
        "\n",
        "for i in range(0,len(theTallest[\"hand_left_keypoints_2d\"]),3):\n",
        "  pointsLH.append(theTallest[\"hand_left_keypoints_2d\"][i])\n",
        "  pointsLH.append(theTallest[\"hand_left_keypoints_2d\"][i+1])\n",
        "\n",
        "\n",
        "for i in range(0,len(theTallest[\"hand_right_keypoints_2d\"]),3):\n",
        "  pointsRH.append(theTallest[\"hand_right_keypoints_2d\"][i])\n",
        "  pointsRH.append(theTallest[\"hand_right_keypoints_2d\"][i+1])\n",
        "\n",
        "for i in range(0,len(theTallest[\"face_keypoints_2d\"]),3):\n",
        "  pointsFace.append(theTallest[\"face_keypoints_2d\"][i])\n",
        "  pointsFace.append(theTallest[\"face_keypoints_2d\"][i+1])\n",
        "print(len(pointsP))\n",
        "print(len(pointsLH))\n",
        "print(len(pointsRH))\n",
        "print(len(pointsFace))\n",
        "pose = np.array(pointsP[:16:]+ pointsLH + pointsRH + pointsFace)\n",
        "print(pose.shape)\n",
        "fig = plt.figure(figsize=(8, 6),dpi = 90)\n",
        "ax  = fig.add_subplot(111)\n",
        "def drawLine(i,j,pose,ax):\n",
        "  ax.plot([pose[i*2],pose[j*2]],[-pose[i*2+1],-pose[j*2+1]], color = 'b')\n",
        "\n",
        "# structure = skeletalModel.getSkeletalModelStructure()\n",
        "lines4disp = ((0,1),(1,2),(2,3),(3,4),(1,5),(5,6),(6,7),(7,8),(8,9),(9,10),(10,11),(11,12),(8,13),(13,14),(14,15),(15,16),(8,17),(7,18),(18,19),(19,20),(8,21),(21,22),(22,23),(23,24),(8,25),(25,26),(26,27,(27,28),(4,29),(29,30),(30,31),(31,32),(32,33),29,34),(34,35),(35,36),(36,37),(29,38),(38,39),(39,40),(40,41),(29,42),(42,43),(43,44),(44,45),(29,46),(46,47),(47,48),(48,49))\n",
        "\n",
        "for s in lines4disp:\n",
        "  drawLine(s[0],s[1],pose,ax)\n",
        "\n",
        "ax.scatter(pose[150::2],-1 * pose[151::2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it845dyqxQHS"
      },
      "source": [
        "# the following code converts .sh script to proper format \n",
        "# !hexdump -C swf2mp4.sh \n",
        "# !cat swf2mp4.sh | tr -d '\\r' >> yournewscript.sh"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}